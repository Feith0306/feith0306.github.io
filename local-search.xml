<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>凸优化学习系列 - 2. 凸优化问题分类</title>
    <link href="/2026/01/26/03=Convex_Optimization/CVX-2/"/>
    <url>/2026/01/26/03=Convex_Optimization/CVX-2/</url>
    
    <content type="html"><![CDATA[<blockquote><p><strong>这个文档用于梳理几类常见的凸优化问题的标准定义与包含关系。</strong></p></blockquote><h1 id="线性规划linear-program-lp的标准定义">1. 线性规划（Linear Program, LP）的标准定义</h1><div class="note note-info">            <p>线性规划（LP）是指目标函数与约束均为线性（仿射）形式的优化问题。标准形式可写为 <span class="math display">\[\begin{align}\min_{x\in\mathbb{R}^n}\quad &amp; c^\top x\\\text{s.t.}\quad &amp; Dx \le d,\\&amp; Ax = b,\end{align}\]</span> 其中 <span class="math inline">\(c\in\mathbb{R}^n\)</span>，<span class="math inline">\(D\in\mathbb{R}^{p\times n}\)</span>，<span class="math inline">\(d\in\mathbb{R}^p\)</span>，<span class="math inline">\(A\in\mathbb{R}^{m\times n}\)</span>，<span class="math inline">\(b\in\mathbb{R}^m\)</span>，不等式 <span class="math inline">\(Dx\le d\)</span> 按分量理解。</p>          </div><blockquote><p>由于线性函数是凸函数，线性不等式与等式定义的可行域是凸集，因此 LP 始终是凸优化问题。</p></blockquote><hr><h1 id="凸二次规划convex-quadratic-program-qp的标准定义">2. 凸二次规划（Convex Quadratic Program, QP）的标准定义</h1><div class="note note-info">            <p>凸二次规划（QP）是指目标函数为凸二次函数、约束为线性（仿射）形式的问题。标准形式可写为 <span class="math display">\[\begin{align}\min_{x\in\mathbb{R}^n}\quad &amp; c^\top x + \frac{1}{2}x^\top Qx\\\text{s.t.}\quad &amp; Dx \le d,\\&amp; Ax = b,\end{align}\]</span> 其中 <span class="math inline">\(Q\in\mathbb{S}^n\)</span>（对称矩阵）。当且仅当<span class="math inline">\(Q\succeq 0\)</span>（半正定）时，上述 QP 为<strong>凸二次规划</strong>；若 <span class="math inline">\(Q\)</span> 非半正定，则问题一般<strong>非凸</strong>。</p>          </div><hr><h1 id="二阶锥规划second-order-cone-program-socp的标准定义">3. 二阶锥规划（Second-Order Cone Program, SOCP）的标准定义</h1><div class="note note-info">            <p>二阶锥规划（SOCP）是指线性目标 + 线性等式约束 + 若干个二阶锥（SOC）约束的问题。常见标准形式为 <span class="math display">\[\begin{align}\min_{x\in\mathbb{R}^n}\quad &amp; c^\top x\\\text{s.t.}\quad &amp; \|D_i x + d_i\|_2 \le e_i^\top x + f_i,\quad i=1,\dots,p,\\&amp; Ax = b,\end{align}\]</span> 其中 <span class="math inline">\(D_i\in\mathbb{R}^{k_i\times n}\)</span>，<span class="math inline">\(d_i\in\mathbb{R}^{k_i}\)</span>，<span class="math inline">\(e_i\in\mathbb{R}^{n}\)</span>，<span class="math inline">\(f_i\in\mathbb{R}\)</span>。</p>          </div><p>二阶锥（Lorentz cone）的定义为 <span class="math display">\[\begin{align}\mathcal{Q}_{k+1} \;=\; \{(u,t)\in\mathbb{R}^k\times\mathbb{R}\;:\;\|u\|_2\le t\}.\end{align}\]</span> 因此 SOCP 约束 <span class="math inline">\(\|D_i x + d_i\|_2 \le e_i^\top x + f_i\)</span> 等价于 <span class="math display">\[\begin{align}(D_i x + d_i,\; e_i^\top x + f_i)\in \mathcal{Q}_{k_i+1}.\end{align}\]</span></p><hr><h1 id="半正定规划semidefinite-program-sdp的标准定义">4. 半正定规划（Semidefinite Program, SDP）的标准定义</h1><div class="note note-info">            <p>半正定规划（SDP）是指线性目标 + 线性等式约束 + 线性矩阵不等式（LMI）约束的问题。常用标准形式为 <span class="math display">\[\begin{align}\min_{x\in\mathbb{R}^n}\quad &amp; c^\top x\\\text{s.t.}\quad &amp; x_1F_1 + \cdots + x_nF_n \preceq F_0,\\&amp; Ax = b,\end{align}\]</span> 其中 <span class="math inline">\(F_i\in\mathbb{S}^d\)</span>（<span class="math inline">\(d\times d\)</span> 对称矩阵），<span class="math inline">\(\preceq\)</span> 表示半正定序（<span class="math inline">\(M\preceq N \Leftrightarrow N-M\succeq 0\)</span>）。</p>          </div><p>将矩阵不等式改写为“落在半正定锥中”的形式更直接： <span class="math display">\[\begin{align}F_0 - \sum_{i=1}^n x_iF_i \in \mathbb{S}^d_+,\end{align}\]</span> 其中 <span class="math inline">\(\mathbb{S}^d_+\)</span> 为半正定锥（闭凸锥）。</p><hr><h1 id="锥规划conic-program的标准定义">5. 锥规划（Conic Program）的标准定义</h1><h2 id="锥规划的标准定义">5.1 锥规划的标准定义</h2><div class="note note-info">            <p>锥规划（Conic Program）以“某个线性映射的像落在闭凸锥 <span class="math inline">\(K\)</span> 中”为核心约束。标准形式可写为 <span class="math display">\[\begin{align}\min_{x\in\mathbb{R}^n}\quad &amp; c^\top x\\\text{s.t.}\quad &amp; Ax=b,\\&amp; D(x)+d \in K,\end{align}\]</span> 其中 - <span class="math inline">\(c\in\mathbb{R}^n\)</span>，<span class="math inline">\(A\in\mathbb{R}^{m\times n}\)</span>，<span class="math inline">\(b\in\mathbb{R}^m\)</span>； - <span class="math inline">\(D:\mathbb{R}^n\to Y\)</span> 是线性映射，<span class="math inline">\(d\in Y\)</span>； - <span class="math inline">\(Y\)</span> 是欧氏空间（可看作带内积的有限维向量空间）； - <span class="math inline">\(K\subseteq Y\)</span> 是<strong>闭凸锥</strong>（closed convex cone）。</p>          </div><hr><h1 id="五类凸优化问题的包含关系">6. 五类凸优化问题的包含关系</h1><p>我们要说明如下包含链： <span class="math display">\[\begin{align}\mathrm{LPs}\subseteq \mathrm{QPs}\subseteq \mathrm{SOCPs}\subseteq \mathrm{SDPs}\subseteq \mathrm{Conic\ Programs}.\end{align}\]</span> 下面逐个证明“前者是后者的特例/可等价表示”。</p><hr><h2 id="证明mathrmlpssubseteq-mathrmqps">6.1 证明：<span class="math inline">\(\mathrm{LPs}\subseteq \mathrm{QPs}\)</span></h2><p>给定任意 LP： <span class="math display">\[\begin{align}\min_x\; c^\top x\quad \text{s.t.}\; Dx\le d,\;Ax=b,\end{align}\]</span> 取 QP 中 <span class="math inline">\(Q=0\)</span>（零矩阵），则 <span class="math display">\[\begin{align}c^\top x + \frac12 x^\top Qx = c^\top x,\end{align}\]</span> 并且约束不变，因此每个 LP 都是一个（凸）QP 的特例，从而 <span class="math inline">\(\mathrm{LPs}\subseteq \mathrm{QPs}\)</span>。</p><hr><h2 id="证明mathrmqpssubseteq-mathrmsocps凸-qp-的-socp-表示">6.2 证明：<span class="math inline">\(\mathrm{QPs}\subseteq \mathrm{SOCPs}\)</span>（凸 QP 的 SOCP 表示）</h2><p>考虑凸 QP： <span class="math display">\[\begin{align}\min_x\quad &amp; c^\top x + \frac12 x^\top Qx\\\text{s.t.}\quad &amp; Dx\le d,\;Ax=b,\;\;Q\succeq 0.\end{align}\]</span> 用上图形式的 SOCP（线性目标）通常通过<strong>引入上图（epigraph）变量</strong>实现。令 <span class="math inline">\(t\in\mathbb{R}\)</span>，等价改写为 <span class="math display">\[\begin{align}\min_{x,t}\quad &amp; c^\top x + t\\\text{s.t.}\quad &amp; Dx\le d,\;Ax=b,\\&amp; t \ge \frac12 x^\top Qx.\end{align}\]</span> 由于 <span class="math inline">\(Q\succeq 0\)</span>，存在矩阵 <span class="math inline">\(R\)</span> 使得 <span class="math inline">\(Q=R^\top R\)</span>（如 Cholesky 或谱分解后取平方根），则 <span class="math display">\[\begin{align}x^\top Qx = x^\top R^\top R x = \|Rx\|_2^2.\end{align}\]</span> 因此约束变为 <span class="math display">\[\begin{align}t \ge \frac12 \|Rx\|_2^2.\end{align}\]</span> 该二次上界可用<strong>旋转二阶锥</strong>（rotated SOC）表示：旋转二阶锥定义为 <span class="math display">\[\begin{align}\mathcal{Q}_r^{k+2}=\{(u,v,w)\in\mathbb{R}\times\mathbb{R}\times\mathbb{R}^k:\; u\ge 0,\; v\ge 0,\; 2uv\ge \|w\|_2^2\}.\end{align}\]</span> 令 <span class="math inline">\(u=t,\;v=1,\;w=Rx\)</span>，则 <span class="math display">\[\begin{align}(t,1,Rx)\in \mathcal{Q}_r^{k+2}\;\Longleftrightarrow\;t\ge \frac12\|Rx\|_2^2.\end{align}\]</span> 于是凸 QP 可改写为带（旋转）SOC 约束的 SOCP，因此 <span class="math inline">\(\mathrm{QPs}\subseteq \mathrm{SOCPs}\)</span>（在允许旋转 SOC 的标准 SOCP 框架下成立；工程与优化文献中通常默认 SOCP 允许该等价变换）。</p><hr><h2 id="证明mathrmsocpssubseteq-mathrmsdpssoc-约束等价为-lmi">6.3 证明：<span class="math inline">\(\mathrm{SOCPs}\subseteq \mathrm{SDPs}\)</span>（SOC 约束等价为 LMI）</h2><p>SOCP 的核心约束为 <span class="math display">\[\begin{align}\|u\|_2 \le t,\end{align}\]</span> 其中 <span class="math inline">\(u\in\mathbb{R}^k\)</span>，<span class="math inline">\(t\in\mathbb{R}\)</span>。证明其可写为半正定约束（LMI）。</p><p>考虑块对称矩阵 <span class="math display">\[\begin{align}M=\begin{bmatrix}tI_k &amp; u\\u^\top &amp; t\end{bmatrix}.\end{align}\]</span> 若 <span class="math inline">\(M\succeq 0\)</span>，则必有 <span class="math inline">\(t\ge 0\)</span>，并由 Schur 补（对 <span class="math inline">\(tI_k\)</span>）可得 <span class="math display">\[\begin{align}M\succeq 0\;\Longleftrightarrow\;tI_k\succeq 0\ \text{且}\ t - u^\top (tI_k)^{-1}u \ge 0\;\Longleftrightarrow\;t\ge 0,\ \ t - \frac{1}{t}\|u\|_2^2 \ge 0.\end{align}\]</span> 当 <span class="math inline">\(t&gt;0\)</span> 时，上式等价于 <span class="math display">\[\begin{align}t^2 \ge \|u\|_2^2 \;\Longleftrightarrow\; \|u\|_2 \le t.\end{align}\]</span> 当 <span class="math inline">\(t=0\)</span> 时，<span class="math inline">\(M\succeq 0\)</span> 强制 <span class="math inline">\(u=0\)</span>，也满足 <span class="math inline">\(\|u\|_2\le t\)</span>。因此 <span class="math display">\[\begin{align}\|u\|_2 \le t\;\Longleftrightarrow\;\begin{bmatrix}tI_k &amp; u\\u^\top &amp; t\end{bmatrix}\succeq 0.\end{align}\]</span> SOCP 中每个 SOC 约束都可用一个 LMI 表示，线性目标与仿射约束保持不变，因此 SOCP 是 SDP 的特例：<span class="math inline">\(\mathrm{SOCPs}\subseteq \mathrm{SDPs}\)</span>。</p><hr><h2 id="证明mathrmsdpssubseteq-mathrmconic-programs">6.4 证明：<span class="math inline">\(\mathrm{SDPs}\subseteq \mathrm{Conic\ Programs}\)</span></h2><p>SDP 的矩阵不等式 <span class="math display">\[\begin{align}\sum_{i=1}^n x_iF_i \preceq F_0\end{align}\]</span> 等价于 <span class="math display">\[\begin{align}F_0-\sum_{i=1}^n x_iF_i \in \mathbb{S}_+^d.\end{align}\]</span> 令欧氏空间 <span class="math inline">\(Y=\mathbb{S}^d\)</span>，闭凸锥 <span class="math inline">\(K=\mathbb{S}_+^d\)</span>，并定义线性映射 <span class="math display">\[\begin{align}D(x)= -\sum_{i=1}^n x_iF_i,\qquad d=F_0,\end{align}\]</span> 则约束恰为 <span class="math inline">\(D(x)+d\in K\)</span>。同时保留线性等式约束 <span class="math inline">\(Ax=b\)</span> 与线性目标 <span class="math inline">\(c^\top x\)</span>，因此每个 SDP 都是锥规划的特例，从而 <span class="math display">\[\begin{align}\mathrm{SDPs}\subseteq \mathrm{Conic\ Programs}.\end{align}\]</span></p><hr><h2 id="小结">6.5 小结</h2><p>由 6.1–6.4 的逐步构造与等价变换，得到 <span class="math display">\[\begin{align}\mathrm{LPs}\subseteq \mathrm{QPs}\subseteq \mathrm{SOCPs}\subseteq \mathrm{SDPs}\subseteq \mathrm{Conic\ Programs}.\end{align}\]</span> 该关系可通过下图直观表达。</p><figure><img src="/img/cvx-problem-relation.png" alt="CVX问题包含关系"><figcaption aria-hidden="true">CVX问题包含关系</figcaption></figure><blockquote><p><strong>这条链表达的是：越往右，允许的锥/矩阵锥结构越一般，建模能力越强，但求解器实现与计算代价通常也越高。</strong></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>optimization</category>
      
    </categories>
    
    
    <tags>
      
      <tag>convex set</tag>
      
      <tag>convex function</tag>
      
      <tag>affine</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>凸优化学习系列 - 4. 梯度下降法</title>
    <link href="/2026/01/26/03=Convex_Optimization/CVX-4/"/>
    <url>/2026/01/26/03=Convex_Optimization/CVX-4/</url>
    
    <content type="html"><![CDATA[<blockquote><p><strong>目标：本文档从零学习如何用梯度下降法求解凸优化问题。读完这篇文档，应当能</strong></p></blockquote><ul><li>理解梯度下降法的基本原理；</li><li>学会如何用梯度下降法求解凸优化问题；</li><li>分析梯度下降的收敛性以及对步长选择的敏感性；</li></ul><hr><h2 id="一问题描述">一、问题描述</h2><p>本文档讨论的是无约束，目标函数为凸函数且可微的优化问题： <span class="math display">\[\begin{align}\label{eq:cvx-problem}\min_{x\in\mathbb{R}^n} f(x).\end{align}\]</span></p><blockquote><p>后续讨论前提：优化问题 <span class="math inline">\(\eqref{eq:cvx-problem}\)</span> 有解，且最优值记为 <span class="math inline">\(f^\star\)</span>，最优解集合记为 <span class="math inline">\(\mathcal{X}^\star\)</span>。 注意优化问题 <span class="math inline">\(\eqref{eq:cvx-problem}\)</span> 解不一定唯一，关于其存在性与唯一性的讨论可参考<a href="/2026/01/26/03=Convex_Optimization/CVX-1/" title="凸优化学习系列 - 1. 基本概念">凸优化学习系列 - 1. 基本概念-2.3</a>。</p></blockquote><!-- > **为何从无约束开始**：无约束时更新形式最简洁，核心困难集中在“一阶信息 + 步长 + 曲率控制”。此框架也便于引出 Lipschitz 梯度、强凸、线搜索与收敛速率等基本概念。 --><!-- ### 2. 带约束凸优化：梯度下降并非不能用，但需改造若存在凸约束 $x\in\mathcal{C}$（闭凸集），问题为：$$\begin{align}\min_{x\in\mathcal{C}}~ f(x).\end{align}$$此时“直接的” $x^+=x-t\nabla f(x)$ 可能离开可行域，需要结合约束结构采用等价/扩展的一阶方法。#### (A) 投影梯度法（Projected Gradient Descent, PGD）> 有待后续文档中详细讨论当集合 $\mathcal{C}$ 的欧氏投影易计算时，使用：$$\begin{align}x_{k+1} = \Pi_{\mathcal{C}}\!\left(x_k - t_k \nabla f(x_k)\right),\quad\Pi_{\mathcal{C}}(z):=\arg\min_{y\in\mathcal{C}}\|y-z\|_2.\end{align}$$- **适用约束类型**：盒约束、球约束、单纯形、一些简单线性约束等（投影可高效实现）。- **要点**：更新由“梯度步 + 投影回可行域”组成，本质上是将无约束梯度步的点通过最近点映射拉回约束集。#### (B) 近端梯度法（Proximal Gradient）统一约束与不可微正则> 有待后续文档中详细讨论考虑复合形式：$$\begin{align}\min_x~ f(x)+g(x),\end{align}$$其中 $f$ 可微凸，$g$ 凸但可不可微。定义近端算子：$$\begin{align}\operatorname{prox}_{t ,g}(z):=\arg\min_y\left(g(y)+\frac{1}{2t}\|y-z\|_2^2\right).\end{align}$$近端梯度更新为：$$\begin{align}x_{k+1}=\operatorname{prox}_{t_k g}\left(x_k-t_k\nabla f(x_k)\right).\end{align}$$- **约束作为指标函数**：令 $g=\iota_{\mathcal{C}}$，其中 $\iota_{\mathcal{C}}(x)=0$ 若 $x\in\mathcal{C}$，否则 $+\infty$，则 $\operatorname{prox}_{t \iota_{\mathcal{C}}}$ 恰为 $\Pi_{\mathcal{C}}$，因此 PGD 是近端梯度的特例。#### (C) 约束复杂时：原始-对偶/障碍/增广拉格朗日等更常用（后续文档讨论）当投影或近端子问题难以求解（例如一般不等式约束的复杂可行域），通常转向原始-对偶方法、内点法或增广拉格朗日法等，而非“朴素梯度 + 硬投影”。 --><hr><h2 id="二梯度下降法的推导与解读">二、梯度下降法的推导与解读</h2><h3 id="梯度下降法的由来motivation">1. 梯度下降法的由来（motivation）</h3><p>对于凸优化问题 <span class="math inline">\(\eqref{eq:cvx-problem}\)</span>，真实的目标函数 <span class="math inline">\(f\)</span> 可能复杂难解，直接求解 <span class="math inline">\(\min_x f(x)\)</span> 不易（不能一眼看出来）。因此我们希望用一个更简单的函数（二次）近似原函数，从而简化优化问题的求解。最常见的方法是利用泰勒展开构造局部近似。 将可微函数 <span class="math inline">\(f\)</span> 在 <span class="math inline">\(x_k\)</span> 附近的二阶泰勒展开作为 <span class="math inline">\(f\)</span> 的近似： <span class="math display">\[\begin{align}\label{eq:taylor-second}\hat f(x;x_k) = f(x_k) + \nabla f(x_k)^\top (x - x_k) + \frac{1}{2}(x - x_k)^\top \nabla^2 f(x_k) (x - x_k).\end{align}\]</span></p><div class="note note-info">            <ul><li>问题一：为什么要对目标函数做<strong>二阶</strong>泰勒展开？<ul><li>如果只用一阶信息：<span class="math inline">\(\hat f(x;x_k) = f(x_k) + \nabla f(x_k)^\top (x - x_k)\)</span>，这个近似函数对 <span class="math inline">\(x\)</span> 是线性的，没有最小值（除非梯度为零-常函数），无法进行更新。也即仅靠一阶展开不能定义出下一步应该去哪里找 <span class="math inline">\(x\)</span>。</li><li>如果将二阶项纳入 <span class="math inline">\(\hat f(x;x_k)\)</span> 即有 <span class="math inline">\(\eqref{eq:taylor-second}\)</span>，则该近似函数为关于 <span class="math inline">\(x\)</span> 的二次函数，<strong>通常</strong>有唯一最小点，可以据此定义更新方向和步长。</li></ul></li></ul>          </div><p>基于 <span class="math inline">\(\eqref{eq:taylor-second}\)</span>，利用驻点条件，可构造牛顿法更新： <span class="math display">\[\begin{align}\label{eq:newton-update}x_{k+1} = x_k - (\nabla^2 f(x_k))^{-1} \nabla f(x_k).\end{align}\]</span> 如果按照 <span class="math inline">\(\eqref{eq:newton-update}\)</span> 迭代，那么每步都需要计算 Hessian <span class="math inline">\(\nabla^2 f(x_k)\)</span> 并求其逆矩阵，这在高维问题中代价巨大。 那么我们是否可以简化 <span class="math inline">\(\eqref{eq:taylor-second}\)</span>，从而避免 Hessian 的计算与求逆呢？首先，要明确我们的核心逻辑：</p><blockquote><p>我们需要一个<strong>二次项</strong>能正确“控制”函数在<strong>邻域</strong>内的变化的模型，不一定要精确的 Hessian，从而构造迭代收敛到最优解。</p></blockquote><p>因此，我们可以将复杂的 Hessian <span class="math inline">\(\nabla^2 f(x_k)\)</span> 替换为一个简单的 <span class="math inline">\(\frac{1}{t}I\)</span>，其中 <span class="math inline">\(t&gt;0\)</span> 是参数。这样，可以得到如下局部二次近似模型： <span class="math display">\[\begin{align}\label{eq:local-model}\hat f(x;x_k)=f(x_k)+\nabla f(x_k)^\top (x-x_k)+\frac{1}{2t}\|x-x_k\|_2^2.\end{align}\]</span></p><div class="note note-info">            <ul><li>问题二：为什么可以用 <span class="math inline">\(\frac{1}{t}I\)</span> 代替 Hessian？<ul><li>这样做最标准的理论前提是 <span class="math inline">\(\nabla f\)</span> 是 Lipschitz 连续的（也即 <span class="math inline">\(f\)</span> 为 <span class="math inline">\(L\)</span>-smooth）， 即存在常数 <span class="math inline">\(L&gt;0\)</span>，使得对任意 <span class="math inline">\(x,y\)</span>： <span class="math display">\[\begin{align}\|\nabla f(x) - \nabla f(y)\|_2 \le L \|x - y\|_2\label{eq:L-smooth}\end{align}\]</span> 基于 <span class="math inline">\(\eqref{eq:L-smooth}\)</span> 可得到 <span class="math display">\[\begin{align}f(x) \le f(x_k) + \nabla f(x_k)^\top (x - x_k) + \frac{L}{2} \|x - x_k\|_2^2\label{eq:descent-lemma}\end{align}\]</span> 这其实是<code>Descent Lemma</code>，具体证明见 <a href="https://zhuanlan.zhihu.com/p/369961290">【强凸与光滑性】</a>。将 <span class="math inline">\(\eqref{eq:local-model}\)</span> 中的 <span class="math inline">\(t\)</span> 取为 <span class="math inline">\(\frac{1}{L}\)</span>，即可保证 <span class="math inline">\(\hat f(x;x_0)\)</span> 是 <span class="math inline">\(f(x)\)</span> 的上界。</li><li>注意，如果 <span class="math inline">\(f\)</span> 确实是 <span class="math inline">\(L\)</span>-smooth 的，那么当然可以直接使用 <code>Descent Lemma</code> <span class="math inline">\(\eqref{eq:descent-lemma}\)</span>，从而将最小化 <span class="math inline">\(f(x)\)</span> 转化为最小化 <span class="math inline">\(f(x)\)</span> 的上界，通过牛顿法 <span class="math inline">\(\eqref{eq:newton-update}\)</span> 求解（将曲率替换为 <span class="math inline">\(\frac{1}{t}I\)</span>），也就不需要 <span class="math inline">\(f\)</span> 的二阶信息。</li><li>但从二阶泰勒展开的视角更直观地揭示了梯度下降法的本质：<strong>用一个简单的二次模型替代真实函数，从而简化每步的优化问题。</strong></li><li>另一方面，如果 <span class="math inline">\(f\)</span> 不是 <span class="math inline">\(L\)</span>-smooth，那么 <span class="math inline">\(\frac{1}{t}I\)</span> 就是用一个标量曲率上界来替代 Hessian，从而保证每步更新的稳定性（<strong>这种视角是否需要前提：<span class="math inline">\(f\)</span> 二阶可导？</strong>）。此时参数 <span class="math inline">\(t\)</span> 的选择就更为关键（没有 <span class="math inline">\(L\)</span> 作为参考），直接影响每步更新的稳定性与收敛性。在这种情况下，为了让 <span class="math inline">\(\eqref{eq:local-model}\)</span> 中的 <span class="math inline">\(\hat f(x;x_0)\)</span> 尽可能作为 <span class="math inline">\(f(x)\)</span> 的上界，我们<strong>一般</strong>希望 <span class="math inline">\(t\)</span> 较小。</li></ul></li></ul>          </div><h3 id="梯度下降法的构造">2. 梯度下降法的构造</h3><p>基于上述讨论，假设我们以及有了合适的参数 <span class="math inline">\(t\)</span> 使得 <span class="math inline">\(f(x)\)</span> 的上界由 <span class="math inline">\(\eqref{eq:local-model}\)</span> 刻画。因此，对于 <span class="math inline">\(f\)</span> 的凸优化问题 <span class="math inline">\(\eqref{eq:cvx-problem}\)</span> 转为对于 <span class="math inline">\(f\)</span> 上界 <span class="math inline">\(\hat f\)</span> 的优化问题： <span class="math display">\[\begin{align}\label{eq:cvx-problem-2}    \min_x~ \hat f(x;x_k) = \min_x\left(f(x_k)+\nabla f(x_k)^\top (x-x_k)+\frac{1}{2t}\|x-x_k\|_2^2\right).\end{align}\]</span></p><p>因此，下一点 <span class="math inline">\(x_{k+1}\)</span> 在二次函数 <span class="math inline">\(\hat f(x;x_k)\)</span> 的对称轴处取得： <span class="math display">\[\begin{align}\label{eq:gd-update}\begin{split}&amp; x_{k+1} = \arg\min_{x}\left(f(x_k)+\nabla f(x_k)^\top (x-x_k)+\frac{1}{2t}\|x-x_k\|_2^2\right), \\\Rightarrow \quad &amp; x_{k+1}=x_k - t\nabla f(x_k)\end{split}\end{align}\]</span> 式 <span class="math inline">\(\eqref{eq:gd-update}\)</span> 即为求解凸优化问题 <span class="math inline">\(\eqref{eq:cvx-problem-2}\)</span> 的梯度下降法更新律，其中 <span class="math inline">\(t\)</span> 即为步长。</p><div class="note note-info">            <p>问题三：问题 <span class="math inline">\(\eqref{eq:cvx-problem}\)</span> 与 <span class="math inline">\(\eqref{eq:cvx-problem-2}\)</span> 是否等价？如果等价，为什么？如果不等价，谁是谁的充分条件/必要条件？ <!-- - 不等价。$\eqref{eq:cvx-problem-2}$ 是 $\eqref{eq:cvx-problem}$ 在 $x_k$ 附近的局部近似问题，其最优解 $x_{k+1}$ 仅作为 $\eqref{eq:cvx-problem}$ 的下一个迭代点，而非其最优解。  - 但如果 $t$ 选取合适，并且按照 $\eqref{eq:gd-update}$ 迭代足够多次，则 $\eqref{eq:cvx-problem-2}$ 的解 $x_{k+1}$ 会越来越接近 $x^\star$。 --></p>          </div><h2 id="三.-梯度下降的收敛性">三. 梯度下降的收敛性</h2><h3 id="是否能收敛">3.1 是否能收敛？</h3><h3 id="收敛到哪里">3.2 收敛到哪里？</h3><h3 id="以什么样的形式收敛">3.3 以什么样的形式收敛？</h3><div class="note note-success">            <p><strong>定义一：对于凸优化问题 <span class="math inline">\(\eqref{eq:cvx-problem}\)</span>，可从以下两种角度定义收敛性</strong></p><ul><li><p>收敛速率：若存在函数 <span class="math inline">\(r(k)\)</span> 与常数 <span class="math inline">\(p &gt; 0\)</span>，使得对所有 <span class="math inline">\(k \ge 0\)</span>，有 <span class="math display">\[\begin{align}f(x^{(k)}) - f^\star \le p \cdot r(k),\end{align}\]</span> 则称算法以速率 <span class="math inline">\(O(r(k))\)</span> 收敛。</p></li><li><p><span class="math inline">\(\varepsilon\)</span>-精度：给定精度 <span class="math inline">\(\varepsilon&gt;0\)</span>，令 <span class="math inline">\(T(\varepsilon) = \min_k \{f(x_k) - f^\star \le \varepsilon\}\)</span>，若存在常数 <span class="math inline">\(q &gt; 0\)</span>，使得 <span class="math display">\[\begin{align}T(\varepsilon) \le q \cdot g(\varepsilon)\end{align}\]</span> 则称算法有 <span class="math inline">\(\varepsilon\)</span>-精度，且迭代复杂度为 <span class="math inline">\(T(\varepsilon)\)</span>，<span class="math inline">\(x_k\)</span> 称为 <span class="math inline">\(\varepsilon\)</span>-近似解。</p></li></ul>          </div><ul><li>两种指标的常见对应关系：<ul><li>若 <span class="math inline">\(f(x_k) - f^\star \le O(\frac{1}{k})\)</span>，则 <span class="math inline">\(T(\varepsilon) = O(\frac{1}{\varepsilon})\)</span>；</li><li>若 <span class="math inline">\(f(x_k) - f^\star \le O(\frac{1}{\sqrt{k}})\)</span>，则 <span class="math inline">\(T(\varepsilon) = O(\frac{1}{\varepsilon^2})\)</span>；</li><li>若 <span class="math inline">\(f(x_k) - f^\star \le O(\rho^k)\)</span>，则 <span class="math inline">\(T(\varepsilon) = O(\log\frac{1}{\varepsilon})\)</span>。</li></ul></li></ul><p><strong>对于凸优化问题 <span class="math inline">\(\eqref{eq:cvx-problem}\)</span>，梯度下降法 <span class="math inline">\(\eqref{eq:gd-update}\)</span> 的收敛性依赖于目标函数 <span class="math inline">\(f\)</span> 的性质。下面总结几种常见情形下的收敛结果。</strong></p><div class="note note-warning">            <p><strong>定理 1：</strong> 如果目标函数 <span class="math inline">\(f\)</span> 是凸函数且 Lipschitz 连续，则梯度下降法 <span class="math inline">\(\eqref{eq:gd-update}\)</span> 以 <span class="math inline">\(O(\frac{1}{\varepsilon^2})\)</span> 收敛。</p>          </div><details><p>由平方范数展开（欧氏范数）： <span class="math display">\[\begin{align}\|x_{k+1}-x^\star\|_2^2&amp;=\|x_k-tg_k-x^\star\|_2^2 \\&amp;=\|x_k-x^\star\|_2^2 -2t\, g_k^\top(x_k-x^\star)+t^2\|g_k\|_2^2. \label{eq:t1-1}\end{align}\]</span></p><p>由凸性的一阶次梯度不等式（对任意 <span class="math inline">\(g_k\in\partial f(x_k)\)</span>）： <span class="math display">\[\begin{align}f(x_k)-f^\star \le g_k^\top(x_k-x^\star). \label{eq:t1-subg}\end{align}\]</span></p><p>将  代入  并用 <span class="math inline">\(\|g_k\|_2\le G\)</span>： <span class="math display">\[\begin{align}2t\,(f(x_k)-f^\star)&amp;\le 2t\, g_k^\top(x_k-x^\star) \\&amp;\le \|x_k-x^\star\|_2^2-\|x_{k+1}-x^\star\|_2^2+t^2 G^2. \label{eq:t1-2}\end{align}\]</span></p><p>对 <span class="math inline">\(k=0,1,\dots,K-1\)</span> 求和，望远镜消去： <span class="math display">\[\begin{align}2t\sum_{k=0}^{K-1} (f(x_k)-f^\star)&amp;\le \|x_0-x^\star\|_2^2-\|x_K-x^\star\|_2^2 + K t^2 G^2 \\&amp;\le \|x_0-x^\star\|_2^2 + K t^2 G^2. \label{eq:t1-3}\end{align}\]</span></p><p>令 <span class="math inline">\(\bar x_K=\frac{1}{K}\sum_{k=0}^{K-1} x_k\)</span>，由凸性（Jensen）： <span class="math display">\[\begin{align}f(\bar x_K)-f^\star \le \frac{1}{K}\sum_{k=0}^{K-1}(f(x_k)-f^\star)\le \frac{\|x_0-x^\star\|_2^2}{2tK}+\frac{tG^2}{2}. \label{eq:t1-4}\end{align}\]</span></p><p>取最优常数步长（使右端两项平衡），例如 <span class="math display">\[\begin{align}t=\frac{\|x_0-x^\star\|_2}{G\sqrt{K}},\end{align}\]</span> 代入  得 <span class="math display">\[\begin{align}f(\bar x_K)-f^\star \le \frac{G\|x_0-x^\star\|_2}{\sqrt{K}}.\end{align}\]</span></p>因此若要求 <span class="math inline">\(f(\bar x_K)-f^\star\le \varepsilon\)</span>，只需 <span class="math display">\[\begin{align}K \ge \frac{G^2\|x_0-x^\star\|_2^2}{\varepsilon^2},\end{align}\]</span> 即迭代复杂度为 <span class="math inline">\(O(1/\varepsilon^2)\)</span>，证毕。</details><hr><div class="note note-warning">            <p><strong>定理 2：</strong> 如果目标函数 <span class="math inline">\(f\)</span> 是凸函数且 <span class="math inline">\(L\)</span>-smooth，则梯度下降法 <span class="math inline">\(\eqref{eq:gd-update}\)</span> 以 <span class="math inline">\(O(\frac{1}{\varepsilon})\)</span> 收敛。</p>          </div><details><p>由 <span class="math inline">\(L\)</span>-smooth 的下降引理（descent lemma）：对任意 <span class="math inline">\(x,y\)</span>， <span class="math display">\[\begin{align}f(y)\le f(x)+\nabla f(x)^\top(y-x)+\frac{L}{2}\|y-x\|_2^2. \label{eq:t2-descent}\end{align}\]</span></p><p>取 <span class="math inline">\(x=x_k,\ y=x_{k+1}=x_k-\frac{1}{L}\nabla f(x_k)\)</span>，则 <span class="math display">\[\begin{align}f(x_{k+1})&amp;\le f(x_k)+\nabla f(x_k)^\top\Big(-\frac{1}{L}\nabla f(x_k)\Big)+\frac{L}{2}\Big\|\frac{1}{L}\nabla f(x_k)\Big\|_2^2 \\&amp;= f(x_k)-\frac{1}{2L}\|\nabla f(x_k)\|_2^2. \label{eq:t2-1}\end{align}\]</span></p><p>另一方面，由凸性一阶条件（对最优点 <span class="math inline">\(x^\star\)</span>）： <span class="math display">\[\begin{align}f(x_k)-f^\star \le \nabla f(x_k)^\top(x_k-x^\star). \label{eq:t2-cvx}\end{align}\]</span></p><p>再利用“梯度一步”的平方距离展开（把 <span class="math inline">\(x^\star\)</span> 视为常量）： <span class="math display">\[\begin{align}\|x_{k+1}-x^\star\|_2^2&amp;=\Big\|x_k-\frac{1}{L}\nabla f(x_k)-x^\star\Big\|_2^2 \\&amp;=\|x_k-x^\star\|_2^2-\frac{2}{L}\nabla f(x_k)^\top(x_k-x^\star)+\frac{1}{L^2}\|\nabla f(x_k)\|_2^2. \label{eq:t2-2}\end{align}\]</span></p><p>将  代入 ，并与  结合消去 <span class="math inline">\(\|\nabla f(x_k)\|^2\)</span>，可得经典不等式： <span class="math display">\[\begin{align}f(x_{k+1})-f^\star \le \frac{L}{2}\big(\|x_k-x^\star\|_2^2-\|x_{k+1}-x^\star\|_2^2\big). \label{eq:t2-3}\end{align}\]</span></p><p>对 <span class="math inline">\(k=0,\dots,K-1\)</span> 求和并整理可得： <span class="math display">\[\begin{align}\sum_{k=0}^{K-1}\big(f(x_{k+1})-f^\star\big)&amp;\le \frac{L}{2}\big(\|x_0-x^\star\|_2^2-\|x_K-x^\star\|_2^2\big) \\&amp;\le \frac{L}{2}\|x_0-x^\star\|_2^2. \label{eq:t2-4}\end{align}\]</span></p><p>由于 <span class="math inline">\(f(x_k)\)</span> 单调不增（由 ），有 <span class="math display">\[\begin{align}K\big(f(x_K)-f^\star\big)\le \sum_{k=0}^{K-1}\big(f(x_{k+1})-f^\star\big)\le \frac{L}{2}\|x_0-x^\star\|_2^2,\end{align}\]</span> 即 <span class="math display">\[\begin{align}f(x_K)-f^\star \le \frac{L\|x_0-x^\star\|_2^2}{2K}. \label{eq:t2-rate}\end{align}\]</span></p>令右端 <span class="math inline">\(\le \varepsilon\)</span> 得 <span class="math inline">\(K\ge \frac{L\|x_0-x^\star\|_2^2}{2\varepsilon}\)</span>， 故迭代复杂度为 <span class="math inline">\(O(1/\varepsilon)\)</span>，证毕。</details><hr><div class="note note-warning">            <p><strong>定理 3：</strong> 如果目标函数 <span class="math inline">\(f\)</span> 是 <span class="math inline">\(m\)</span>-强凸函数且 <span class="math inline">\(L\)</span>-smooth，则梯度下降法 <span class="math inline">\(\eqref{eq:gd-update}\)</span> 以线性 <span class="math inline">\(O(\log(\frac{1}{\varepsilon}))\)</span> 收敛，其中 <span class="math inline">\(0 &lt; \rho &lt; 1\)</span>。</p>          </div><details><p>由定理 2 的下降结论（对任意 <span class="math inline">\(k\)</span>）： <span class="math display">\[\begin{align}f(x_{k+1}) \le f(x_k)-\frac{1}{2L}\|\nabla f(x_k)\|_2^2. \label{eq:t3-1}\end{align}\]</span></p><p>对 <span class="math inline">\(m\)</span>-强凸且可微函数，有 Polyak–ojasiewicz 型下界（强凸 <span class="math inline">\(\Rightarrow\)</span> PL）： <span class="math display">\[\begin{align}\|\nabla f(x)\|_2^2 \ge 2m\big(f(x)-f^\star\big), \quad \forall x. \label{eq:t3-plfromsc}\end{align}\]</span> 证明要点：强凸给出 <span class="math display">\[\begin{align}f(y)\ge f(x)+\nabla f(x)^\top(y-x)+\frac{m}{2}\|y-x\|_2^2,\end{align}\]</span> 取 <span class="math inline">\(y=x^\star\)</span>，并对右端关于 <span class="math inline">\(y\)</span> 的二次函数最小化（或用 Cauchy-Schwarz 完成平方），即可得到 。</p><p>将  代入 ： <span class="math display">\[\begin{align}f(x_{k+1})-f^\star&amp;\le f(x_k)-f^\star -\frac{1}{2L}\|\nabla f(x_k)\|_2^2 \\&amp;\le f(x_k)-f^\star -\frac{1}{2L}\cdot 2m\big(f(x_k)-f^\star\big) \\&amp;=\Big(1-\frac{m}{L}\Big)\big(f(x_k)-f^\star\big). \label{eq:t3-2}\end{align}\]</span></p><p>令 <span class="math inline">\(\rho=1-\frac{m}{L}\in(0,1)\)</span>，迭代递推得 <span class="math display">\[\begin{align}f(x_k)-f^\star \le \rho^k \big(f(x_0)-f^\star\big). \label{eq:t3-lin}\end{align}\]</span></p>因此若要求 <span class="math inline">\(f(x_k)-f^\star\le \varepsilon\)</span>，只需 <span class="math display">\[\begin{align}k \ge \frac{\log\big((f(x_0)-f^\star)/\varepsilon\big)}{\log(1/\rho)}=O(\log(1/\varepsilon)),\end{align}\]</span> 证毕。</details><hr><div class="note note-warning">            <p><strong>定理 4：</strong> 如果目标函数 <span class="math inline">\(f\)</span> 满足 Polyak-Łojasiewicz (PL) 条件： <span class="math display">\[\begin{align}\frac{1}{2}\|\nabla f(x)\|_2^2 \ge \mu (f(x) - f^\star), \quad \forall x,\end{align}\]</span> 且 <span class="math inline">\(L\)</span>-smooth，则梯度下降法 <span class="math inline">\(\eqref{eq:gd-update}\)</span> 以线性 <span class="math inline">\(O(\log(\frac{1}{\varepsilon}))\)</span> 收敛，其中 <span class="math inline">\(0 &lt; \rho &lt; 1\)</span>。</p>          </div><details><p>由 <span class="math inline">\(L\)</span>-smooth 的下降引理（同定理2推导）直接得到： <span class="math display">\[\begin{align}f(x_{k+1}) \le f(x_k)-\frac{1}{2L}\|\nabla f(x_k)\|_2^2. \label{eq:t4-1}\end{align}\]</span></p><p>由 PL 条件 (11)： <span class="math display">\[\begin{align}\|\nabla f(x_k)\|_2^2 \ge 2\mu\big(f(x_k)-f^\star\big). \label{eq:t4-2}\end{align}\]</span></p><p>将  代入 ： <span class="math display">\[\begin{align}f(x_{k+1})-f^\star&amp;\le f(x_k)-f^\star-\frac{1}{2L}\|\nabla f(x_k)\|_2^2 \\&amp;\le f(x_k)-f^\star-\frac{1}{2L}\cdot 2\mu\big(f(x_k)-f^\star\big) \\&amp;=\Big(1-\frac{\mu}{L}\Big)\big(f(x_k)-f^\star\big). \label{eq:t4-3}\end{align}\]</span></p><p>令 <span class="math inline">\(\rho=1-\frac{\mu}{L}\in(0,1)\)</span>，递推得 <span class="math display">\[\begin{align}f(x_k)-f^\star \le \rho^k\big(f(x_0)-f^\star\big). \label{eq:t4-lin}\end{align}\]</span></p>因此达到精度 <span class="math inline">\(\varepsilon\)</span> 需要 <span class="math display">\[\begin{align}k \ge \frac{\log\big((f(x_0)-f^\star)/\varepsilon\big)}{\log(1/\rho)}=O(\log(1/\varepsilon)),\end{align}\]</span> 证毕。</details><hr><div class="note note-warning">            <p><strong>定理 5：</strong> 如果目标函数 <span class="math inline">\(f\)</span> 是非凸函数且 <span class="math inline">\(L\)</span>-smooth，则梯度下降法 <span class="math inline">\(\eqref{eq:gd-update}\)</span> 以 <span class="math inline">\(O(\frac{1}{\varepsilon^2})\)</span> 收敛。</p>          </div><details><p>由下降引理（同定理2）对一步更新： <span class="math display">\[\begin{align}f(x_{k+1}) \le f(x_k)-\frac{1}{2L}\|\nabla f(x_k)\|_2^2. \label{eq:t5-1}\end{align}\]</span></p><p>对 <span class="math inline">\(k=0,\dots,K-1\)</span> 求和： <span class="math display">\[\begin{align}\sum_{k=0}^{K-1}\frac{1}{2L}\|\nabla f(x_k)\|_2^2&amp;\le f(x_0)-f(x_K). \label{eq:t5-2}\end{align}\]</span></p><p>若进一步假设 <span class="math inline">\(f\)</span> 下界有界（常见于非凸分析）：存在 <span class="math inline">\(f_{\inf}\)</span> 使 <span class="math inline">\(f(x)\ge f_{\inf}\)</span>， 则 <span class="math display">\[\begin{align}\sum_{k=0}^{K-1}\|\nabla f(x_k)\|_2^2\le 2L\big(f(x_0)-f_{\inf}\big). \label{eq:t5-3}\end{align}\]</span></p><p>因此最小梯度范数平方满足 <span class="math display">\[\begin{align}\min_{0\le k\le K-1}\|\nabla f(x_k)\|_2^2\le \frac{1}{K}\sum_{k=0}^{K-1}\|\nabla f(x_k)\|_2^2\le \frac{2L\big(f(x_0)-f_{\inf}\big)}{K}. \label{eq:t5-4}\end{align}\]</span></p>若要求存在某个迭代点满足 <span class="math inline">\(\|\nabla f(x_k)\|_2\le \varepsilon\)</span>， 只需令右端 <span class="math inline">\(\le \varepsilon^2\)</span>，即 <span class="math display">\[\begin{align}K \ge \frac{2L\big(f(x_0)-f_{\inf}\big)}{\varepsilon^2},\end{align}\]</span> 从而迭代复杂度为 <span class="math inline">\(O(1/\varepsilon^2)\)</span>，证毕。</details><hr><table style="width:100%;"><thead><tr class="header"><th>性质 &amp; 条件</th><th>定理一</th><th>定理二</th><th>定理三</th><th>定理四</th><th>定理五</th></tr></thead><tbody><tr class="odd"><td>收敛精度</td><td><span class="math inline">\(O(\frac{1}{\varepsilon^2})\)</span></td><td><span class="math inline">\(O(\frac{1}{\varepsilon})\)</span></td><td><span class="math inline">\(O(\log \frac{1}{\varepsilon})\)</span></td><td><span class="math inline">\(O(\log \frac{1}{\varepsilon})\)</span></td><td><span class="math inline">\(O( \frac{1}{\varepsilon^2})\)</span></td></tr><tr class="even"><td>收敛速率</td><td><span class="math inline">\(O(\frac{1}{\sqrt{k}})\)</span></td><td><span class="math inline">\(O(\frac{1}{k})\)</span></td><td><span class="math inline">\(O(\rho^k)\)</span></td><td><span class="math inline">\(O(\rho^k)\)</span></td><td><span class="math inline">\(O(\frac{1}{\sqrt{k}})\)</span></td></tr></tbody></table><blockquote><p>给定同一小精度 <span class="math inline">\(\varepsilon \to 0\)</span>，精度排序为：<span class="math inline">\(O(\log \frac{1}{\varepsilon}) &lt; O(\frac{1}{\varepsilon}) &lt; O(\frac{1}{\varepsilon^2})\)</span>。这也很容易理解：条件越强，收敛精度越高。</p></blockquote><h2 id="四梯度下降的更新步长">四、梯度下降的更新步长</h2><p>步长 <span class="math inline">\(t\)</span> 决定每次更新幅度与稳定性。其核心作用可概括为：在局部线性化可信的范围内取尽可能大的步长，以加快收敛。</p><p>用一个例子说明<strong>步长大了会发散、步长小了收敛慢</strong>，考虑如下二次标量函数： <span class="math display">\[\begin{align}\label{eq:quadratic-example}f(x)= \frac{a}{2}x^2,\end{align}\]</span> 则 <span class="math display">\[\begin{align}\nabla f(x)=ax,\quadx_{k+1}=x_k-tax_k=(1-ta)x_k.\end{align}\]</span> 收敛要求收缩因子 <span class="math inline">\(|1-ta|&lt;1\)</span> （因为 <span class="math inline">\(x_k = (1-ta)^k x_0\)</span>），等价于 <span class="math display">\[\begin{align}\label{eq:step-size-condition}0 &lt; t &lt;\frac{2}{a}.\end{align}\]</span></p><ul><li><strong>步长过大</strong>：不满足步长条件 <span class="math inline">\(\eqref{eq:step-size-condition}\)</span>，导致迭代发散。</li><li><strong>步长过小</strong>：收缩因子接近 1，每次只减少很小比例，导致达到精度需大量迭代，表现为收敛慢。</li></ul><blockquote><p><strong>这也与 <span class="math inline">\(L\)</span>-smooth 的讨论一致：若步长 <span class="math inline">\(t\)</span> 过大（甚至超过 <span class="math inline">\(\frac{1}{L}\)</span>），则上界不可信，可能导致发散；若步长过小，则每步更新幅度受限，收敛变慢。</strong></p></blockquote><p><strong>下面讨论几种常见的步长选择策略。</strong></p><h3 id="固定步长">1. 固定步长</h3><p>若已知 <span class="math inline">\(L\)</span>（Lipschitz 梯度常数），常用选择为： <span class="math display">\[\begin{align}0 &lt; t \le \frac{1}{L} \end{align}\]</span> 优点是实现简单；缺点是 <span class="math inline">\(L\)</span> 往往未知导致步长容易过大使得迭代发散，且过保守时收敛速度会显著减慢。</p><h3 id="backtracking-line-search回溯线搜索">2. Backtracking line search（回溯线搜索）</h3><p>当 <span class="math inline">\(L\)</span> 不可得时，backtracking 通过“充分下降条件”自适应选择步长。给定参数 <span class="math inline">\(0&lt;\beta&lt;1\)</span>、<span class="math inline">\(0&lt;\alpha&lt;\frac{1}{2}\)</span>，从初始步长 <span class="math inline">\(t=t_{\mathrm{init}}\)</span> 开始，每一次 <span class="math inline">\(x^+=x-t\nabla f(x)\)</span> 迭代前检查 Armijo 条件： <span class="math display">\[\begin{align}\label{eq:armijo-condition}f(x-t\nabla f(x)) &gt; f(x)-\alpha t\|\nabla f(x)\|_2^2.\end{align}\]</span> 若满足，则缩小： <span class="math display">\[\begin{align}\label{eq:backtracking-update}t \leftarrow \beta t,\end{align}\]</span> 否则按照上一步的步长更新。</p><h4 id="armijo-条件的逻辑和意义">(1) Armijo 条件的逻辑和意义</h4><p><span class="math inline">\(-\alpha t\|\nabla f(x)\|^2\)</span> 可视为一次近似预测下降量 <span class="math inline">\(-t\|\nabla f(x)\|^2\)</span> 的一个比例：将 f(x-tf(x)) 在 <span class="math inline">\(x\)</span> 一阶泰勒展开可得 <span class="math display">\[\begin{align}f(x-t\nabla f(x)) \approx f(x) - t \nabla f(x)^\top \nabla f(x) = f(x) - t \|\nabla f(x)\|_2^2.\end{align}\]</span> 条件 <span class="math inline">\(\eqref{eq:armijo-condition}\)</span> 表明，当前步长 <span class="math inline">\(t\)</span> 导致的实际下降量 <span class="math inline">\(f(x) - f(x-t\nabla f(x))\)</span> 不足以达到预测下降量 <span class="math inline">\(t\|\nabla f(x)\|_2^2\)</span> 的 <span class="math inline">\(\alpha\)</span> 比例，说明步长过大，二次近似的上界不可信，需要减小步长（<strong>有点反直觉：只盯着单步大小，忽略了“能不能稳定下降”</strong>）。</p><ul><li>先试某个较大的步长</li><li>只在失败时缩小（说明当前步长在当前点根本不合格）</li><li>一旦合格就停止缩小并更新</li></ul><h4 id="为什么取-0beta1-与-0alpha12">(3) 为什么取 <span class="math inline">\(0&lt;\beta&lt;1\)</span> 与 <span class="math inline">\(0&lt;\alpha&lt;1/2\)</span>？</h4><ul><li><span class="math inline">\(0&lt;\beta&lt;1\)</span>：保证步长缩小，有限次缩小即可进入可接受区间，从而线搜索终止。</li><li><span class="math inline">\(0&lt;\alpha&lt;1/2\)</span>：<ul><li><p>在 <span class="math inline">\(L\)</span>-smooth条件下，可证明存在足够小的步长区间使 Armijo 条件成立；取 <span class="math inline">\(\alpha&lt;1/2\)</span> 能保证 <span class="math inline">\(t \le \frac{1}{L}\)</span> 使得二次近似的上界更可靠。证明过程如下：</p><details><p> 由 <span class="math inline">\(L\)</span>-smooth 的下降引理（Descent Lemma），对任意 <span class="math inline">\(t&gt;0\)</span>， <span class="math display">\[\begin{align*}f(x_k-tg_k)\le f(x_k)-t\|g_k\|^2+\frac{L}{2}t^2\|g_k\|^2.\end{align*}\]</span> 当 <span class="math inline">\(t\le \frac{1}{L}\)</span> 时，有 <span class="math inline">\(1-\frac{Lt}{2}\ge \frac12\)</span>，从而 <span class="math display">\[\begin{align*}f(x_k-tg_k)\le f(x_k)-\frac12 t\|g_k\|^2 \le f(x_k)-\alpha t\|g_k\|^2,\end{align*}\]</span> 其中使用了 <span class="math inline">\(\alpha\le \tfrac12\)</span>。因此任意 <span class="math inline">\(t\le 1/L\)</span> 都必满足 Armijo。 回溯从 <span class="math inline">\(t_{\rm init}\)</span> 开始按 <span class="math inline">\(t\leftarrow \beta t\)</span> 缩小，有限步后必得到 <span class="math inline">\(t\le 1/L\)</span>，故终止。 设终止时得到 <span class="math inline">\(t_k\)</span>，则若 <span class="math inline">\(t_{\rm init}\le 1/L\)</span>，显然 <span class="math inline">\(t_k\ge \beta t_{\rm init}\)</span>； 若 <span class="math inline">\(t_{\rm init}&gt;1/L\)</span>，令 <span class="math inline">\(m\)</span> 为最小整数使 <span class="math inline">\(t_{\rm init}\beta^m\le 1/L\)</span>， 则 <span class="math inline">\(t_k=t_{\rm init}\beta^m &gt; \beta(1/L)\)</span>。综上 <span class="math inline">\(t_k\ge \beta\min\{t_{\rm init},1/L\}=t_{\min}&gt;0\)</span>。</p><p> Armijo 条件直接给出 <span class="math display">\[\begin{align*}f(x_{k+1})\le f(x_k)-\alpha t_k\|g_k\|^2 \le f(x_k)-\alpha t_{\min}\|g_k\|^2.\end{align*}\]</span> 两边从 <span class="math inline">\(k=0\)</span> 累加并利用 <span class="math inline">\(f(x_k)\ge f^\star\)</span> 得 <span class="math display">\[\begin{align*}\alpha t_{\min}\sum_{k=0}^{\infty}\|g_k\|^2 \le f(x_0)-f^\star&lt;\infty,\end{align*}\]</span> 故 <span class="math inline">\(\sum\|g_k\|^2&lt;\infty\)</span>，从而 <span class="math inline">\(\|g_k\|\to 0\)</span>。</p><p> 由凸性， <span class="math display">\[\begin{align*}f(x_k)-f^\star \le \langle g_k,\,x_k-x^\star\rangle \le \|g_k\|\,\|x_k-x^\star\|\le R\|g_k\|.\end{align*}\]</span> 因此 <span class="math inline">\(\|g_k\|\ge \frac{f(x_k)-f^\star}{R}\)</span>。 代入下降不等式（用 <span class="math inline">\(t_k\ge t_{\min}\)</span>）： <span class="math display">\[\begin{align*}f(x_k)-f(x_{k+1}) \ge \alpha t_{\min}\|g_k\|^2\ge \alpha t_{\min}\frac{(f(x_k)-f^\star)^2}{R^2}.\end{align*}\]</span> 令 <span class="math inline">\(\Delta_k:=f(x_k)-f^\star&gt;0\)</span>，则 <span class="math display">\[\begin{align*}\Delta_k-\Delta_{k+1}\ge \frac{\alpha t_{\min}}{R^2}\Delta_k^2\quad\Longrightarrow\quad\frac{1}{\Delta_{k+1}}-\frac{1}{\Delta_k}\ge \frac{\alpha t_{\min}}{R^2}.\end{align*}\]</span> 累加得 <span class="math inline">\(\frac{1}{\Delta_k}\ge \frac{1}{\Delta_0}+k\frac{\alpha t_{\min}}{R^2}\)</span>， 从而 <span class="math display">\[\begin{align*}\Delta_k \le \frac{R^2}{\alpha t_{\min}}\cdot \frac{1}{k}.\end{align*}\]</span></p><p> 若 <span class="math inline">\(f\)</span> 为 <span class="math inline">\(\mu\)</span>-强凸，则满足 Polyak–ojasiewicz 不等式 <span class="math display">\[\begin{align*}\|g_k\|^2 \ge 2\mu\,(f(x_k)-f^\star)=2\mu\,\Delta_k.\end{align*}\]</span> 由 Armijo 下降： <span class="math display">\[\begin{align*}\Delta_{k+1}\le \Delta_k-\alpha t_k\|g_k\|^2\le \Delta_k-2\alpha\mu t_{\min}\Delta_k=(1-2\alpha\mu t_{\min})\Delta_k,\end{align*}\]</span> 迭代即得线性收敛结论。</p></details></li><li><p>在 Lipshitz 条件下，取 <span class="math inline">\(\alpha&lt;1/2\)</span> 也能保证 <span class="math inline">\(t \ge \frac{1}{2}\)</span>，即存在足够小的步长区间使 Armijo 条件成立，证明如下：</p><details><p>XXX</p></details></li></ul></li></ul><h4 id="为什么-backtracking-对初值要求不高且看起来更快">(4) 为什么 backtracking 对初值要求不高且看起来更快？</h4><ul><li><p>固定步长梯度下降的初值敏感性体现在：若初值离最优解较远，且函数<strong>曲率</strong>变化较大（例如条件数大），则固定步长往往难以兼顾稳定性与收敛速度，容易出现发散或过慢的情况。</p></li><li><p>需要明确的是：backtracking 不改变理论收敛性能，但它能避免两类灾难：</p><ul><li>步长过大导致发散；</li><li>步长过小导致极慢。</li></ul></li></ul><hr><h2 id="四实操梯度下降求解凸优化的方法实现">四、实操：梯度下降求解凸优化的方法实现</h2><h3 id="伪代码">4.1 伪代码</h3><blockquote><p>算法1：固定步长梯度下降</p></blockquote><p><span class="math display">\[\begin{array}{l}\textbf{Algorithm 1: Fixed-Step Size Gradient Descent}\\\textbf{Input:} \ x^{(0)},\ \alpha&gt;0, \ K,\  \varepsilon&gt;0\\1:\ \textbf{for } k=0,1,\dots,K-1\ \textbf{do}\\2:\ \ \ \ g^{(k)} \leftarrow \nabla f\!\left(x^{(k)}\right)\\3:\ \ \ \ \textbf{if }\ \|g^{(k)}\|\le \varepsilon\ \textbf{then break}\\4:\ \ \ \ x^{(k+1)} \leftarrow x^{(k)}-\alpha\, g^{(k)}\\5:\ \textbf{end for}\\\textbf{Output:} \ (x^{(k)},\ f\!\left(x^{(k)}\right))\end{array}\]</span></p><blockquote><p>算法2：变步长梯度下降（Backtracking Line Search）</p></blockquote><p><span class="math display">\[\begin{array}{l}\textbf{Algorithm 2: Gradient Descent with Backtracking Line Search }\\\textbf{Input:}  x^{(0)},\ \alpha_0&gt;0, \ \beta\in(0,1),\ c\in(0,1),\ K,\ \varepsilon&gt;0\\1:\ \textbf{for } k=0,1,\dots,K-1\ \textbf{do}\\2:\ \ \ \ g^{(k)} \leftarrow \nabla f\!\left(x^{(k)}\right)\\3:\ \ \ \ \textbf{if }\ \|g^{(k)}\|\le \varepsilon\ \textbf{then break}\\4:\ \ \ \ \alpha \leftarrow \alpha_0\\5:\ \ \ \ \textbf{while }\ f\!\left(x^{(k)}-\alpha g^{(k)}\right) &gt; f\!\left(x^{(k)}\right) - c\,\alpha\,\|g^{(k)}\|^2\ \textbf{do}\\6:\ \ \ \ \ \ \alpha \leftarrow \beta\,\alpha\\7:\ \ \ \ \textbf{end while}\\8:\ \ \ \ x^{(k+1)} \leftarrow x^{(k)}-\alpha\, g^{(k)}\\9:\ \textbf{end for}\\\textbf{Output:} \ (x^{(k)},\ f\!\left(x^{(k)}\right))\end{array}\]</span></p><ul><li>backtracking line search 不改变梯度法在一般光滑凸上的收敛阶（仍为 <span class="math inline">\(O(1/k)\)</span>），但能通过自适应步长避免发散与过慢；</li><li>在强凸光滑场景下，与合理步长同量级的 backtracking 可保留线性收敛结构（常数因子层面差异）；</li></ul><h3 id="示例与演示">4.2 示例与演示</h3><p><a href="/app/gd_demo/">梯度下降对比演示（GD Demo）</a></p>]]></content>
    
    
    <categories>
      
      <category>optimization</category>
      
    </categories>
    
    
    <tags>
      
      <tag>convex set</tag>
      
      <tag>convex function</tag>
      
      <tag>affine</tag>
      
      <tag>gradient descent</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>凸优化学习系列 - 5. 次梯度法</title>
    <link href="/2026/01/26/03=Convex_Optimization/CVX-5/"/>
    <url>/2026/01/26/03=Convex_Optimization/CVX-5/</url>
    
    <content type="html"><![CDATA[<blockquote><p><strong>目标：读完这篇博客，应当能</strong></p></blockquote><ul><li>理解次梯度与次微分的必要性并会算常见例子；</li><li>学会如何用次梯度法求解凸优化问题；</li><li>知道为何次梯度法收敛慢、收敛到哪、需要什么前提条件；</li></ul><hr><h2 id="为什么需要次梯度不可微凸目标的一阶方法入口">1. 为什么需要次梯度：不可微凸目标的“一阶方法”入口</h2><p>对无约束凸优化 <span class="math display">\[\begin{align}\min_{x\in\mathbb{R}^n} f(x),\end{align}\]</span> 如果 <span class="math inline">\(f\)</span> 可微，梯度下降更新 <span class="math display">\[\begin{align}x^{(k)}=x^{(k-1)}-t_k\nabla f(x^{(k-1)}).\end{align}\]</span> 但很多凸目标 <strong>在关键点不可微</strong>（如 <span class="math inline">\(|x|\)</span>、<span class="math inline">\(\|x\|_1\)</span>、<span class="math inline">\(\max_i a_i^\top x+b_i\)</span>等）。梯度不存在，不代表“没有一阶信息”，凸性保证我们仍有 <strong>支撑超平面</strong>，从而得到次梯度。</p><p>因此：<strong>次梯度法 = 梯度下降把 <span class="math inline">\(\nabla f\)</span> 换成某个 <span class="math inline">\(g\in\partial f(x)\)</span></strong>。</p><hr><h2 id="次梯度与次微分">2. 次梯度与次微分</h2><h3 id="次梯度subgradient的定义">2.1 次梯度（subgradient）的定义</h3><p>设 <span class="math inline">\(f:\mathbb{R}^n\to\mathbb{R}\)</span> 为凸函数。向量 <span class="math inline">\(g\in\mathbb{R}^n\)</span> 称为 <span class="math inline">\(f\)</span> 在点 <span class="math inline">\(x\)</span> 的一个次梯度，如果对任意 <span class="math inline">\(y\in\mathbb{R}^n\)</span> 有 <span class="math display">\[\begin{align}f(y)\ge f(x)+g^\top(y-x).\end{align}\]</span> 这条不等式就是“在 <span class="math inline">\(x\)</span> 处用一条仿射函数从下方支撑 <span class="math inline">\(f\)</span>”。</p><h3 id="次微分subdifferential">2.2 次微分（subdifferential）</h3><p>所有次梯度的集合称为次微分： <span class="math display">\[\begin{align}\partial f(x)=\{g:\ f(y)\ge f(x)+g^\top(y-x),\ \forall y\}.\end{align}\]</span></p><h3 id="可微时的退化情形">2.3 可微时的退化情形</h3><p>若 <span class="math inline">\(f\)</span> 在 <span class="math inline">\(x\)</span> 可微，则 <span class="math display">\[\begin{align}\partial f(x)=\{\nabla f(x)\}.\end{align}\]</span> 也就是说：<strong>次梯度严格扩展了梯度</strong>。</p><hr><h2 id="几何直觉支撑超平面与斜率集合">3. 几何直觉：支撑超平面与“斜率集合”</h2><ul><li>在 1D，凸函数在不可微点（如 <span class="math inline">\(|x|\)</span> 的 0）会出现“折角”。此处不存在唯一斜率，但存在一个 <strong>斜率区间</strong>，即 <span class="math inline">\(\partial f(0)=[-1,1]\)</span>。</li><li>在高维，<span class="math inline">\(\partial f(x)\)</span> 对应所有能在 <span class="math inline">\(x\)</span> 处支撑图像的超平面的法向量集合。</li></ul><p>把“梯度是切线斜率”升级为“次梯度是支撑超平面的法向量”。</p><hr><h2 id="常见次梯度计算">4. 常见次梯度计算</h2><p>下面给出“可直接上手算”的典型例子（建议自己再推一遍定义）。</p><h3 id="绝对值与-ell_1-范数">4.1 绝对值与 <span class="math inline">\(\ell_1\)</span> 范数</h3><p>1D： <span class="math display">\[\begin{align}f(x)=|x|,\quad\partial f(x)=\begin{cases}\{1\}, &amp; x&gt;0,\\[-1,1], &amp; x=0,\\\{-1\}, &amp; x&lt;0.\end{cases}\end{align}\]</span> 向量形式： <span class="math display">\[\begin{align}f(x)=\|x\|_1=\sum_i |x_i|,\quad[\partial f(x)]_i=\begin{cases}\{1\}, &amp; x_i&gt;0,\\[-1,1], &amp; x_i=0,\\\{-1\}, &amp; x_i&lt;0.\end{cases}\end{align}\]</span></p><h3 id="最大函数max-of-convex">4.2 最大函数（max of convex）</h3><p>若 <span class="math display">\[\begin{align}f(x)=\max_{i=1,\dots,m} f_i(x),\end{align}\]</span> 其中每个 <span class="math inline">\(f_i\)</span> 凸，则有次梯度规则：在达到最大值的“活跃集合”上取次梯度并做凸包。 更具体地： <span class="math display">\[\begin{align}\partial f(x)=\mathrm{conv}\Big(\bigcup_{i\in I(x)} \partial f_i(x)\Big),\quad I(x)=\{i:\ f_i(x)=f(x)\}.\end{align}\]</span> 特别地，如果某个 <span class="math inline">\(i^\star\in I(x)\)</span>，取任意 <span class="math inline">\(g\in\partial f_{i^\star}(x)\)</span>，则 <span class="math inline">\(g\in\partial f(x)\)</span>（实现上经常这么做）。</p><p>典型：<span class="math inline">\(f(x)=\max_i (a_i^\top x+b_i)\)</span>，则在活跃的 <span class="math inline">\(i^\star\)</span> 上可取 <span class="math display">\[\begin{align}g=a_{i^\star}.\end{align}\]</span></p><h3 id="距离函数distance-to-a-convex-set">4.3 距离函数（distance to a convex set）</h3><p>距离函数 <span class="math display">\[\begin{align}\mathrm{dist}(x,C)=\min_{y\in C}\|y-x\|_2\end{align}\]</span> 在 <span class="math inline">\(x\notin C\)</span> 时可微，梯度为 <span class="math display">\[\begin{align}\nabla \mathrm{dist}(x,C)=\frac{x-P_C(x)}{\|x-P_C(x)\|_2},\end{align}\]</span> 其中 <span class="math inline">\(P_C(x)\)</span> 是到 <span class="math inline">\(C\)</span> 的欧氏投影。</p><p>这个公式在后面“集合交集/交替投影”里是核心。</p><hr><h2 id="次梯度法subgradient-method算法形式与关键差异">5. 次梯度法（Subgradient Method）：算法形式与关键差异</h2><h3 id="无约束次梯度法">5.1 无约束次梯度法</h3><p>对于 <span class="math display">\[\begin{align}\min_{x\in\mathbb{R}^n} f(x)\quad (f\ \text{凸，不要求可微}),\end{align}\]</span> 次梯度法迭代： <span class="math display">\[\begin{align}x^{(k)}=x^{(k-1)}-t_k g^{(k-1)},\quad g^{(k-1)}\in \partial f(x^{(k-1)}).\end{align}\]</span></p><h3 id="重要它不一定下降">5.2 重要：它不一定“下降”</h3><p>与梯度下降不同，次梯度法<strong>不一定每步都让 <span class="math inline">\(f(x^{(k)})\)</span> 下降</strong>。因此次梯度法强调要跟踪“best iterate”： <span class="math display">\[\begin{align}f\big(x^{(k)}_{\mathrm{best}}\big)=\min_{i=0,\dots,k} f(x^{(i)}).\end{align}\]</span> 实践中：应当输出/记录 <span class="math inline">\(x_{\mathrm{best}}\)</span> 而不是盯着最后一次迭代。</p><hr><h2 id="步长step-size怎么选三类最常用规则">6. 步长（step size）怎么选：三类最常用规则</h2><blockquote><p>次梯度法的步长通常 <strong>预先规定</strong>，而不是像光滑情形那样靠线搜索“自适应”找下降。</p></blockquote><h3 id="固定步长fixed-step">6.1 固定步长（fixed step）</h3><p><span class="math display">\[\begin{align}t_k=t.\end{align}\]</span> 结论：若 <span class="math inline">\(f\)</span> Lipschitz（常数 <span class="math inline">\(G\)</span>），则固定步长只能保证收敛到一个邻域： <span class="math display">\[\begin{align}\lim_{k\to\infty} f(x^{(k)}_{\mathrm{best}})\le f^\star+\frac{G^2 t}{2}.\end{align}\]</span> 含义：<span class="math inline">\(t\)</span> 不趋零，就无法逼近最优值到任意精度。</p><h3 id="递减步长diminishing-step">6.2 递减步长（diminishing step）</h3><p>选择 <span class="math inline">\(\{t_k\}\)</span> 满足（平方可和但不可和） <span class="math display">\[\begin{align}\sum_{k=1}^\infty t_k^2&lt;\infty,\qquad \sum_{k=1}^\infty t_k=\infty.\end{align}\]</span> 典型例子：<span class="math inline">\(t_k=\frac{c}{\sqrt{k}}\)</span>（需调 <span class="math inline">\(c\)</span>）。<br>结论：在同样 Lipschitz 条件下， <span class="math display">\[\begin{align}\lim_{k\to\infty} f(x^{(k)}_{\mathrm{best}})=f^\star.\end{align}\]</span></p><h3 id="polyak-步长知道最优值-fstar-时">6.3 Polyak 步长（知道最优值 <span class="math inline">\(f^\star\)</span> 时）</h3><p>当<strong>已知</strong>最优值 <span class="math inline">\(f^\star\)</span>（比如某些可行性问题 <span class="math inline">\(f^\star=0\)</span>），可用： <span class="math display">\[\begin{align}t_k=\frac{f(x^{(k-1)})-f^\star}{\|g^{(k-1)}\|_2^2}.\end{align}\]</span> 动机：它最小化后面“基本不等式”右侧上界。 经验：Polyak 往往比盲选递减步长稳定，但前提苛刻（要真知道 <span class="math inline">\(f^\star\)</span> 或有可靠下界）。</p><hr><h2 id="收敛性分析一条基本不等式统领全部结果">7. 收敛性分析：一条“基本不等式”统领全部结果</h2><h3 id="关键假设lipschitz等价于次梯度有界">7.1 关键假设：Lipschitz（等价于次梯度有界）</h3><p>采用假设：存在 <span class="math inline">\(G&gt;0\)</span>，使得对任意 <span class="math inline">\(x,y\)</span>： <span class="math display">\[\begin{align}|f(x)-f(y)|\le G\|x-y\|_2. \tag{Lipschitz}\end{align}\]</span> 对凸函数，这通常对应“任意 <span class="math inline">\(g\in\partial f(x)\)</span> 都满足 <span class="math inline">\(\|g\|_2\le G\)</span>”这类有界性直觉（用于把 <span class="math inline">\(\|g\|^2\)</span> 控住）。</p><h3 id="基本不等式最重要">7.2 基本不等式（最重要）</h3><p>令 <span class="math inline">\(x^\star\)</span> 为最优解之一，<span class="math inline">\(R=\|x^{(0)}-x^\star\|_2\)</span>。给出推导骨架：</p><p><strong>第一步（用次梯度定义 + 展开平方）：</strong> <span class="math display">\[\begin{align}\|x^{(k)}-x^\star\|_2^2\le\|x^{(k-1)}-x^\star\|_2^2-2t_k\big(f(x^{(k-1)})-f^\star\big)+t_k^2\|g^{(k-1)}\|_2^2.\end{align}\]</span></p><p><strong>累加迭代并用 <span class="math inline">\(\|g\|\le G\)</span>：</strong> <span class="math display">\[\begin{align}0\le R^2-2\sum_{i=1}^k t_i\big(f(x^{(i-1)})-f^\star\big)+G^2\sum_{i=1}^k t_i^2.\end{align}\]</span></p><p>引入 best iterate 并整理，得到“基本上界”： <span class="math display">\[\begin{align}f(x^{(k)}_{\mathrm{best}})-f^\star\le\frac{R^2+G^2\sum_{i=1}^k t_i^2}{2\sum_{i=1}^k t_i}.\tag{★}\end{align}\]</span></p><blockquote><p>要真正掌握次梯度法，就必须能看懂并复现这条不等式，因为：</p></blockquote><ul><li>固定步长/递减步长/Polyak 的结论都是从 (★) 直接推出的；<br></li><li>复杂问题（加约束、随机、分布式）也经常以它为模板改造。</li></ul><hr><h2 id="收敛速率为什么它本质上慢">8. 收敛速率：为什么它“本质上慢”？</h2><p>由 (★)，固定步长 <span class="math inline">\(t_k=t\)</span> 时，给出： <span class="math display">\[\begin{align}f(x^{(k)}_{\mathrm{best}})-f^\star\le\frac{R^2}{2kt}+\frac{G^2 t}{2}.\end{align}\]</span> 为了达到精度 <span class="math inline">\(\varepsilon\)</span>，令两项都 <span class="math inline">\(\le \varepsilon/2\)</span>，可取 <span class="math display">\[\begin{align}t=\frac{\varepsilon}{G^2},\qquadk=\frac{R^2G^2}{\varepsilon^2}.\end{align}\]</span> 因此次梯度法复杂度是 <span class="math display">\[\begin{align}k=O(1/\varepsilon^2),\end{align}\]</span> 明显慢于光滑凸梯度法的 <span class="math inline">\(O(1/\varepsilon)\)</span>。</p><p>更扎心的是：引用 Nesterov 的下界结论——在“仅靠次梯度 oracle 的非光滑一阶法”类里，整体上无法把最坏情形做得比 <span class="math inline">\(O(1/\varepsilon^2)\)</span> 更好（除非换问题结构或换信息模型）。</p><hr><!-- ## 9. 典型应用 1：闭凸集交集可行性（以及交替投影的由来）问题：找$$\begin{align}x^\star\in \bigcap_{i=1}^m C_i,\end{align}$$其中 $C_i$ 闭凸。定义$$\begin{align}f_i(x)=\mathrm{dist}(x,C_i),\qquadf(x)=\max_{i=1,\dots,m} f_i(x),\end{align}$$并解$$\begin{align}\min_x f(x).\end{align}$$检验：$f^\star=0 \iff x^\star\in\cap_i C_i$。因为 $f$ 是“max of convex”，可取最远集合 $C_{i^\star}$ 上的梯度作为次梯度：$$\begin{align}g=\nabla f_{i^\star}(x)=\frac{x-P_{C_{i^\star}}(x)}{\|x-P_{C_{i^\star}}(x)\|_2}.\end{align}$$若用 Polyak 步长且 $f^\star=0$，进一步得到更新：$$\begin{align}x^+=x-f(x)\,g= P_{C_{i^\star}}(x).\end{align}$$也就是：**每步直接投影到“当前最远的集合”**。特别地，当只有两个集合时，这退化为著名的 **交替投影（alternating projections）**：在 $C_1$ 与 $C_2$ 之间来回投影。---## 10. 典型应用 2：带约束的凸优化 —— 投影次梯度法（Projected Subgradient）问题形式：$$\begin{align}\min_{x\in C} f(x),\end{align}$$其中 $f$ 凸可不可微均可，$C$ 闭凸且可投影。投影次梯度法：$$\begin{align}x^{(k)}=P_C\Big(x^{(k-1)}-t_k g^{(k-1)}\Big),\quad g^{(k-1)}\in\partial f(x^{(k-1)}).\end{align}$$理解：先沿“次梯度方向”走一步，再把点拉回可行域。### 10.1 哪些集合易投影？看起来简单的多面体投影可能很难。典型易投影例子：- 非负正交锥 $\mathbb{R}^n_+$：逐分量截断 $P(x)=\max(x,0)$- $\ell_2$ 球、$\ell_\infty$ 球：都有闭式- 仿射子空间/线性方程解集：正交投影可显式写出（或解线性方程） --><hr><h2 id="会用次梯度法">9. “会用”次梯度法</h2><h3 id="必须明确的-4-个对象">9.1 必须明确的 4 个对象</h3><ol type="1"><li>目标函数 <span class="math inline">\(f(x)\)</span><br></li><li>次梯度算子：给定 <span class="math inline">\(x\)</span>，能返回一个 <span class="math inline">\(g\in\partial f(x)\)</span><br></li><li>步长规则：fixed / diminishing / Polyak<br></li><li>约束投影 <span class="math inline">\(P_C(\cdot)\)</span>（若无约束可省）</li></ol><h3 id="伪代码无约束">9.2 伪代码（无约束）</h3><blockquote><p>算法 1: 次梯度法（无约束；返回 best iterate）</p><p>输入：初值 <span class="math inline">\(x_0\)</span>；步长规则 <span class="math inline">\(\{t_k\}\)</span>（fixed / diminishing / Polyak）；最大迭代次数 <span class="math inline">\(K\)</span></p><p>输出：<span class="math inline">\(x_{\mathrm{best}}\)</span>，<span class="math inline">\(f_{\mathrm{best}}\)</span></p></blockquote><p><span class="math display">\[\begin{array}{l}\textbf{Algorithm 1: Unconstrained Subgradient Method}\\\textbf{Input:}\; x_0,\; \{t_k\},\; K \qquad1:\; x \leftarrow x_0\\2:\; x_{\mathrm{best}} \leftarrow x_0\\3:\; f_{\mathrm{best}} \leftarrow f(x_0)\\4:\; \textbf{for } k = 1,2,\ldots,K \textbf{ do}\\5:\; \qquad \text{choose any } g \in \partial f(x)\\6:\; \qquad t \leftarrow \text{stepsize}(k,x,g)\quad (\text{fixed/diminishing/Polyak})\\7:\; \qquad x \leftarrow x - t\, g\\8:\; \qquad  \textbf{if } f(x) &lt; f_{\mathrm{best}} \\9:\; \qquad\qquad x_{\mathrm{best}} \leftarrow x\\10:\; \qquad\qquad f_{\mathrm{best}} \leftarrow f(x)\\11:\; \qquad \textbf{end if}\\12:\; \textbf{end for}\\\textbf{Output:}\; (x_{\mathrm{best}}, f_{\mathrm{best}})\\[2pt]\end{array}\]</span></p><h3 id="伪代码有约束投影次梯度">9.3 伪代码（有约束➡️投影次梯度）</h3><blockquote><p>算法 2: 投影次梯度法（约束集 C；返回 best iterate）</p><p>输入：初值 <span class="math inline">\(x_0\)</span>；约束集 <span class="math inline">\(C\)</span>；步长规则 <span class="math inline">\(\{t_k\}\)</span>；最大迭代次数 <span class="math inline">\(K\)</span></p><p>输出：<span class="math inline">\(x_{\mathrm{best}}\)</span>，<span class="math inline">\(f_{\mathrm{best}}\)</span></p></blockquote><p><span class="math display">\[\begin{array}{l}\textbf{Algorithm 2: Projected Subgradient Method }\\\textbf{Input:}\; x_0,\; C,\; \{t_k\},\; K \\1:\; x \leftarrow x_0\\2:\; x_{\mathrm{best}} \leftarrow x_0\\3:\; f_{\mathrm{best}} \leftarrow f(x_0)\\4:\; \textbf{for } k = 1,2,\ldots,K \textbf{ do}\\5:\; \qquad \text{choose any } g \in \partial f(x)\\6:\; \qquad t \leftarrow \text{stepsize}(k,x,g)\\7:\; \qquad x \leftarrow \mathrm{Proj}_C(x - t\, g)\\8:\; \qquad \textbf{if } f(x) &lt; f_{\mathrm{best}} \textbf{ then}\\9:\; \qquad\qquad x_{\mathrm{best}} \leftarrow x\\10:\; \qquad\qquad f_{\mathrm{best}} \leftarrow f(x)\\11:\; \qquad \textbf{end if}\\12:\; \textbf{end for}\\13:\; \textbf{Output:}\; (x_{\mathrm{best}}, f_{\mathrm{best}})\\[2pt]\end{array}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>optimization</category>
      
    </categories>
    
    
    <tags>
      
      <tag>convex set</tag>
      
      <tag>convex function</tag>
      
      <tag>affine</tag>
      
      <tag>subgradient</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>凸优化学习系列 - 3. 一阶方法</title>
    <link href="/2026/01/26/03=Convex_Optimization/CVX-3/"/>
    <url>/2026/01/26/03=Convex_Optimization/CVX-3/</url>
    
    <content type="html"><![CDATA[<table><tbody><tr class="odd"><td><a href="https://www.stat.cmu.edu/~ryantibs/convexopt/">CMU 课程10-725-2019</a>的 <a href="https://www.stat.cmu.edu/~ryantibs/convexopt/scribes/convex-opt-scribed.pdf">Lecture 3</a> 和 <a href="https://www.stat.cmu.edu/~ryantibs/convexopt/scribes/canonical-probs-scribed.pdf">Lecture 4</a> 分别介绍了 <code>问题怎么写才是凸的（如何构造凸问题）</code> 以及 <code>某些变换后的问题为什么还是凸的（凸性保持）</code>，作为<strong>背景/预备知识</strong>，暂时不作深入讨论，遇到时再补充。</td></tr><tr class="even"><td>这个文档基于 <a href="https://www.stat.cmu.edu/~ryantibs/convexopt/">CMU 课程10-725-2019</a> 的 Lecture 5-9，整理和总结凸优化问题的一阶求解方法，求解原理、收敛性与复杂度，并给出对比分析。</td></tr></tbody></table><h1 id="一阶方法总结">一阶方法总结</h1><blockquote><p><strong><code>一阶</code>是指求解过程中只用到目标函数的一阶信息，即 <span class="math inline">\(\nabla f\)</span>，而不适用二阶信息（如 Hessian 矩阵 <span class="math inline">\(\nabla^2 f\)</span>）。一阶方法通常每步计算开销较低，适合大规模问题，但收敛速度相对较慢。</strong></p></blockquote><h2 id="梯度下降gradient-descent-gd">1. 梯度下降（Gradient Descent, GD）</h2><p><strong>问题设定</strong>： 最小化光滑凸函数 <span class="math inline">\(f(x)\)</span>，假设 <span class="math inline">\(\nabla f\)</span> 为 <span class="math inline">\(L\)</span>-Lipschitz。</p><p><strong>迭代格式</strong>： <span class="math display">\[\begin{align} x^{(k)} = x^{(k-1)} - t_k \nabla f(x^{(k-1)}), \quad k=1,2,\dots\end{align}\]</span></p><p><strong>收敛性与速度</strong>：</p><ul><li>若 <span class="math inline">\(\nabla f\)</span> 为 <span class="math inline">\(L\)</span>-Lipschitz，则函数值收敛率为 <span class="math inline">\(O(1/k)\)</span>，达到 <span class="math inline">\(\varepsilon\)</span> 精度需 <span class="math inline">\(O(1/\varepsilon)\)</span> 次迭代。</li><li>若 <span class="math inline">\(f\)</span> 进一步 <span class="math inline">\(m\)</span>-强凸，则线性收敛： <span class="math display">\[\begin{align}f(x^{(k)}) - f^* = O(\gamma^k), \quad 0&lt;\gamma&lt;1\end{align}\]</span> 因而迭代复杂度为 <span class="math inline">\(O(\log(1/\varepsilon))\)</span>。</li><li>条件数 <span class="math inline">\(L/m\)</span> 越大，收敛越慢。</li></ul><p><strong>复杂度</strong>：每步一次梯度计算（成本依赖问题规模）。</p><p><strong>优缺点</strong>：</p><ul><li>优：简单、每步便宜；对良好条件数的强凸问题很快。</li><li>缺：要求可微；对病态问题慢。</li></ul><hr><h2 id="次梯度法subgradient-method">2. 次梯度法（Subgradient Method）</h2><p><strong>次梯度定义</strong>（用于非光滑凸函数）： <span class="math display">\[\begin{align} f(y) \ge f(x) + g^T (y-x), \quad g \in \partial f(x)\end{align}\]</span></p><p><strong>迭代格式</strong>： <span class="math display">\[\begin{align} x^{(k)} = x^{(k-1)} - t_k g^{(k-1)}, \quad g^{(k-1)} \in \partial f(x^{(k-1)})\end{align}\]</span> 并用当前最优函数值的迭代点 <span class="math inline">\(x_{\text{best}}^{(k)}\)</span> 作为输出。</p><p><strong>收敛性与速度</strong>（<span class="math inline">\(f\)</span> 为 <span class="math inline">\(G\)</span>-Lipschitz）： - 固定步长 <span class="math inline">\(t\)</span>： <span class="math display">\[\begin{align}  \lim_{k\to\infty} f(x_{\text{best}}^{(k)}) \le f^* + \frac{G^2 t}{2}  \end{align}\]</span> - 递减步长（平方可和但不可和）：<span class="math inline">\(f(x_{\text{best}}^{(k)}) \to f^*\)</span>。 - 收敛率：<span class="math inline">\(O(1/\varepsilon^2)\)</span>（明显慢于 GD 的 <span class="math inline">\(O(1/\varepsilon)\)</span>）。</p><p><strong>复杂度</strong>：每步一次次梯度计算；不保证单调下降。</p><p><strong>优缺点</strong>：</p><ul><li>优：可处理不可微凸问题，适用面广。</li><li>缺：收敛慢；对步长较敏感。</li></ul><hr><h2 id="近端梯度下降proximal-gradient-descent-pgd">3. 近端梯度下降（Proximal Gradient Descent, PGD）</h2><p><strong>问题设定</strong>：复合目标 <span class="math inline">\(f(x)=g(x)+h(x)\)</span>，其中 <span class="math inline">\(g\)</span> 光滑凸、<span class="math inline">\(h\)</span> 凸且可计算近端算子。</p><p><strong>近端映射</strong>： <span class="math display">\[\begin{align} \mathrm{prox}_{h,t}(x)=\arg\min_z \; \frac{1}{2t}\|x-z\|_2^2 + h(z)\end{align}\]</span></p><p><strong>迭代格式</strong>： <span class="math display">\[\begin{align} x^{(k)} = \mathrm{prox}_{h,t_k}\big(x^{(k-1)}-t_k \nabla g(x^{(k-1)})\big)\end{align}\]</span></p><p><strong>收敛性与速度</strong>： - 若 <span class="math inline">\(\nabla g\)</span> 为 <span class="math inline">\(L\)</span>-Lipschitz，则 <span class="math display">\[\begin{align}  f(x^{(k)})-f^* = O(1/k)  \end{align}\]</span> 迭代复杂度 <span class="math inline">\(O(1/\varepsilon)\)</span>。</p><p><strong>复杂度</strong>：每步一次梯度 + 一次 prox 计算（若 prox 便宜则总体高效）。</p><p><strong>优缺点</strong>：</p><ul><li>优：可处理常见非光滑正则（如 <span class="math inline">\(\ell_1\)</span>、核范数），保留 GD 的速率。</li><li>缺：prox 若代价高会成为瓶颈；误差近端会影响速率。</li></ul><hr><h2 id="加速近端梯度accelerated-pgd-nesterov-加速">4. 加速近端梯度（Accelerated PGD / Nesterov 加速）</h2><p><strong>核心思想</strong>：在 PGD 上加入动量外推；当 <span class="math inline">\(h=0\)</span> 时退化为加速梯度法。</p><p><strong>典型迭代</strong>（讲义中的形式）： <span class="math display">\[\begin{align} v^{(k)} &amp;= x^{(k-1)} + \frac{k-2}{k+1}\big(x^{(k-1)}-x^{(k-2)}\big) \\ x^{(k)} &amp;= \mathrm{prox}_{h,t_k}\big(v^{(k)}-t_k \nabla g(v^{(k)})\big)\end{align}\]</span></p><p><strong>收敛性与速度</strong>： - 函数值收敛率达到 <span class="math inline">\(O(1/k^2)\)</span>（一阶方法最优）。 - 相应的精度复杂度为 <span class="math inline">\(O(1/\sqrt{\varepsilon})\)</span>。</p><p><strong>复杂度</strong>：每步一次梯度 + 一次 prox；但并非单调下降。</p><p><strong>优缺点</strong>：</p><ul><li>优：显著加速，理论最优速率。</li><li>缺：对步长/动量更敏感；不一定单调下降。</li></ul><hr><h2 id="随机梯度下降stochastic-gradient-descent-sgd">5. 随机梯度下降（Stochastic Gradient Descent, SGD）</h2><p><strong>问题设定</strong>：最小化均值型目标 <span class="math inline">\(f(x)=\frac{1}{m}\sum_{i=1}^m f_i(x)\)</span>。</p><p><strong>迭代格式</strong>： <span class="math display">\[\begin{align} x^{(k)} = x^{(k-1)} - t_k \nabla f_{i_k}(x^{(k-1)})\end{align}\]</span> 其中 <span class="math inline">\(i_k\)</span> 为随机抽样索引，<span class="math inline">\(\mathbb{E}[\nabla f_{i_k}(x)] = \nabla f(x)\)</span>。</p><p><strong>收敛性与速度</strong>：</p><ul><li>对凸函数、递减步长： <span class="math display">\[\begin{align}\mathbb{E}[f(x^{(k)})]-f^* = O(1/k)\end{align}\]</span></li><li>即使强凸+Lipschitz 梯度，SGD 仍只有 <span class="math inline">\(O(1/k)\)</span> 的期望收敛，不具备 GD 的线性收敛。</li></ul><p><strong>复杂度</strong>：</p><ul><li>一次 batch 梯度：<span class="math inline">\(O(np)\)</span>；</li><li>一次随机梯度：<span class="math inline">\(O(p)\)</span>；</li><li>mini-batch 大小 <span class="math inline">\(b\)</span>：<span class="math inline">\(O(bp)\)</span>。</li></ul><p><strong>优缺点</strong>：</p><ul><li>优：单步开销小，适合大规模数据。</li><li>缺：噪声导致收敛慢；需谨慎设置步长/停止策略。</li></ul><hr><h2 id="方法性质对比">6. 方法性质对比</h2><table><thead><tr class="header"><th style="text-align: center;">方法</th><th style="text-align: center;">目标函数<span class="math inline">\(f\)</span></th><th style="text-align: center;">每步计算</th><th style="text-align: center;">典型收敛率（<span class="math inline">\(f\)</span>一般凸）</th><th style="text-align: center;">主要优点</th><th style="text-align: center;">主要缺点</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">GD</td><td style="text-align: center;">光滑凸</td><td style="text-align: center;">1 次全梯度</td><td style="text-align: center;"><span class="math inline">\(O(1/k)\)</span></td><td style="text-align: center;">简单、稳定</td><td style="text-align: center;">需可微</td></tr><tr class="even"><td style="text-align: center;">Subgradient</td><td style="text-align: center;">非光滑凸</td><td style="text-align: center;">1 次次梯度</td><td style="text-align: center;"><span class="math inline">\(O(1/\sqrt{k})\)</span></td><td style="text-align: center;">目标函数无需可微</td><td style="text-align: center;">收敛慢</td></tr><tr class="odd"><td style="text-align: center;">PGD</td><td style="text-align: center;">复合凸</td><td style="text-align: center;">1 梯度 + 1 prox</td><td style="text-align: center;"><span class="math inline">\(O(1/k)\)</span></td><td style="text-align: center;">目标函数无需可微</td><td style="text-align: center;">prox 可能昂贵</td></tr><tr class="even"><td style="text-align: center;">Acc-PGD</td><td style="text-align: center;">复合凸</td><td style="text-align: center;">1 梯度 + 1 prox</td><td style="text-align: center;"><span class="math inline">\(O(1/k^2)\)</span></td><td style="text-align: center;">理论最优速率</td><td style="text-align: center;">参数敏感、非单调</td></tr><tr class="odd"><td style="text-align: center;">SGD</td><td style="text-align: center;">均值目标</td><td style="text-align: center;">1 次随机梯度</td><td style="text-align: center;"><span class="math inline">\(O(1/\sqrt{k})\)</span></td><td style="text-align: center;">适合大数据</td><td style="text-align: center;">噪声、收敛慢</td></tr></tbody></table><hr><h2 id="总结">7. 总结</h2><ul><li>小中规模、光滑凸：优先 GD；若强凸且条件数好，GD 收敛很快。</li><li>非光滑凸：若有可计算的 prox，优先 PGD/Acc-PGD；否则用次梯度法。</li><li>大规模数据：SGD 是首选，尤其当全梯度代价高时。</li></ul>]]></content>
    
    
    <categories>
      
      <category>optimization</category>
      
    </categories>
    
    
    <tags>
      
      <tag>convex set</tag>
      
      <tag>convex function</tag>
      
      <tag>affine</tag>
      
      <tag>gradient descent</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>凸优化学习系列 - 6. 近端梯度下降法</title>
    <link href="/2026/01/26/03=Convex_Optimization/CVX-6/"/>
    <url>/2026/01/26/03=Convex_Optimization/CVX-6/</url>
    
    <content type="html"><![CDATA[<blockquote><p><strong>目标：读完这篇博客，应当能</strong></p></blockquote><ul><li>理解近端梯度的必要性以及作用机制；</li><li>学会如何用近端梯度法求解复合凸优化问题；</li><li>知道近端梯度法的收敛速率及其与标准梯度下降的关系；</li><li>掌握加速近端梯度法的原理与实现；</li></ul><hr><h2 id="问题背景为什么要引入近端">1. 问题背景：为什么要引入“近端”？</h2><p>上一讲常见的工具是 <strong>次梯度法（subgradient method）</strong>：对一般凸函数 <span class="math display">\[\begin{align}\min_x f(x)\end{align}\]</span> 用更新 <span class="math display">\[\begin{align}x^{(k)} = x^{(k-1)} - t_k g^{(k-1)},\quad g^{(k-1)}\in \partial f(x^{(k-1)}).\end{align}\]</span> 它的优点是通用；缺点是<strong>慢</strong>：若 <span class="math inline">\(f\)</span> Lipschitz，则典型复杂度是 <span class="math inline">\(O(1/\varepsilon^2)\)</span>（或写作 <span class="math inline">\(O(1/\sqrt{k})\)</span> 的函数值误差）。</p><p>现实里大量问题不是“完全不可微”，而是“<strong>一部分光滑 + 一部分非光滑</strong>”。次梯度法把结构浪费掉了，于是近端梯度出现。</p><hr><h2 id="复合凸优化近端梯度要解决的标准形式">2. 复合凸优化：近端梯度要解决的标准形式</h2><p>近端梯度针对 <strong>复合目标（composite objective）</strong>： <span class="math display">\[\begin{align}\min_{x\in\mathbb{R}^n} f(x)=g(x)+h(x),\end{align}\]</span> 满足：</p><ul><li><span class="math inline">\(g\)</span>：凸、可微，且 <span class="math inline">\(\mathrm{dom}(g)=\mathbb{R}^n\)</span>；</li><li><span class="math inline">\(h\)</span>：凸，但<strong>可以不可微</strong>（如 <span class="math inline">\(\ell_1\)</span>、核范数、指标函数等）；</li><li>关键结构：<strong>只有 <span class="math inline">\(g\)</span> 需要梯度</strong>，而 <span class="math inline">\(h\)</span> 通过一个特殊算子处理。</li></ul><p>你应该立刻意识到：这覆盖了“带正则项”“带凸约束”的大量模型。</p><hr><h2 id="从梯度下降的动机出发为什么是二次近似-保留-h">3. 从梯度下降的动机出发：为什么是“二次近似 + 保留 <span class="math inline">\(h\)</span>”？</h2><h3 id="光滑情形的梯度下降来自二次上界二次模型">3.1 光滑情形的梯度下降来自“二次上界/二次模型”</h3><p>若 <span class="math inline">\(f\)</span> 可微，常见动机是：在当前点 <span class="math inline">\(x\)</span> 处，用二次函数近似（或上界） <span class="math inline">\(f\)</span>： <span class="math display">\[\begin{align}\bar f_t(z)= f(x)+\nabla f(x)^\top(z-x)+\frac{1}{2t}\|z-x\|_2^2,\end{align}\]</span> 然后令 <span class="math display">\[\begin{align}x^+ = \arg\min_z \bar f_t(z),\end{align}\]</span> 可推出 <span class="math inline">\(x^+=x-t\nabla f(x)\)</span>。</p><h3 id="复合情形的关键想法只近似-g把-h-原样保留">3.2 复合情形的关键想法：只近似 <span class="math inline">\(g\)</span>，把 <span class="math inline">\(h\)</span> 原样保留</h3><p>当 <span class="math inline">\(f=g+h\)</span> 且 <span class="math inline">\(h\)</span> 可能不可微时，直接做 <span class="math inline">\(\nabla f\)</span> 不现实。 但我们可以对 <strong>光滑部分 <span class="math inline">\(g\)</span></strong> 做二次近似，对 <strong>非光滑部分 <span class="math inline">\(h\)</span></strong> 不动： <span class="math display">\[\begin{align}x^+ = \arg\min_z \Big[g(x)+\nabla g(x)^\top(z-x)+\frac{1}{2t}\|z-x\|_2^2 + h(z)\Big].\end{align}\]</span> 把与 <span class="math inline">\(z\)</span> 无关的常数去掉、配方可得等价形式： <span class="math display">\[\begin{align}x^+ = \arg\min_z\ \frac{1}{2t}\Big\|z-\big(x-t\nabla g(x)\big)\Big\|_2^2 + h(z).\end{align}\]</span> 这句式非常重要：</p><ul><li>第一项逼迫 <span class="math inline">\(z\)</span> 靠近“<strong>对 <span class="math inline">\(g\)</span> 做梯度一步</strong>”得到的点 <span class="math inline">\(x-t\nabla g(x)\)</span>；</li><li>第二项逼迫 <span class="math inline">\(h(z)\)</span> 小（体现正则化/约束）。</li></ul><hr><h2 id="核心定义近端映射proximal-mapping">4. 核心定义：近端映射（proximal mapping）</h2><h3 id="近端算子定义">4.1 近端算子定义</h3><p>对凸函数 <span class="math inline">\(h\)</span>，步长 <span class="math inline">\(t&gt;0\)</span>，定义 <span class="math display">\[\begin{align}\mathrm{prox}_{h,t}(x)=\arg\min_z \frac{1}{2t}\|x-z\|_2^2 + h(z).\end{align}\]</span> 这叫 <strong>近端映射 / 近端算子</strong>。</p><p><strong>直觉</strong>：它是在“离 <span class="math inline">\(x\)</span> 不要太远”的前提下，让 <span class="math inline">\(h\)</span> 尽量小。<br>因此它常被理解为“对 <span class="math inline">\(h\)</span> 做一次带二次正则的局部最小化”。</p><h3 id="近端梯度下降pgd的迭代公式">4.2 近端梯度下降（PGD）的迭代公式</h3><p>把上一节的更新写成 prox 形式： <span class="math display">\[\begin{align}x^{(k)}=\mathrm{prox}_{h,t_k}\big(x^{(k-1)}-t_k\nabla g(x^{(k-1)})\big),\quad k=1,2,\dots\end{align}\]</span> 这就是 <strong>Proximal Gradient Descent (PGD)</strong>。</p><hr><h2 id="广义梯度视角它看起来更像梯度下降">5. “广义梯度”视角：它看起来更像梯度下降</h2><p>定义 <strong>广义梯度（generalized gradient）/梯度映射（gradient mapping）</strong> <span class="math display">\[\begin{align}G_t(x)=\frac{x-\mathrm{prox}_{h,t}(x-t\nabla g(x))}{t}.\end{align}\]</span> 则迭代可写成 <span class="math display">\[\begin{align}x^{(k)}=x^{(k-1)}-t_k\,G_{t_k}(x^{(k-1)}).\end{align}\]</span> 当 <span class="math inline">\(h=0\)</span> 时，<span class="math inline">\(\mathrm{prox}_{h,t}(y)=y\)</span>，所以 <span class="math inline">\(G_t(x)=\nabla g(x)\)</span>，退化为标准梯度下降。</p><p>这解释了“为什么近端梯度仍然是一阶法”：它每步只用到 <span class="math inline">\(\nabla g\)</span> 与一次 prox。</p><hr><h2 id="prox-会不会比原问题更难">6. prox 会不会比原问题更难？</h2><p>关键点：很多重要的 <span class="math inline">\(h\)</span>，其 <span class="math inline">\(\mathrm{prox}_{h,t}\)</span> <strong>有闭式解或高效实现</strong>，且 prox 只依赖于 <span class="math inline">\(h\)</span>，与 <span class="math inline">\(g\)</span> 无关。</p><p>因此典型分工是：</p><ul><li><span class="math inline">\(g\)</span>：可能很复杂，但只需要算梯度；</li><li><span class="math inline">\(h\)</span>：设计成“prox 好算”的正则/约束（<span class="math inline">\(\ell_1\)</span>、指标函数、核范数等）。</li></ul><hr><!-- ## 7. 例子 1：Lasso 与 ISTA（软阈值）### 7.1 Lasso 的复合结构给定 $y\in\mathbb{R}^n$, $X\in\mathbb{R}^{n\times p}$，Lasso：$$\begin{align}\min_\beta f(\beta)=\underbrace{\frac12\|y-X\beta\|_2^2}_{g(\beta)}+\underbrace{\lambda\|\beta\|_1}_{h(\beta)}.\end{align}$$其中$$\begin{align}\nabla g(\beta)=-X^\top(y-X\beta).\end{align}$$### 7.2 $\ell_1$ 的 prox 是软阈值（soft-thresholding）$$\begin{align}\mathrm{prox}_{t}(\beta)=\arg\min_z \frac{1}{2t}\|\beta-z\|_2^2+\lambda\|z\|_1= S_{\lambda t}(\beta),\end{align}$$其中软阈值算子 $S_\lambda(\cdot)$ 按坐标定义：$$\begin{align}[S_\lambda(\beta)]_i=\begin{cases}\beta_i-\lambda, & \beta_i>\lambda,\\0, & -\lambda\le \beta_i\le \lambda,\\\beta_i+\lambda, & \beta_i<-\lambda.\end{cases}\end{align}$$### 7.3 ISTA 更新公式（即 PGD 应用于 Lasso）$$\begin{align}\beta^+ = S_{\lambda t}\big(\beta+tX^\top(y-X\beta)\big).\end{align}$$这常称为 **ISTA（Iterative Soft-Thresholding Algorithm）**。> 关键理解：一步 ISTA = “对光滑最小二乘做梯度一步” + “对 $\ell_1$ 做软阈值修正”。 --><hr><h2 id="步长怎么选固定步长与回溯线搜索backtracking">7. 步长怎么选：固定步长与回溯线搜索（Backtracking）</h2><h3 id="固定步长的理论条件">7.1 固定步长的理论条件</h3><p>若 <span class="math inline">\(\nabla g\)</span> 是 <span class="math inline">\(L\)</span>-Lipschitz 连续（即 <span class="math inline">\(\|\nabla g(x)-\nabla g(y)\|\le L\|x-y\|\)</span>），则 PGD 取 <span class="math display">\[\begin{align}t\le \frac{1}{L}\end{align}\]</span> 可保证标准收敛性质。</p><h3 id="回溯线搜索对-g-而不是对-f">7.2 回溯线搜索（对 <span class="math inline">\(g\)</span> 而不是对 <span class="math inline">\(f\)</span>）</h3><p>回溯准则：取 <span class="math inline">\(0&lt;\beta&lt;1\)</span>，每次从 <span class="math inline">\(t=t_{\text{init}}\)</span> 开始，如果 <span class="math display">\[\begin{align}g\big(x-tG_t(x)\big)\ &gt;\ g(x)-t\nabla g(x)^\top G_t(x) + \frac{t}{2}\|G_t(x)\|_2^2,\end{align}\]</span> 则缩小 <span class="math inline">\(t\leftarrow \beta t\)</span>，直到满足为止，再做一次 prox 更新。</p><blockquote><p>注意：这里判断的是 <span class="math inline">\(g\)</span> 的充分下降/二次上界条件，而不是直接判断 <span class="math inline">\(f\)</span>。这是复合结构下的关键差异。</p></blockquote><hr><h2 id="pgd-的收敛率它能达到梯度下降同级别的-o1k">8. PGD 的收敛率：它能达到梯度下降同级别的 <span class="math inline">\(O(1/k)\)</span></h2><p>在假设： - <span class="math inline">\(g\)</span> 凸可微、<span class="math inline">\(\nabla g\)</span> 为 <span class="math inline">\(L\)</span>-Lipschitz； - <span class="math inline">\(h\)</span> 凸，且 <span class="math inline">\(\mathrm{prox}_{h,t}\)</span> 可计算；</p><p>则 <strong>定理（PGD 收敛）</strong>：若固定步长 <span class="math inline">\(t\le 1/L\)</span>，则 <span class="math display">\[\begin{align}f(x^{(k)})-f^\star \le \frac{\|x^{(0)}-x^\star\|_2^2}{2tk}.\end{align}\]</span> 回溯线搜索也成立，但常数上用 <span class="math inline">\(t=\beta/L\)</span> 类似替换。</p><p>因此 PGD 具有 - 迭代复杂度：<span class="math inline">\(f(x^{(k)})-f^\star = O(1/k)\)</span>； - 误差 <span class="math inline">\(\varepsilon\)</span> 下的迭代次数：<span class="math inline">\(k=O(1/\varepsilon)\)</span>。</p><p>这比次梯度法的 <span class="math inline">\(O(1/\varepsilon^2)\)</span> 要快一个量级（在同样的一阶框架内）。</p><hr><!-- ## 10. 例子 2：矩阵补全（Matrix Completion）与“矩阵软阈值”### 10.1 模型：观测子集上的平方损失 + 核范数正则给定 $Y\in\mathbb{R}^{m\times n}$，只观测 $\Omega$ 中的条目，解：$$\begin{align}\min_B \frac12\sum_{(i,j)\in\Omega}(Y_{ij}-B_{ij})^2+\lambda\|B\|_{\mathrm{tr}},\end{align}$$其中核范数（trace/nuclear norm）$$\begin{align}\|B\|_{\mathrm{tr}}=\sum_{i=1}^r \sigma_i(B).\end{align}$$定义投影算子 $P_\Omega$：$$\begin{align}[P_\Omega(B)]_{ij}=\begin{cases}B_{ij},&(i,j)\in\Omega,\\0,&(i,j)\notin\Omega.\end{cases}\end{align}$$则目标可写为$$\begin{align}f(B)=\underbrace{\frac12\|P_\Omega(Y)-P_\Omega(B)\|_F^2}_{g(B)}+\underbrace{\lambda\|B\|_{\mathrm{tr}}}_{h(B)}.\end{align}$$### 10.2 两个必需部件：梯度与 prox- 梯度：$$\begin{align}\nabla g(B)=-(P_\Omega(Y)-P_\Omega(B)).\end{align}$$- prox：$$\begin{align}\mathrm{prox}_{t}(B)=\arg\min_Z \frac{1}{2t}\|B-Z\|_F^2+\lambda\|Z\|_{\mathrm{tr}}.\end{align}$$结论：它等于“**对奇异值做软阈值**”（矩阵软阈值 / SVT）：若 $B=U\Sigma V^\top$ 为 SVD，则$$\begin{align}S_\lambda(B)=U\Sigma_\lambda V^\top,\quad (\Sigma_\lambda)_{ii}=\max\{\Sigma_{ii}-\lambda,0\}.\end{align}$$并且$$\begin{align}\mathrm{prox}_{t}(B)=S_{\lambda t}(B).\end{align}$$### 10.3 更新：Soft-Impute因此 PGD 更新为$$\begin{align}B^+=S_{\lambda t}\Big(B+t(P_\Omega(Y)-P_\Omega(B))\Big).\end{align}$$并且这里 $\nabla g$ 的 Lipschitz 常数 $L=1$，可取 $t=1$，得到更简洁形式$$\begin{align}B^+=S_{\lambda}\big(P_\Omega(Y)+P_{\Omega^\perp}(B)\big),\end{align}$$这就是经典的 **soft-impute** 思路。 --><hr><h2 id="三个特殊情形你应该把-prox-当作统一抽象">9. 三个“特殊情形”：你应该把 prox 当作统一抽象</h2><p>PGD 也叫 composite/generalized gradient descent，因为很多方法都是它的特例：</p><ol type="1"><li><p><strong><span class="math inline">\(h=0\)</span></strong>：<br><span class="math display">\[\begin{align}x^+=x-t\nabla g(x)\end{align}\]</span> 退化为梯度下降。</p></li><li><p><strong><span class="math inline">\(h=I_C\)</span></strong>（指标函数，表示约束 <span class="math inline">\(x\in C\)</span>）：<br><span class="math display">\[\begin{align}I_C(x)=\begin{cases}0,&amp;x\in C,\\\infty,&amp;x\notin C,\end{cases}\end{align}\]</span> 则 <span class="math display">\[\begin{align}\mathrm{prox}_{I_C,t}(x)=\arg\min_{z\in C}\|x-z\|_2^2 = P_C(x),\end{align}\]</span> 于是 <span class="math display">\[\begin{align}x^+=P_C\big(x-t\nabla g(x)\big),\end{align}\]</span> 这就是 <strong>投影梯度下降（Projected GD）</strong>。</p></li><li><p><strong><span class="math inline">\(g=0\)</span></strong>：<br><span class="math display">\[\begin{align}x^+=\mathrm{prox}_{h,t}(x)\end{align}\]</span> 称为 <strong>近端最小化/近端点法</strong> 的基本迭代（但只有当 prox 易算时才可用）。</p></li></ol><blockquote><p>统一视角的价值：当你看到一个问题“约束 + 正则 + 光滑损失”，第一反应应该是：能不能写成 <span class="math inline">\(g+h\)</span>，并让 <span class="math inline">\(h\)</span> 的 prox 易算？</p></blockquote><hr><h2 id="如果-prox-不能精确算怎么办inexact-prox">10. 如果 prox 不能精确算怎么办？（inexact prox）</h2><p>理论通常假设 prox 子问题能<strong>精确</strong>求解： <span class="math display">\[\begin{align}\mathrm{prox}_{h,t}(x)=\arg\min_z \frac{1}{2t}\|x-z\|^2+h(z).\end{align}\]</span> 提醒：一般情况下，近似求解会怎样不一定清楚；但如果你能严格控制近似误差，则仍可恢复原收敛率（有相关文献分析）。实践上，若用近似 prox，应保证足够高精度。</p><hr><h2 id="加速把-o1k-提升到最优的-o1k2">11. 加速：把 <span class="math inline">\(O(1/k)\)</span> 提升到最优的 <span class="math inline">\(O(1/k^2)\)</span></h2><h3 id="动机nesterov-加速思想在复合问题上同样可用">11.1 动机：Nesterov 加速思想在复合问题上同样可用</h3><p>回顾 Nesterov 的多条加速线索，并采用 Beck &amp; Teboulle (2008) 的形式：对复合 <span class="math inline">\(g+h\)</span> 只需每步 <strong>一次 prox</strong>，并只用最近两步信息。</p><h3 id="加速近端梯度法apgd的迭代">11.2 加速近端梯度法（APGD）的迭代</h3><p>初始化 <span class="math inline">\(x^{(0)}=x^{(-1)}\)</span>。对 <span class="math inline">\(k=1,2,\dots\)</span>： <span class="math display">\[\begin{align}v = x^{(k-1)}+\frac{k-2}{k+1}\big(x^{(k-1)}-x^{(k-2)}\big),\end{align}\]</span> <span class="math display">\[\begin{align}x^{(k)}=\mathrm{prox}_{h,t_k}\big(v-t_k\nabla g(v)\big).\end{align}\]</span> 解释： - 当 <span class="math inline">\(k=1\)</span> 时，<span class="math inline">\(\frac{k-2}{k+1}=-\frac{1}{2}\)</span>，但通常第一步就是普通 PGD（说明“第一步就是 usual proximal gradient update”）； - 之后 <span class="math inline">\(v\)</span> 是带“动量”的外推点，利用历史方向加速。</p><p>重要提醒：<strong>加速方法不一定是下降法</strong>（目标值可能不单调）。</p><h3 id="加速的收敛定理达到最优-o1k2">11.3 加速的收敛定理：达到最优 <span class="math inline">\(O(1/k^2)\)</span></h3><p>在与 PGD 相同的假设下，若固定步长 <span class="math inline">\(t\le 1/L\)</span>，则： <span class="math display">\[\begin{align}f(x^{(k)})-f^\star \le \frac{2\|x^{(0)}-x^\star\|_2^2}{t(k+1)^2}.\end{align}\]</span> 因此 - 函数值误差 <span class="math inline">\(O(1/k^2)\)</span>； - 达到 <span class="math inline">\(\varepsilon\)</span> 误差所需迭代数 <span class="math inline">\(k=O(1/\sqrt{\varepsilon})\)</span>，这是一般一阶方法可达的最优数量级。</p><!-- ## 14. FISTA：加速版本的 ISTA（Lasso 的“快”算法）对 Lasso，ISTA 为$$\begin{align}\beta^{(k)}=S_{\lambda t_k}\Big(\beta^{(k-1)}+t_kX^\top(y-X\beta^{(k-1)})\Big).\end{align}$$加速后得到 **FISTA**：对 $k=1,2,\dots$$$\begin{align}v = \beta^{(k-1)}+\frac{k-2}{k+1}\big(\beta^{(k-1)}-\beta^{(k-2)}\big),\end{align}$$$$\begin{align}\beta^{(k)}=S_{\lambda t_k}\Big(v+t_kX^\top(y-Xv)\Big).\end{align}$$经验上加速常显著提速（给出多组对比曲线）。 --><hr><h2 id="回溯线搜索在加速情形下简单但可能导致步长递减">12. 回溯线搜索在加速情形下：简单但可能导致步长递减</h2><p>简单策略：取 <span class="math inline">\(\beta&lt;1\)</span>，<span class="math inline">\(t_0=1\)</span>。第 <span class="math inline">\(k\)</span> 步从 <span class="math inline">\(t=t_{k-1}\)</span> 开始，若 <span class="math display">\[\begin{align}g(x^+) &gt; g(v)+\nabla g(v)^\top(x^+-v)+\frac{1}{2t}\|x^+-v\|_2^2,\end{align}\]</span> 则缩小 <span class="math inline">\(t\leftarrow \beta t\)</span>，并令 <span class="math display">\[\begin{align}x^+=\mathrm{prox}_{h,t}(v-t\nabla g(v)).\end{align}\]</span> 但它会“强迫步长递减”，更复杂的策略可以避免该问题。</p><hr><h2 id="实战警告加速回溯并非总是更好">13. 实战警告：加速/回溯并非总是更好</h2><p>在后面非常直接地提醒两类常见“反效果”：</p><h3 id="warm-start-下加速收益会被削弱">13.1 warm start 下加速收益会被削弱</h3><p>例如要解一系列 Lasso，<span class="math inline">\(\lambda_1&gt;\lambda_2&gt;\dots&gt;\lambda_r\)</span>，每次用上一次解作初值： - 解 <span class="math inline">\(\lambda_1\)</span>：从 0 开始； - 解 <span class="math inline">\(\lambda_j\)</span>：从 <span class="math inline">\(\hat x(\lambda_{j-1})\)</span> 开始； 当 <span class="math inline">\(\lambda\)</span> 网格足够密，普通 PGD 可能已经很快，加速额外收益变小。</p><h3 id="当-prox-很贵如需-svd时回溯加速可能更慢">13.2 当 prox 很贵（如需 SVD）时，回溯/加速可能更慢</h3><p>矩阵补全中 prox 需要 SVD： - 回溯一次要多次 prox ⇒ 多次 SVD ⇒ 成本爆炸； - 加速改变 prox 的输入点结构，可能让 SVD 失去“低秩 + 稀疏”的可利用结构，从而更慢。</p><blockquote><p>结论：是否加速/是否回溯，不是信仰问题，是“每步代价 × 迭代数”的工程权衡。</p></blockquote><hr><h2 id="如何用-pgdapgd-求解凸优化">14. 如何用 PGD/APGD 求解凸优化？</h2><h3 id="步骤总结">14.1 步骤总结</h3><ol type="1"><li><strong>拆分：写成 <span class="math inline">\(f=g+h\)</span></strong><br></li></ol><ul><li>把“可微的损失”放进 <span class="math inline">\(g\)</span>；</li><li>把“不可微正则/简单凸约束”放进 <span class="math inline">\(h\)</span>。</li></ul><ol start="2" type="1"><li><strong>确认：<span class="math inline">\(\nabla g\)</span> 是否 Lipschitz（或能否回溯）</strong><br></li></ol><ul><li>若能估计 <span class="math inline">\(L\)</span>，直接取 <span class="math inline">\(t\le 1/L\)</span>；</li><li>否则用回溯线搜索（注意 prox 的调用成本）。</li></ul><ol start="3" type="1"><li><strong>计算两件事</strong><br></li></ol><ul><li><span class="math inline">\(\nabla g(x)\)</span>；</li><li><span class="math inline">\(\mathrm{prox}_{h,t}(\cdot)\)</span>（最好闭式/高效）。<ul><li><span class="math inline">\(\ell_1\)</span>：软阈值；</li><li>指标函数 <span class="math inline">\(I_C\)</span>：投影 <span class="math inline">\(P_C\)</span>；</li><li>核范数：奇异值软阈值（SVT）。</li></ul></li></ul><ol start="4" type="1"><li><strong>选算法</strong><br></li></ol><ul><li>先用 PGD（稳定、常单调更好调试）；</li><li>再尝试 APGD（更快但不单调、对“每步代价结构”更敏感）。</li></ul><ol start="5" type="1"><li><strong>停止准则建议</strong><br>理论常用 <span class="math inline">\(f(x^{(k)})-f^\star\)</span>，但 <span class="math inline">\(f^\star\)</span> 不可得。可用：</li></ol><ul><li>相对变化：<span class="math inline">\(\frac{|f(x^{(k)})-f(x^{(k-1)})|}{\max\{1,|f(x^{(k-1)})|\}} \le \tau\)</span>；</li><li>迭代差：<span class="math inline">\(\frac{\|x^{(k)}-x^{(k-1)}\|}{\max\{1,\|x^{(k-1)}\|\}} \le \tau\)</span>；</li><li>或广义梯度 <span class="math inline">\(\|G_t(x^{(k)})\|\)</span> 小（若可算）。</li></ul><hr><h2 id="伪代码总结">14.2 伪代码总结</h2><blockquote><p><strong>算法 1： 近端梯度下降（PGD）</strong></p></blockquote><p><span class="math display">\[\begin{array}{l}\textbf{Algorithm 1: Proximal Gradient Descent}\\\textbf{Input: } x^{(0)}\in\mathbb{R}^n;\ t&gt;0\ (\text{or }t_0&gt;0,\beta\in(0,1)\text{ for backtracking});\ K.\\1:\ \textbf{for } k=1,2,\ldots,K\ \textbf{do}\\2:\ \ \ \ \textbf{(optional backtracking)}\ \text{set } t\leftarrow t_0\ \text{(or reuse last }t\text{)}\\3:\ \ \ \ \textbf{repeat}\\4:\ \ \ \ \ \ \ y \leftarrow x^{(k-1)}-t\,\nabla g\!\left(x^{(k-1)}\right)\\5:\ \ \ \ \ \ \ x_{\text{trial}} \leftarrow \mathrm{prox}_{h,t}(y)\\6:\ \ \ \ \ \ \ \textbf{if } g(x_{\text{trial}})\le g(x^{(k-1)})+\nabla g(x^{(k-1)})^\top(x_{\text{trial}}-x^{(k-1)})+\frac{1}{2t}\|x_{\text{trial}}-x^{(k-1)}\|_2^2\\7:\ \ \ \ \ \ \ \textbf{then break}\\8:\ \ \ \ \ \ \ \textbf{else } t\leftarrow \beta t\\9:\ \ \ \ \textbf{until } \text{condition holds}\\10:\ \ \ \ x^{(k)} \leftarrow x_{\text{trial}}\\11:\ \ \ \ \textbf{(stopping)}\ \textbf{if }\frac{\|x^{(k)}-x^{(k-1)}\|_2}{\max\{1,\|x^{(k-1)}\|_2\}}\le \varepsilon\ \textbf{then break}\\12:\ \textbf{end for}\\\textbf{Output: } x^{(k)}\ (\text{final iterate}),\ f(x^{(k)}).\\[2mm]\end{array}\]</span></p><blockquote><p><strong>算法 2：加速近端梯度下降（APGD）</strong></p></blockquote><p><span class="math display">\[\begin{array}{l}\textbf{Algorithm 2: Accelerated Proximal Gradient Descent}\\\textbf{Input: } x^{(0)}\in\mathbb{R}^n;\ x^{(-1)}=x^{(0)};\ t&gt;0\ (\text{or }t_0&gt;0,\beta\in(0,1)\text{ for backtracking});\ K.\\1:\ \textbf{for } k=1,2,\ldots,K\ \textbf{do}\\2:\ \ \ \ \alpha_k \leftarrow \frac{k-2}{k+1}\qquad(\text{momentum parameter})\\3:\ \ \ \ v^{(k-1)} \leftarrow x^{(k-1)}+\alpha_k\big(x^{(k-1)}-x^{(k-2)}\big)\qquad(\text{extrapolation})\\4:\ \ \ \ \textbf{(optional backtracking)}\ \text{set } t\leftarrow t_0\ \text{(or reuse last }t\text{)}\\5:\ \ \ \ \textbf{repeat}\\6:\ \ \ \ \ \ \ y \leftarrow v^{(k-1)}-t\,\nabla g\!\left(v^{(k-1)}\right)\\7:\ \ \ \ \ \ \ x_{\text{trial}} \leftarrow \mathrm{prox}_{h,t}(y)\\8:\ \ \ \ \ \ \ \textbf{if } g(x_{\text{trial}})\le g(v^{(k-1)})+\nabla g(v^{(k-1)})^\top(x_{\text{trial}}-v^{(k-1)})+\frac{1}{2t}\|x_{\text{trial}}-v^{(k-1)}\|_2^2\\9:\ \ \ \ \ \ \ \textbf{then break}\\10:\ \ \ \ \ \ \ \textbf{else } t\leftarrow \beta t\\11:\ \ \ \ \textbf{until } \text{condition holds}\\12:\ \ \ \ x^{(k)} \leftarrow x_{\text{trial}}\\13:\ \ \ \ \textbf{(stopping)}\ \textbf{if }\frac{\|x^{(k)}-x^{(k-1)}\|_2}{\max\{1,\|x^{(k-1)}\|_2\}}\le \varepsilon\ \textbf{then break}\\14:\ \textbf{end for}\\\textbf{Output: } x^{(k)}\ (\text{final iterate}),\ f(x^{(k)}).\\[2mm]\end{array}\]</span></p><hr>]]></content>
    
    
    <categories>
      
      <category>optimization</category>
      
    </categories>
    
    
    <tags>
      
      <tag>convex set</tag>
      
      <tag>convex function</tag>
      
      <tag>affine</tag>
      
      <tag>proximal gradient descent</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>凸优化学习系列 - 1. 基本概念</title>
    <link href="/2026/01/26/03=Convex_Optimization/CVX-1/"/>
    <url>/2026/01/26/03=Convex_Optimization/CVX-1/</url>
    
    <content type="html"><![CDATA[<p>从【<a href="https://www.stat.cmu.edu/~ryantibs/convexopt/">CMU 课程，10-725, 2019</a>】出发，这一系列的文档用于整理/讨论凸优化问题相关的基本概念和重要求解方法，主要包括（暂定）：</p><ul><li>凸优化问题的重要概念（什么是凸优化问题，为什么要做凸优化）</li><li>一阶求解方法<details><ul><li>梯度下降法（Gradient Descent）</li><li>子梯度方法（Subgradient Method）</li><li>近端梯度下降法（Proximal Gradient Method）</li><li>随机梯度下降法（Stochastic Gradient Descent）</li></ul></details></li><li>对偶理论和最优理论<details><ul><li>线性/非线性对偶理论</li><li>KKT 条件（Karush-Kuhn-Tucker 条件）</li><li>对偶性应用</li></ul></details></li><li>二阶求解方法<details><ul><li>牛顿法</li><li>屏障法（Barrier Method）</li><li>原始对偶内点法（Primal-Dual Interior Point Method）</li><li>类牛顿法（Quasi-Newton Method）</li></ul></details></li><li>凸优化问题的前沿理论<details><ul><li>数值线性代数（Numerical linear algebra）</li><li>坐标下降法（Coordinate descent）</li><li>对偶分解（Dual decomposition）</li><li>乘子交替方向法（Alternating direction method of multipliers, ADMM）</li><li>条件梯度法（Frank-Wolfe method）</li><li>现代随机方法（Modern stochastic methods）</li><li>一阶非凸优化（First-order nonconvex optimization）</li><li>Bregman 近端方法（Bregman proximal methods）</li></ul></details></li></ul><p><strong>这个文档简要介绍凸优化问题的重要概念。</strong></p><h1 id="基础预备知识">1、基础预备知识</h1><h2 id="凸集compact-set">1.1 凸集（Compact set）</h2><div class="note note-info">            <p><strong>定义 1</strong>：对任意 <span class="math inline">\(x, y \in C \subseteq \mathbb{R}^n\)</span> 和任意 <span class="math inline">\(\lambda \in [0,1]\)</span>，有 <span class="math display">\[\begin{align}\lambda x + (1-\lambda) y \in C\end{align}\]</span> 则称集合 <span class="math inline">\(C\)</span> 是凸集。</p>          </div><p>从几何角度来看，凸集的定义可以理解为：对于集合 <span class="math inline">\(C\)</span> 中的任意两点 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(y\)</span>，连接 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(y\)</span> 的<strong>线段</strong>也完全包含在集合 <span class="math inline">\(C\)</span> 中。</p><h2 id="仿射集affine-set">1.2 仿射集（Affine set）</h2><div class="note note-info">            <p><strong>定义 2</strong>：对任意 <span class="math inline">\(x, y \in A \subseteq \mathbb{R}^n\)</span> 和任意 <span class="math inline">\(\theta \in \mathbb{R}\)</span>，有 <span class="math display">\[\begin{align}\theta x + (1-\theta) y \in A\end{align}\]</span> 则称集合 <span class="math inline">\(A\)</span> 是仿射集。</p>          </div><p>从几何角度来看，仿射集的定义可以理解为：对于集合 <span class="math inline">\(A\)</span> 中的任意两点 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(y\)</span>，连接 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(y\)</span> 的<strong>直线</strong>也完全包含在集合 <span class="math inline">\(A\)</span> 中。</p><h2 id="凸函数convex-function">1.3 凸函数（Convex function）</h2><div class="note note-info">            <p><strong>定义 3</strong>：设 <span class="math inline">\(C \subseteq \mathbb{R}^n\)</span> 是一个凸集，函数 <span class="math inline">\(f: C \to \mathbb{R}\)</span> 称为凸函数，如果对任意 <span class="math inline">\(x, y \in C\)</span> 和任意 <span class="math inline">\(\lambda \in [0,1]\)</span>，有 <span class="math display">\[\begin{align}f(\lambda x + (1-\lambda) y) \leq \lambda f(x) + (1-\lambda) f(y)\end{align}\]</span></p>          </div><p>从几何角度来看，凸函数的定义可以理解为：对于函数 <span class="math inline">\(f\)</span> 的任意两点 <span class="math inline">\((x, f(x))\)</span> 和 <span class="math inline">\((y, f(y))\)</span>，连接这两点的<strong>线段</strong>位于函数图像的<strong>上方</strong>。</p><ul><li>严格凸函数（strictly convex）：<span class="math inline">\(f(\lambda x + (1-\lambda) y) &lt; \lambda f(x) + (1-\lambda) f(y)\)</span>；</li><li><span class="math inline">\(m\)</span>- 强凸函数（strongly convex）：存在常数 <span class="math inline">\(m&gt;0\)</span>，使得 <span class="math inline">\(f(x) - \frac{m}{2}\|x\|_2^2\)</span> 是凸函数。</li></ul><blockquote><p>强凸 <span class="math inline">\(\Rightarrow\)</span> 严格凸 <span class="math inline">\(\Rightarrow\)</span> 凸。</p></blockquote><h2 id="仿射函数affine-function">1.4 仿射函数（Affine function）</h2><div class="note note-info">            <p><strong>定义 4</strong>：函数 <span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> 称为仿射函数，如果存在向量 <span class="math inline">\(a \in \mathbb{R}^n\)</span> 和标量 <span class="math inline">\(b \in \mathbb{R}\)</span>，使得对任意 <span class="math inline">\(x \in \mathbb{R}^n\)</span> 和 <span class="math inline">\(\theta \in \mathbb{R}\)</span>，有 <span class="math display">\[\begin{align}f\bigl(\theta x + (1-\theta)y\bigr) = \theta f(x) + (1-\theta)f(y)\end{align}\]</span></p>          </div><p>从几何角度来看，仿射函数的定义可以理解为：对于函数 <span class="math inline">\(f\)</span> 的任意两点 <span class="math inline">\((x, f(x))\)</span> 和 <span class="math inline">\((y, f(y))\)</span>，连接这两点的<strong>直线</strong>位于函数图像上。常见的仿射函数形式为 <span class="math inline">\(f(x) = a^T x + b\)</span>。</p><h1 id="凸优化问题">2、凸优化问题</h1><h1 id="为什么要做凸优化why">2.1、为什么要做凸优化（Why）</h1><p>为什么不能做非凸优化？ 最小化问题为什么不能用其他理论结构？</p><h1 id="凸优化问题的标准形式what">2.2、凸优化问题的标准形式（What）</h1><div class="note note-success">            <p>典型的凸优化问题可以表示为以下标准形式： <span class="math display">\[\begin{align}    \min_{x \in D} \quad &amp; f(x) \\    \text{subject to} \quad &amp;     \begin{cases}    g_i(x) \le 0, \quad i = 1, \cdots, m \\    h_j(x) = 0, \quad j = 1, \cdots, p    \end{cases}\end{align}\]</span></p>          </div><p>其中：</p><ul><li><span class="math inline">\(D\)</span> 为满足约束条件且 <span class="math inline">\(f,g_i,h_j\)</span> 有意义的集合，称为<strong>可行域</strong>；</li><li>目标函数 <span class="math inline">\(f: D \to \mathbb{R}\)</span> 是<strong>凸函数（优化目标）</strong>；</li><li>变量 <span class="math inline">\(x \in \mathbb{R}^n\)</span> 是需要优化的参数（<strong>决策变量</strong>）；</li><li>等式约束函数 <span class="math inline">\(h_i: D \to \mathbb{R}\)</span> 是仿射函数（<strong>等式约束</strong>）；</li><li>不等式约束函数 <span class="math inline">\(g_j: D \to \mathbb{R}\)</span> 是凸函数（<strong>不等式约束</strong>）。</li></ul><div class="note note-warning">            <p><strong>问题一：为什么不等式约束 <span class="math inline">\(g_i\)</span> 是凸函数？</strong></p><ul><li>不等式约束 <span class="math inline">\(g_i(x) \le 0\)</span>：<strong><code>为了保证可行域是凸集</code></strong>，必须要求 <span class="math inline">\(g_i\)</span> 是凸函数；因为凸函数的<strong>次水平集</strong>（sublevel set）<span class="math inline">\(\{x | g_i(x) \le 0\}\)</span> 是凸集，证明如下：<ul><li>对任意 <span class="math inline">\(x, y\)</span> 满足 <span class="math inline">\(g_i(x) \le 0\)</span> 和 <span class="math inline">\(g_i(y) \le 0\)</span>，以及任意 <span class="math inline">\(\lambda \in [0,1]\)</span>，有 <span class="math display">\[\begin{align}g_i(\lambda x + (1-\lambda) y) &amp;\leq \lambda g_i(x) + (1-\lambda) g_i(y) \quad (\text{因为 } g_i \text{ 是凸函数}) \\&amp;\leq \lambda \cdot 0 + (1-\lambda) \cdot 0 = 0\end{align}\]</span></li><li>因此，<span class="math inline">\(\{x | g_i(x) \le 0\}\)</span> 是凸集。</li></ul></li></ul><p><strong>问题二：为什么等式约束 <span class="math inline">\(h_j\)</span> 是仿射函数？</strong></p><ul><li>等式约束 <span class="math inline">\(h_j(x) = 0\)</span>：<strong><code>为了保证可行域是凸集</code></strong>，必须要求 <span class="math inline">\(h_j\)</span> 是仿射函数；因为仿射函数的<strong>零水平集</strong>（zero level set）<span class="math inline">\(\{x | h_j(x) = 0\}\)</span> 是凸集，证明如下：<ul><li>对任意 <span class="math inline">\(x, y\)</span> 满足 <span class="math inline">\(h_j(x) = 0\)</span> 和 <span class="math inline">\(h_j(y) = 0\)</span>，以及任意 <span class="math inline">\(\theta \in \mathbb{R}\)</span>，有 <span class="math display">\[\begin{align}h_j(\theta x + (1-\theta) y) &amp;= \theta h_j(x) + (1-\theta) h_j(y) \quad (\text{因为 } h_j \text{ 是仿射函数}) \\&amp;= \theta \cdot 0 + (1-\theta) \cdot 0 = 0\end{align}\]</span></li><li>因此，<span class="math inline">\(\{x | h_j(x) = 0\}\)</span> 是凸集。</li><li>另一方面。如果 <span class="math inline">\(h_j(x)\)</span> 不是仿射函数，如 <span class="math inline">\(h(x) = x^2 - 1\)</span>，<span class="math inline">\(h(x) = 0\)</span> 的解集为 <span class="math inline">\(\{-1, 1\}\)</span>，显然不是凸集。</li></ul></li></ul><p><strong>问题三：为什么可行域必须是凸集？</strong></p><p><strong>问题四：为什么目标函数必须是凸的？</strong></p>          </div><h1 id="凸优化问题解的存在性和唯一性">2.3、凸优化问题解的存在性和唯一性</h1><h2 id="什么叫凸优化问题有解">2.3.1、什么叫凸优化问题有解？</h2><p>针对凸优化问题</p><div class="note note-info">            <p><strong>定义 5</strong>：设 <span class="math inline">\(D \subseteq \mathbb{R}^n\)</span> 是非空凸集，函数 <span class="math inline">\(f: D \to \mathbb{R}\)</span> 是凸函数。若存在 <span class="math inline">\(x^\star \in D\)</span> 使得 <span class="math display">\[\begin{align}f(x^\star) = \min_{x \in D} f(x),\end{align}\]</span> 则称 <span class="math inline">\(x^\star\)</span> 是凸优化问题的一个<strong>最优解</strong>，且称该凸优化问题<strong>有解</strong>。</p>          </div><blockquote><p>也就是说，有可行域 + 可达到最优值 = 有解。</p></blockquote><h2 id="解的存在性">2.3.2、解的存在性</h2><div class="note note-info">            <p><strong>定理 1（Weierstrass 极值定理）</strong>：设 <span class="math inline">\(D \subseteq \mathbb{R}^n\)</span> 是非空凸集，函数 <span class="math inline">\(f: D \to \mathbb{R}\)</span> 在 <span class="math inline">\(D\)</span> 是<strong>适当的（proper）</strong>、<strong>下半连续的（lower semicontinuous, I.s.c）</strong>凸函数。若 <span class="math inline">\(f\)</span> 的<strong>下水平集非空且有界（lower level-bounded）</strong>，则 <span class="math inline">\(\arg\min_{x \in D} f(x) \neq \emptyset\)</span>。</p>          </div><blockquote><p>1、什么是 <code>proper</code>？</p><p>函数 <span class="math inline">\(f: D \to \mathbb{R}\)</span> 是适当的（proper）：如果： （1）它在 <span class="math inline">\(D\)</span> 上不恒等于 <span class="math inline">\(+\infty\)</span>：例子：non-proper 函数：<span class="math inline">\(f(x) = +\infty， \forall x \in D\)</span>； （2）对任意 <span class="math inline">\(x \in D\)</span>，有 <span class="math inline">\(f(x) &gt; -\infty\)</span>；例子：non-proper 函数：<span class="math inline">\(f(0) = -\infty\)</span>，$ f(x) &gt; -$, $x  $。</p><p><code>proper</code> ➡️ 函数值不是总是无穷大，且没有点取到负无穷。 <!-- 目标函数是一个有意义的讨论对象，如果硬要定义函数在某些点取到 $-\infty$，那这个优化问题就没有讨论的意义了。 --></p><p>2、什么是 <code>lower semicontinuous</code>？</p><p>函数 <span class="math inline">\(f: D \to \mathbb{R}\)</span> 是下半连续的（lower semicontinuous, I.s.c）：如果 <span class="math inline">\(\forall x\)</span>，<span class="math inline">\(\liminf_{y \to x} f(y) \ge f(x)\)</span>。</p><p>也可以等价描述为：如果对任意 $ $，集合 <span class="math inline">\(\{x \in D | f(x) \leq \alpha\}\)</span> 是<code>闭集</code>（若任何收敛到 z 的序列<span class="math inline">\(\{z_k\} \subseteq M\)</span> 的极限仍然属于 <span class="math inline">\(M\)</span>，则称 <span class="math inline">\(M\)</span> 是闭集。例如 <span class="math inline">\([0,1]\)</span> 是闭集，因为 <span class="math inline">\(z_k = \frac{1}{k} \in [0,1]\)</span> 且极限 <span class="math inline">\(\lim_{k \in + \infty} z_k = 0 \in [0,1]\)</span>）；<span class="math inline">\((0,1]\)</span> 不是闭集,因为 <span class="math inline">\(\lim_{k \in + \infty} z_k = 0 \notin (0,1]\)</span>。 例如： <span class="math inline">\(f(x) = \begin{cases} 0, x = 0 \\ 1 , x \ne 0 \end{cases}\)</span> 满足 <span class="math inline">\(\liminf_{x \to 0} f = 0 &lt; 1\)</span> (i.s.c)；而 <span class="math inline">\(f(x) = \begin{cases} 1, x = 0 \\ 0 , x \ne 0 \end{cases}\)</span> <span class="math inline">\(\liminf_{x \to 0} f = 1 &gt; 0\)</span> (非 i.s.c)。</p><p><code>lower semicontinuous</code> ➡️ 允许函数值在极限处“跳高”，但不允许“跳低”。</p><p><strong>注意这里有一个坑：连续函数一定是下半连续函数！而下半连续函数不一定是连续函数！这里的下半连续只是一个更弱的条件！（不要被<code>连续</code>二字误导！！！）</strong></p><p>3、什么是 <code>lower level-bounded</code>？</p><p>函数 <span class="math inline">\(f: D \to \mathbb{R}\)</span> 的下水平集非空且有界（lower level-bounded）：给定 <span class="math inline">\(\alpha \in \mathbb{R}\)</span>，集合 <span class="math inline">\(\{x \in D | f(x) \leq \alpha\}\)</span> 是<code>非空有界集</code>。</p><p>这个条件和 i.s.c 条件结合起来，可以保证给定常数 <span class="math inline">\(\beta \in {\mathbb R}\)</span>，<span class="math inline">\(f(x)\)</span> 的下水平集 <span class="math inline">\(L_a = \{x \in D | f(x) \leq \beta\}\)</span> 是非空有界的闭集，即紧集。紧集里任意序列都有收敛子序列：存在 <span class="math inline">\(\{x_{k_k}\} \to x^\star \in L_a \subset D\)</span>。紧集里的每个点都能取到，从而保证最优解 <span class="math inline">\(x^\star\)</span> 能被取到。</p><p>I.s.c 给出： <span class="math inline">\(f( x^\star ) \le \liminf_{j \to \infty} f(x_{k_j}) = f^\star\)</span>，且 <span class="math inline">\(f^\star \le f(x^\star)\)</span> 总成立，所以 <span class="math inline">\(f(x^\star) = f^\star\)</span>，即 <span class="math inline">\(x^\star\)</span> 是最优解。</p></blockquote><blockquote><p>⚠️⚠️⚠️：这三个条件都是<strong>充分不必要条件</strong>，即：满足这些条件时，凸优化问题一定有解；但凸优化问题有解时，不一定满足这些条件。因此，上述内容其实讨论的是：<strong>有意义的凸优化问题</strong>的解的存在性。（上述内容还有待完善）</p></blockquote><h2 id="解的唯一性">2.3.3、解的唯一性</h2><div class="note note-info">            <p><strong>定理 2</strong>：在定理 1 的基础上，若函数 <span class="math inline">\(f: D \to \mathbb{R}\)</span> 在 <span class="math inline">\(D\)</span> 上严格凸，则存在唯一的 <span class="math inline">\(x^\star \in D\)</span> 使得 <span class="math display">\[\begin{align}f(x^\star) = \min_{x \in D} f(x).\end{align}\]</span></p>          </div><h1 id="凸优化问题的重要性质">2.4、凸优化问题的重要性质</h1><ul><li><strong>性质一</strong>：在一个凸可行域 <span class="math inline">\(D\)</span>上最小化一个凸函数 <span class="math inline">\(f\)</span> 时，<strong>任何局部极小点都是全局极小点</strong>（注意，这里的全局指的是整个凸可行域 <span class="math inline">\(D\)</span>，而不是 <span class="math inline">\(\mathbb{R}^n\)</span>）。对于凸优化问题(5)，该性质可用数学语言描述为：<ul><li><p><strong>如果</strong>存在 <span class="math inline">\(x^\star \in D\)</span> 以及常数 <span class="math inline">\(\rho &gt; 0\)</span> 满足 <span class="math inline">\(\|x^\star - y\|_2 \le \rho\)</span>, <span class="math inline">\(\forall y \in D\)</span>，使得： <span class="math display">\[\begin{align}       f(x^\star) \le f(y), \quad \forall y \in D \text{ 且 } \|x^\star - y\|_2 \le \rho    \end{align}\]</span></p></li><li><p><strong>则</strong> <span class="math inline">\(x^\star\)</span> 也是全局最优解，即：<span class="math inline">\(f(x^\star) \le f(y), \forall y \in D\)</span>。</p></li><li><p>这个性质可以用反证法证明，过程如下：</p><details><p>证明：假设结论不成立，则 <span class="math inline">\(x^\star\)</span> 不是全局最小点。于是存在某个可行点 <span class="math inline">\(\bar y\in D\)</span> 使得 <span class="math display">\[\begin{align}f(\bar y)&lt;f(x^\star).\end{align}\]</span> 由于 <span class="math inline">\(D\)</span> 是凸集，故对任意 <span class="math inline">\(\theta\in[0,1]\)</span>，点 <span class="math display">\[\begin{align}z(\theta) := (1-\theta)x^\star+\theta \bar y\end{align}\]</span> 仍属于 <span class="math inline">\(D\)</span>。</p><p>又由于 <span class="math inline">\(f\)</span> 是凸函数，故对任意 <span class="math inline">\(\theta\in[0,1]\)</span> 有 <span class="math display">\[\begin{align}f\big(z(\theta)\big)\le (1-\theta)f(x^\star)+\theta f(\bar y).\end{align}\]</span> 结合 <span class="math inline">\(f(\bar y)&lt;f(x^\star)\)</span>，可得当任意 <span class="math inline">\(\theta\in(0,1]\)</span> 时 <span class="math display">\[\begin{align}(1-\theta)f(x^\star)+\theta f(\bar y)= f(x^\star)+\theta\big(f(\bar y)-f(x^\star)\big)&lt; f(x^\star),\end{align}\]</span> 从而 <span class="math display">\[\begin{align}f\big(z(\theta)\big)&lt;f(x^\star),\quad \forall\,\theta\in(0,1].\end{align}\]</span> 另一方面，注意到 <span class="math display">\[\begin{align}\|z(\theta)-x^\star\|_2&amp;= \|(1-\theta)x^\star+\theta \bar y-x^\star\|_2= \|\theta(\bar y-x^\star)\|_2= \theta\|\bar y-x^\star\|_2.\end{align}\]</span> 因此只要取 <span class="math display">\[\begin{align}0&lt;\theta\le \min\left\{1,\ \frac{\rho}{\|\bar y-x^\star\|_2}\right\},\end{align}\]</span> 就有 <span class="math inline">\(\|z(\theta)-x^\star\|_2\le \rho\)</span>，且 <span class="math inline">\(z(\theta)\in D\)</span>。 于是由“局部最小”的假设应当满足 <span class="math display">\[\begin{align}f(x^\star)\le f\big(z(\theta)\big).\end{align}\]</span> 但上面已经得到 <span class="math inline">\(f\big(z(\theta)\big)&lt;f(x^\star)\)</span>，矛盾。</p><p>故反设不成立，结论成立，即 <span class="math display">\[\begin{align}f(x^\star)\le f(y),\quad \forall\,y\in D.\end{align}\]</span> 这说明 <span class="math inline">\(x^\star\)</span> 是全局最小点。</p></details></li></ul></li><li><strong>性质二</strong>：如果目标函数是严格凸函数（严格不等号），则全局最优解唯一。对于凸优化问题(5)，该性质可用数学语言描述为：<ul><li><p><strong>如果</strong> 凸优化问题 (5) 存在全局最优解，且目标函数 <span class="math inline">\(f\)</span> 是严格凸函数，</p></li><li><p><strong>则</strong>凸优化问题 (5) 的全局最优解唯一。</p></li><li><p>这个形式也可以用反证法证明，过程如下：</p><details><p>证明：严格凸的定义为：对任意不同的 <span class="math inline">\(x_1,x_2\in D\)</span> 以及任意 <span class="math inline">\(\theta\in(0,1)\)</span>，有 <span class="math display">\[\begin{align}f(\theta x_1+(1-\theta)x_2) &lt; \theta f(x_1)+(1-\theta)f(x_2).\end{align}\]</span> 下面用反证法证明唯一性。假设存在两个不同的全局最优解 <span class="math inline">\(x_1^\star\neq x_2^\star\)</span>，并记最优值为 <span class="math display">\[\begin{align}f^\star := f(x_1^\star)=f(x_2^\star)=\inf_{x\in D} f(x).\end{align}\]</span> 由于 <span class="math inline">\(D\)</span> 是凸集，对任意 <span class="math inline">\(\theta\in(0,1)\)</span>，点 <span class="math display">\[\begin{align}x_\theta := \theta x_1^\star+(1-\theta)x_2^\star\end{align}\]</span> 仍属于 <span class="math inline">\(D\)</span>。由严格凸性得 <span class="math display">\[\begin{align}f(x_\theta)&lt; \theta f(x_1^\star)+(1-\theta)f(x_2^\star)= \theta f^\star+(1-\theta)f^\star= f^\star.\end{align}\]</span> 这意味着存在可行点 <span class="math inline">\(x_\theta\in D\)</span> 使得 <span class="math inline">\(f(x_\theta)&lt;f^\star\)</span>，与 <span class="math inline">\(f^\star\)</span> 为全局最小值矛盾。</p><p>因此不可能存在两个不同的全局最优解，故全局最优解唯一。</p></details></li></ul></li><li><strong>性质三</strong>：凸优化问题的解集为凸集。对于凸优化问题(5)，该性质可用数学语言描述为：<ul><li><p><strong>如果</strong> 凸优化问题 (5) 存在解集 <span class="math inline">\(\mathcal X_{\mathrm{opt}}\)</span>，</p></li><li><p><strong>则</strong> 解集 <span class="math inline">\(\mathcal X_{\mathrm{opt}}\)</span> 是凸集。</p></li><li><p>这个性质可以直接从凸函数和凸集的定义推导出来，证明如下：</p><details><p>证明：任取 <span class="math inline">\(x_1, x_2 \in {\mathcal X}_{\mathrm{opt}}\)</span>，则 <span class="math inline">\(x_1,x_2 \in D\)</span> 且 <span class="math display">\[\begin{align}f(x_1)=f(x_2)=f^\star.\end{align}\]</span> 对任意 <span class="math inline">\(\theta\in[0,1]\)</span>，令 <span class="math display">\[\begin{align}x_\theta:=\theta x_1+(1-\theta)x_2.\end{align}\]</span> 由于 <span class="math inline">\(D\)</span> 是凸集，有 <span class="math inline">\(x_\theta\in D\)</span>。</p><p>又由于 <span class="math inline">\(f\)</span> 是凸函数，得到 <span class="math display">\[\begin{align}f(x_\theta)\le \theta f(x_1)+(1-\theta)f(x_2)= \theta f^\star+(1-\theta)f^\star=f^\star.\end{align}\]</span> 另一方面，由 <span class="math inline">\(f^\star\)</span> 是 <span class="math inline">\(D\)</span> 上的下确界（最优值），对所有 <span class="math inline">\(x\in D\)</span> 均有 <span class="math inline">\(f(x)\ge f^\star\)</span>， 特别地 <span class="math display">\[\begin{align}f(x_\theta)\ge f^\star.\end{align}\]</span> 综合两式可得 <span class="math inline">\(f(x_\theta)=f^\star\)</span>，因此 <span class="math inline">\(x_\theta\in \mathcal X_{\mathrm{opt}}\)</span>。</p><p>故对任意 <span class="math inline">\(x_1,x_2\in\mathcal X_{\mathrm{opt}}\)</span> 及任意 <span class="math inline">\(\theta\in[0,1]\)</span>，均有 <span class="math inline">\(\theta x_1+(1-\theta)x_2\in\mathcal X_{\mathrm{opt}}\)</span>，即 <span class="math inline">\(\mathcal X_{\mathrm{opt}}\)</span> 为凸集。</p></details></li></ul></li></ul><div class="note note-primary">            <p><strong>问题五：性质一有什么作用？如何体现在凸优化问题中？</strong></p><blockquote><p><strong>有待后续讨论</strong></p></blockquote><p><strong>问题六：性质二有什么作用？如何体现在凸优化问题中？</strong></p><blockquote><p><strong>有待后续讨论</strong></p></blockquote><p><strong>问题七：性质三有什么作用？如何体现在凸优化问题中？</strong></p><blockquote><p><strong>有待后续讨论</strong></p></blockquote>          </div>]]></content>
    
    
    <categories>
      
      <category>optimization</category>
      
    </categories>
    
    
    <tags>
      
      <tag>convex set</tag>
      
      <tag>convex function</tag>
      
      <tag>affine</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>凸优化学习系列 - 7. 随机梯度下降法</title>
    <link href="/2026/01/26/03=Convex_Optimization/CVX-7/"/>
    <url>/2026/01/26/03=Convex_Optimization/CVX-7/</url>
    
    <content type="html"><![CDATA[<blockquote><p>目标：读完这篇博客，应当能</p></blockquote><ul><li>理解随机梯度的必要性及基本机制；</li><li>学会如何用随机梯度下降法求解凸优化问题；</li><li>知道随机梯度下降法（SGD）的收敛速率及其与全梯度下降的区别；</li><li>理解 mini-batch 与 early stopping 的作用及实现；</li></ul><hr><h2 id="从有限和凸优化问题开始sgd-的标准场景">1. 从“有限和”凸优化问题开始：SGD 的标准场景</h2><p>SGD 最经典的出发点是<strong>有限和（finite-sum）</strong>目标： <span class="math display">\[\begin{align}\min_{x\in\mathbb{R}^n} f(x)\;\triangleq\;\frac{1}{m}\sum_{i=1}^{m} f_i(x),\end{align}\]</span> 其中每个 <span class="math inline">\(f_i\)</span> 通常对应一条数据样本的损失（或一个子目标）。在机器学习/统计中，<span class="math inline">\(m\)</span> 往往极大（百万、亿级），这就是 SGD 诞生的核心动机。</p><hr><h2 id="batch-gradient-descent全梯度下降-vs-sgd到底差在哪">2. Batch Gradient Descent（全梯度下降） vs SGD：到底差在哪？</h2><h3 id="全梯度下降batch-full-gradient-descent">2.1 全梯度下降（Batch / Full Gradient Descent）</h3><p>因为 <span class="math display">\[\begin{align}\nabla f(x)=\frac{1}{m}\sum_{i=1}^m \nabla f_i(x),\end{align}\]</span> 全梯度下降迭代为： <span class="math display">\[\begin{align}x^{(k)} = x^{(k-1)} - t_k \cdot \frac{1}{m}\sum_{i=1}^m \nabla f_i\!\left(x^{(k-1)}\right).\end{align}\]</span></p><p><strong>瓶颈很直接</strong>：每一步都要把 <span class="math inline">\(m\)</span> 项梯度算一遍；当 <span class="math inline">\(m\)</span> 巨大时，一步就非常贵。</p><hr><h3 id="随机梯度下降sgd-incremental-gradient">2.2 随机梯度下降（SGD / Incremental Gradient）</h3><p>SGD 每次只抽一个索引 <span class="math inline">\(i_k\in\{1,\dots,m\}\)</span>： <span class="math display">\[\begin{align}x^{(k)} = x^{(k-1)} - t_k \cdot \nabla f_{i_k}\!\left(x^{(k-1)}\right).\end{align}\]</span></p><p><strong>关键优势</strong>（ 主结论）：</p><ul><li><strong>单步计算成本与 <span class="math inline">\(m\)</span> 无关</strong>（只算一个样本的梯度）</li><li><strong>内存压力更小</strong>（尤其在大规模数据、流式数据、分布式场景）</li></ul><hr><h2 id="i_k-怎么选两条规则与一个核心性质">3. <span class="math inline">\(i_k\)</span> 怎么选？两条规则与一个核心性质</h2><p>两条常见规则：</p><ol type="1"><li><strong>随机规则（更常用）</strong>：<span class="math inline">\(i_k\)</span> 每次在 <span class="math inline">\(\{1,\dots,m\}\)</span> 上<strong>均匀随机</strong>抽取<br></li><li><strong>循环规则</strong>：<span class="math inline">\(i_k=1,2,\dots,m,1,2,\dots,m,\dots\)</span></li></ol><h3 id="为什么随机规则更顺无偏梯度估计">3.1 为什么随机规则“更顺”？——无偏梯度估计</h3><p>若 <span class="math inline">\(i_k\)</span> 均匀随机，则对任意 <span class="math inline">\(x\)</span>： <span class="math display">\[\begin{align}\mathbb{E}\big[\nabla f_{i_k}(x)\big] = \frac{1}{m}\sum_{i=1}^m \nabla f_i(x)= \nabla f(x).\end{align}\]</span></p><p>这句话非常重要：<br><strong>SGD 每一步用的是“全梯度的无偏估计（unbiased estimate）”。</strong></p><blockquote><p>无偏 <strong>不等于</strong> 低噪声。SGD 的问题不在“偏”，而在“方差”。</p></blockquote><hr><h2 id="典型例子逻辑回归logistic-regression为什么需要-sgd">4. 典型例子：逻辑回归（Logistic Regression）为什么需要 SGD？</h2><p>给定数据 <span class="math inline">\((x_i,y_i)\in\mathbb{R}^p\times\{0,1\}\)</span>，logistic 回归（经验风险最小化）： <span class="math display">\[\begin{align}\min_{\beta\in\mathbb{R}^p}\;\frac{1}{n}\sum_{i=1}^n \left(-y_i x_i^\top\beta + \log(1+\exp(x_i^\top\beta))\right)\;\triangleq\;\frac{1}{n}\sum_{i=1}^n f_i(\beta).\end{align}\]</span></p><ul><li>全梯度（batch）一步：要累加 <span class="math inline">\(n\)</span> 个样本梯度，复杂度 <span class="math inline">\(\mathcal{O}(np)\)</span><br></li><li>SGD 一步：只算一个样本梯度，复杂度 <span class="math inline">\(\mathcal{O}(p)\)</span></li></ul><p>当 <span class="math inline">\(n\)</span> 很大（10^6 级别），batch 一步可能比做很多次 SGD 还贵：</p><blockquote><p><strong>SGD 往往在远离最优解时进展很快；接近最优解时会抖动、变慢。</strong></p></blockquote><p>这不是玄学，而是噪声方差导致的“无法精确贴近最优点”。</p><hr><h2 id="步长step-size为什么-sgd-常用递减步长">5. 步长（Step Size）：为什么 SGD 常用递减步长？</h2><p>SGD 的“标准做法”是<strong>递减步长</strong>，例如 <span class="math display">\[\begin{align}t_k = \frac{1}{k}.\end{align}\]</span></p><h3 id="为什么固定步长会出问题直觉推导">5.1 为什么固定步长会出问题？（直觉推导）</h3><p>为直观起见考虑循环规则，并固定 <span class="math inline">\(t_k=t\)</span>，连续做 <span class="math inline">\(m\)</span> 次更新： <span class="math display">\[\begin{align}x^{(k+m)} = x^{(k)} - t\sum_{i=1}^{m} \nabla f_i\!\left(x^{(k+i-1)}\right).\end{align}\]</span></p><p>如果用全梯度、步长取 <span class="math inline">\(mt\)</span>，则是： <span class="math display">\[\begin{align}x^{(k+1)} = x^{(k)} - t\sum_{i=1}^{m} \nabla f_i\!\left(x^{(k)}\right).\end{align}\]</span></p><p>两者差异是： <span class="math display">\[\begin{align}t\sum_{i=1}^{m}\Big(\nabla f_i(x^{(k+i-1)})-\nabla f_i(x^{(k)})\Big).\end{align}\]</span></p><p>如果 <span class="math inline">\(t\)</span> 固定，这个“误差项”一般<strong>不会自动趋于 0</strong>（因为 <span class="math inline">\(x^{(k+i-1)}\)</span> 在变化）。<br>这就是为什么在经典理论里，SGD 用递减步长更容易保证收敛。</p><blockquote><p><strong>SGD 的步长策略不是装饰，而是理论与稳定性的一部分。</strong></p></blockquote><hr><h2 id="收敛速率">6. 收敛速率</h2><h3 id="先回忆gd-的典型速率">6.1 先回忆：GD 的典型速率</h3><ul><li>一般凸（convex）：使用递减步长的 GD<br><span class="math display">\[\begin{align}f(x^{(k)})-f^\star = \mathcal{O}\!\left(\frac{1}{\sqrt{k}}\right)\end{align}\]</span></li><li>若 <span class="math inline">\(f\)</span> 可微且 <span class="math inline">\(\nabla f\)</span> Lipschitz，合适固定步长：<br><span class="math display">\[\begin{align}f(x^{(k)})-f^\star = \mathcal{O}\!\left(\frac{1}{k}\right)\end{align}\]</span></li><li>若强凸且 Lipschitz 梯度：线性收敛<br><span class="math display">\[\begin{align}f(x^{(k)})-f^\star = \mathcal{O}(\gamma^k),\quad 0&lt;\gamma&lt;1\end{align}\]</span></li></ul><h3 id="sgd-的典型速率核心结论">6.2 SGD 的典型速率（核心结论）</h3><p>对一般凸 <span class="math inline">\(f\)</span>，SGD（递减步长）满足： <span class="math display">\[\begin{align}\mathbb{E}\big[f(x^{(k)})\big]-f^\star = \mathcal{O}\!\left(\frac{1}{\sqrt{k}}\right).\end{align}\]</span></p><p>更刺眼的是：即使进一步假设 <span class="math inline">\(\nabla f\)</span> Lipschitz，这个速率<strong>通常也不会提升到 <span class="math inline">\(\mathcal{O}(1/k)\)</span></strong>。</p><p>而在“强凸 + Lipschitz 梯度”条件下： - GD：<span class="math inline">\(\mathcal{O}(\gamma^k)\)</span>（线性）<br>- SGD：<br><span class="math display">\[\begin{align}  \mathbb{E}\big[f(x^{(k)})\big]-f^\star = \mathcal{O}\!\left(\frac{1}{k}\right)  \end{align}\]</span></p><p><strong>结论</strong>：</p><blockquote><p>经典 SGD <strong>享受不到</strong> GD 在强凸情形下的线性收敛。</p></blockquote><p>这是算法机制决定的：噪声方差导致尾部难以“贴近”最优点。</p><hr><h2 id="mini-batch-sgd减少方差但别指望必然更快">7. Mini-batch SGD：减少方差，但别指望“必然更快”</h2><p>mini-batch 在第 <span class="math inline">\(k\)</span> 次迭代随机抽一个小批量 <span class="math inline">\(I_k\subset\{1,\dots,m\}\)</span>，<span class="math inline">\(|I_k|=b\ll m\)</span>： <span class="math display">\[\begin{align}x^{(k)} = x^{(k-1)} - t_k \cdot \frac{1}{b}\sum_{i\in I_k}\nabla f_i(x^{(k-1)}).\end{align}\]</span></p><p>它仍是无偏的： <span class="math display">\[\begin{align}\mathbb{E}\left[\frac{1}{b}\sum_{i\in I_k}\nabla f_i(x)\right]=\nabla f(x).\end{align}\]</span></p><p>给出两个关键信息： - 方差大约缩小为原来的 <span class="math inline">\(1/b\)</span> - 但一次迭代计算量也增大为 <span class="math inline">\(b\)</span> 倍</p><p>理论上在 Lipschitz 梯度下，速率从 <span class="math inline">\(\mathcal{O}(1/\sqrt{k})\)</span> 改为 <span class="math display">\[\begin{align}\mathcal{O}\!\left(\frac{1}{\sqrt{bk}}+\frac{1}{k}\right),\end{align}\]</span> <strong>纯从 flops 角度，mini-batch 未必占优</strong>；它在实践中仍有意义（例如并行、GPU、吞吐）。</p><blockquote><p>别把 mini-batch 当成“SGD 的加速器”。它首先是“方差-并行-吞吐”的工程折中。</p></blockquote><hr><h2 id="规则化-vs-early-stopping一个反直觉但很实用的思路">8. 规则化 vs Early Stopping：一个反直觉但很实用的思路</h2><p>考虑高维 <span class="math inline">\(p\)</span> 很大时拟合 logistic 回归。可以显式做 ridge（<span class="math inline">\(\ell_2\)</span>）正则： <span class="math display">\[\begin{align}\min_\beta\;\frac{1}{n}\sum_{i=1}^n\Big(-y_i x_i^\top\beta+\log(1+e^{x_i^\top\beta})\Big)+\frac{\lambda}{2}\|\beta\|_2^2.\end{align}\]</span></p><p>也可以解“无正则”问题，但<strong>用 GD/SGD 跑几步就停</strong>（early stopping）： <span class="math display">\[\begin{align}\beta^{(k)}=\beta^{(k-1)}-\varepsilon\cdot \frac{1}{n}\sum_{i=1}^n (y_i-p_i(\beta^{(k-1)}))x_i,\end{align}\]</span> 并把 <span class="math inline">\(\beta^{(k)}\)</span> 视作某个正则强度下的近似解。</p><p>直觉（“steepest descent”视角）：</p><ul><li>GD 每一步都在“降低损失”与“控制参数范数增长”之间做隐式平衡<br></li><li>因此“跑到第 <span class="math inline">\(k\)</span> 步”往往对应某种隐式的正则化强度</li></ul><blockquote><p>如果在大规模学习里纠结“要不要把目标解到极致”，大概率是在浪费算力：统计精度常常不需要最优到 <span class="math inline">\(10^{-10}\)</span>。</p></blockquote><hr><h2 id="用-sgd-解凸优化的方法模板">9. 用 SGD 解凸优化的“方法模板”</h2><p>下面给出一个<strong>面向有限和凸问题</strong>的标准 SGD 实施模板。</p><h3 id="优化问题">9.1 优化问题</h3><p><span class="math display">\[\begin{align}\min_x\; f(x)=\frac{1}{m}\sum_{i=1}^m f_i(x),\end{align}\]</span> 并且能计算（或高效近似）单样本梯度 <span class="math inline">\(\nabla f_i(x)\)</span>。</p><h3 id="sgd-伪代码randomized-index">9.2 SGD 伪代码（Randomized index）</h3><p><span class="math display">\[\begin{array}{l}\textbf{Algorithm 1: Stochastic Gradient Descent}\\\textbf{Input:} \ x^{(0)}\in\mathbb{R}^n; \ \text{Step-size schedule } \{t_k\}_{k=1}^{K}; \ K; \ \text{indices } i_k\in\{1,\dots,m\}.\\[2mm]1.\  \textbf{for } k=1,2,\dots,K\ \textbf{do}\\2.\quad \text{Sample index } i_k \sim \mathrm{Unif}\{1,\dots,m\}\\3.\quad \quad g_k \leftarrow \nabla f_{i_k}\!\left(x^{(k-1)}\right) \qquad \text{(stochastic gradient)}\\4.\quad \quad x^{(k)} \leftarrow x^{(k-1)} - t_k\, g_k \qquad \text{(update)}\\5.\  \textbf{end for}\\\textbf{Output:}\;\;x^{(K)}\ \text{, optionally } \bar{x}^{(K)}=\frac{1}{K}\sum_{k=1}^{K} x^{(k)}\ \text{(averaged iterate)}\\[2mm]\end{array}\]</span></p><h3 id="步长怎么选">9.3 步长怎么选</h3><ul><li><strong>理论友好（凸优化教科书默认）</strong>：递减步长，如 <span class="math inline">\(t_k=c/\sqrt{k}\)</span> 或 <span class="math inline">\(c/k\)</span></li><li><strong>工程常用（大规模 ML）</strong>：固定步长 + 调参 + early stopping（s这是常态）</li><li><strong>更高级（本文不展开）</strong>：动量、平均化、AdaGrad/Adam 等自适应步长</li></ul><p><strong>经验事实</strong>： - SGD 在远离最优时很划算（便宜、下降快） - 接近最优时抖动明显（高方差导致“尾部慢”） —</p><h2 id="对比表">10. 对比表</h2><table><thead><tr class="header"><th style="text-align: center;">维度</th><th style="text-align: center;">Full GD</th><th style="text-align: center;">SGD</th><th style="text-align: center;">Mini-batch SGD</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">每步梯度</td><td style="text-align: center;">全量 <span class="math inline">\(\frac{1}{m}\sum\nabla f_i\)</span></td><td style="text-align: center;">单样本 <span class="math inline">\(\nabla f_{i_k}\)</span></td><td style="text-align: center;">小批量均值</td></tr><tr class="even"><td style="text-align: center;">单步成本</td><td style="text-align: center;">高，随 <span class="math inline">\(m\)</span> 线性增长</td><td style="text-align: center;">低，与 <span class="math inline">\(m\)</span> 无关</td><td style="text-align: center;">中，随 <span class="math inline">\(b\)</span> 增长</td></tr><tr class="odd"><td style="text-align: center;">梯度噪声</td><td style="text-align: center;">无</td><td style="text-align: center;">高</td><td style="text-align: center;">较低（约降到 <span class="math inline">\(1/b\)</span>）</td></tr><tr class="even"><td style="text-align: center;">典型收敛表现</td><td style="text-align: center;">贴近最优很稳</td><td style="text-align: center;">远处快、近处抖</td><td style="text-align: center;">折中，且便于并行</td></tr><tr class="odd"><td style="text-align: center;">理论速率（凸）</td><td style="text-align: center;">可到 <span class="math inline">\(\mathcal{O}(1/k)\)</span>（Lipschitz 梯度）</td><td style="text-align: center;"><span class="math inline">\(\mathbb{E}[f(x^{(k)})]-f^\star=\mathcal{O}(1/\sqrt{k})\)</span></td><td style="text-align: center;"><span class="math inline">\(\mathcal{O}(1/\sqrt{bk}+1/k)\)</span></td></tr><tr class="even"><td style="text-align: center;">强凸情形</td><td style="text-align: center;">可线性收敛 <span class="math inline">\(\mathcal{O}(\gamma^k)\)</span></td><td style="text-align: center;">仅 <span class="math inline">\(\mathcal{O}(1/k)\)</span>（期望）</td><td style="text-align: center;">取决于 <span class="math inline">\(b\)</span> 与实现</td></tr></tbody></table><hr><hr>]]></content>
    
    
    <categories>
      
      <category>optimization</category>
      
    </categories>
    
    
    <tags>
      
      <tag>convex set</tag>
      
      <tag>convex function</tag>
      
      <tag>affine</tag>
      
      <tag>stochastic gradient descent</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SOS Programming 系列 - 0 预备知识和底层逻辑</title>
    <link href="/2025/12/12/02=SOS_Programming/SOS_0/"/>
    <url>/2025/12/12/02=SOS_Programming/SOS_0/</url>
    
    <content type="html"><![CDATA[<h1 id="sos-programming">SOS Programming</h1><hr><h2 id="what什么是-sossum-of-squares">1. What：什么是 SOS（Sum-of-Squares）</h2><h3 id="多项式非负性的一个可计算的充分条件">1.1 多项式非负性的一个“可计算的充分条件”</h3><p>在连续系统分析、鲁棒控制、非线性稳定性、优化中，经常需要证明一个多项式在某个集合上<strong>非负</strong>，例如 - Lyapunov 函数条件：<span class="math inline">\(V(x)\ge 0\)</span>、<span class="math inline">\(\dot V(x)\le 0\)</span> - 障碍函数（CBF）条件：<span class="math inline">\(h(x)\ge 0\)</span> 集合不变 - 多项式不等式约束：<span class="math inline">\(p(x)\ge 0\)</span></p><p>难点是：<strong>判定一个一般多项式在全空间（或半代数集合）非负，是很难的（一般是 NP-hard 级别）</strong>。SOS 的思想是：把“非负”替换成一个<strong>更强但可求解</strong>的条件。</p><hr><h3 id="sos-的定义">1.2 SOS 的定义</h3><p>给定一个实系数多项式 <span class="math inline">\(p(x)\)</span>（<span class="math inline">\(x\in\mathbb{R}^n\)</span>），如果存在一些多项式 <span class="math inline">\(f_i(x)\)</span> 使得</p><p><span class="math display">\[p(x)=\sum_{i=1}^m f_i(x)^2,\]</span></p><p>则称 <span class="math inline">\(p(x)\)</span> 是 <strong>Sum-of-Squares（SOS）多项式</strong>，记作 <span class="math inline">\(p(x)\in \Sigma[x]\)</span>。</p><p>显然： - 若 <span class="math inline">\(p(x)\)</span> 是 SOS，则 <span class="math inline">\(p(x)\ge 0\)</span> 对所有 <span class="math inline">\(x\)</span> 成立； - 反过来不一定（存在非负但不是 SOS 的多项式，维度/次数上去后会出现）。</p><p>因此 SOS 是“非负”的一个<strong>可计算的充分条件</strong>。</p><hr><h3 id="sos-与-gram-矩阵关键桥梁把多项式问题变成-psd-矩阵问题">1.3 SOS 与 Gram 矩阵（关键桥梁：把多项式问题变成 PSD 矩阵问题）</h3><p>设 <span class="math inline">\(p(x)\)</span> 的次数为 <span class="math inline">\(2d\)</span>（偶数，若是奇数也可通过引入松弛处理），取单项式向量 <span class="math inline">\(z(x)\)</span> 为所有次数 <span class="math inline">\(\le d\)</span> 的单项式（按某种顺序排列）：</p><p><span class="math display">\[z(x)=\big[1,\ x_1,\dots,\ x_n,\ x_1^2,\ x_1x_2,\dots,\ x_n^d\big]^\top.\]</span></p><p>核心定理（Gram 表示）： <span class="math display">\[p(x)\ \text{是 SOS} \quad \Longleftrightarrow \quad \exists\, Q\succeq 0\ \text{使得}\ p(x)=z(x)^\top Q z(x).\]</span></p><p>这里 <span class="math inline">\(Q\succeq 0\)</span> 表示 <span class="math inline">\(Q\)</span> 是<strong>半正定矩阵</strong>。</p><p>直观理解： - <span class="math inline">\(z^\top Q z\)</span> 展开后是一个多项式； - 约束 <span class="math inline">\(Q\succeq 0\)</span> 保证它可以写成若干平方和（因为 <span class="math inline">\(Q=R^\top R\)</span> 时 <span class="math inline">\(z^\top Q z=\|Rz\|^2\)</span>）。</p><p>这一步是 SOS programming 的“发动机”： - 多项式非负性（难） - 变成 PSD 矩阵可行性（SDP，可数值求解）</p><hr><h2 id="why为什么要用-sos">2. Why：为什么要用 SOS</h2><h3 id="真正想做的是验证合成sos-把它变成可求解的凸优化很多时候">2.1 真正想做的是“验证/合成”，SOS 把它变成可求解的凸优化（很多时候）</h3><p>控制里常见两类任务：</p><p><strong>(A) 验证（analysis）</strong><br>给定候选 <span class="math inline">\(V(x)\)</span>，验证： <span class="math display">\[V(x)-\epsilon\|x\|^{2d}\in \Sigma[x],\qquad -\dot V(x)\in \Sigma[x]\]</span> 用来证明稳定性、吸引域等。</p><p><strong>(B) 合成（synthesis）</strong><br>同时“找”一个多项式 <span class="math inline">\(V(x)\)</span>（甚至还要找控制律 <span class="math inline">\(u(x)\)</span>），使得上述条件成立。此时变量是 <span class="math inline">\(V\)</span> 的系数、控制律系数、以及 Gram 矩阵 <span class="math inline">\(Q\)</span> 等。</p><p>为什么 SOS 特别有价值： - 它把“解析证明”变成“可计算证明”； - 把很多原本非凸/不可判定的问题，替换为 SDP（或分层的 SDP 序列）。</p><hr><h3 id="sos-与半代数集合上的非负性putinar-证书在实际约束里会碰到">2.2 SOS 与半代数集合上的非负性：Putinar 证书（在实际约束里会碰到）</h3><p>很多时候不需要在全空间非负，而是在集合</p><p><span class="math display">\[\mathcal{K}=\{x:\ g_1(x)\ge 0,\dots,g_r(x)\ge 0\}\]</span></p><p>上非负（例如状态约束、吸引域估计的子水平集、CBF 安全集）。</p><p>常用的 SOS 证书形式（典型为 Putinar 型）是：</p><p><span class="math display">\[p(x)\ge 0\ \text{on}\ \mathcal{K}\ \Leftarrow\ p(x)=\sigma_0(x)+\sum_{i=1}^r \sigma_i(x)\,g_i(x),\quad \sigma_i(x)\in \Sigma[x].\]</span></p><p>解释： - <span class="math inline">\(\sigma_i(x)\ge 0\)</span>，<span class="math inline">\(g_i(x)\ge 0\)</span> 在集合内成立； - 因而右侧在集合内必然非负； - 通过“找 <span class="math inline">\(\sigma_i\)</span>”来构造一个<strong>可验证的非负证书</strong>。</p><p>这就是在 SOS Lyapunov、ROA、CBF、鲁棒性里反复看到的乘子（multiplier）逻辑。</p><hr><h3 id="现实代价sos-不是免费午餐">2.3 现实代价：SOS 不是“免费午餐”</h3><p>要清楚 SOS 的边界，否则很容易“跑通例子但不理解本质”：</p><ol type="1"><li><strong>保守性</strong>：非负不一定是 SOS；集合约束证书也需要一定条件（如 archimedean 条件）才有收敛保证。<br></li><li><strong>规模爆炸</strong>：<span class="math inline">\(z(x)\)</span> 的维度是组合数级别，SDP 变量 <span class="math inline">\(Q\)</span> 尺寸随 <span class="math inline">\(n,d\)</span> 急速增长。<br></li><li><strong>数值问题</strong>：SDP 的条件数、松弛项 <span class="math inline">\(\epsilon\)</span>、尺度缩放都会影响可解性与可信度。</li></ol><p>SOS programming，不是为了“把一切都扔进工具箱”，而是为了理解：<strong>它在什么结构下能高效地产生可信证书</strong>。</p><hr><h2 id="how怎么用-sos">3. How：怎么用 SOS</h2><h3 id="标准工作流">3.1 标准工作流</h3><ol type="1"><li><strong>把问题写成多项式不等式</strong>：稳定性、约束不变性、性能界等。<br></li><li><strong>选定证书结构</strong>：全局 SOS 或 半代数集合上的 SOS 乘子证书。<br></li><li><strong>选择次数（degree）</strong>：决定 <span class="math inline">\(V\)</span>、乘子 <span class="math inline">\(\sigma_i\)</span>、以及 <span class="math inline">\(z(x)\)</span> 的单项式上界。<br></li><li><strong>把“多项式等式”转为“系数匹配线性约束”</strong>：这是从符号到数值的关键一步。<br></li><li><strong>得到 SDP</strong>：变量是 Gram 矩阵（PSD）+ 多项式系数（线性）。<br></li><li><strong>调用 SDP 求解器</strong>：SeDuMi / SDPT3 / MOSEK 等。<br></li><li><strong>读回证书</strong>：得到 <span class="math inline">\(V(x)\)</span>、<span class="math inline">\(\sigma_i(x)\)</span> 或控制律系数，并做数值验证（采样检验、残差检验、尺度敏感性检验）。</li></ol><hr><h3 id="例-1lyapunov-稳定性验证最基础的-sos-用法">3.2 例 1：Lyapunov 稳定性验证（最基础的 SOS 用法）</h3><p>考虑多项式系统 <span class="math display">\[\dot x=f(x)\]</span> 给一个候选 <span class="math inline">\(V(x)\)</span>（多项式），希望证明原点渐近稳定。</p><p>常见 SOS 条件（加一个小的正定松弛）： <span class="math display">\[V(x)-\epsilon \|x\|^{2d} \in \Sigma[x],\qquad-\nabla V(x)^\top f(x) - \epsilon \|x\|^{2d} \in \Sigma[x].\]</span></p><p>含义： - 第一条：<span class="math inline">\(V\)</span> 正定（至少强制为 SOS 正定）； - 第二条：<span class="math inline">\(\dot V\le -\epsilon\|x\|^{2d}\)</span>，保证严格下降。</p><p>工具层面做的事情是：选择 <span class="math inline">\(V\)</span> 的形式/次数，然后把两个“属于 <span class="math inline">\(\Sigma[x]\)</span>”转成两个 PSD 约束。</p><hr><h3 id="例-2吸引域roa估计子水平集不变性">3.3 例 2：吸引域（ROA）估计：子水平集不变性</h3><p>想找到一个最大的 <span class="math inline">\(\rho\)</span>，使得集合 <span class="math display">\[\Omega_\rho=\{x:\ V(x)\le \rho\}\]</span> 在动力学下正不变，并且在集合内 <span class="math inline">\(\dot V&lt;0\)</span>。</p><p>经典 SOS 形式（S-procedure / 乘子）： <span class="math display">\[-(\nabla V^\top f)(x) - \sigma(x)\,(\rho - V(x)) \in \Sigma[x],\qquad \sigma(x)\in \Sigma[x].\]</span></p><p>解释： - 当 <span class="math inline">\(V(x)\le \rho\)</span> 时，<span class="math inline">\(\rho - V(x)\ge 0\)</span>； - 若 <span class="math inline">\(\sigma\ge 0\)</span>，则 <span class="math inline">\(\sigma(\rho-V)\)</span> 是非负项； - 上式保证在 <span class="math inline">\(V\le \rho\)</span> 内，<span class="math inline">\(-\dot V\)</span> 仍然非负，从而 <span class="math inline">\(\dot V\le 0\)</span>。</p><p>合成任务通常是最大化 <span class="math inline">\(\rho\)</span>（或相关体积近似指标），这会引入非凸性；实际常用交替迭代（fix <span class="math inline">\(V\)</span> solve <span class="math inline">\(\sigma,\rho\)</span>，再 fix <span class="math inline">\(\sigma\)</span> solve <span class="math inline">\(V\)</span> 等）。</p><hr><h3 id="例-3带约束的非负性把只在集合内成立转为-sos-证书">3.4 例 3：带约束的非负性：把“只在集合内成立”转为 SOS 证书</h3><p>若要保证 <span class="math display">\[p(x)\ge 0\quad \text{for all } x\in\mathcal{K}=\{x:g_i(x)\ge 0\},\]</span> 用乘子证书： <span class="math display">\[p(x)=\sigma_0(x)+\sum_i \sigma_i(x)g_i(x),\quad \sigma_i\in\Sigma[x].\]</span></p><p>应该把它理解为：<strong>在 <span class="math inline">\(\mathcal{K}\)</span> 上的非负性，被“解释”为全空间上的一个结构化分解。</strong> 工具不理解“集合上的逻辑”，它只理解“全空间的等式 + SOS/PSD 约束”。集合语义由通过乘子结构编码进去。</p><hr><h2 id="sos-tools-的底层求解逻辑与细节重点">4. SOS Tools 的“底层求解逻辑与细节”（重点）</h2><p>用 SOSTOOLS / YALMIP / Spotless / SumOfSquares.jl 时，看起来像“写几行就出结果”。但底层发生的事情基本都可以归结为下面这套机制。</p><hr><h3 id="关键抽象sos-约束-存在-psd-gram-矩阵-系数匹配">4.1 关键抽象：SOS 约束 = “存在 PSD Gram 矩阵 + 系数匹配”</h3><p>假设写了一个约束 <span class="math display">\[p(x)\in \Sigma[x].\]</span> 工具会做：</p><ol type="1"><li>选定 <span class="math inline">\(z(x)\)</span>（由指定次数上界决定）。<br></li><li>引入对称矩阵变量 <span class="math inline">\(Q\)</span>。<br></li><li>强制 <span class="math display">\[p(x)=z(x)^\top Q z(x)\]</span> 这不是数值点的等式，而是<strong>多项式恒等式</strong>。<br></li><li>把恒等式变成“系数匹配”：</li></ol><ul><li>将两边展开为单项式线性组合；</li><li>对每个单项式 <span class="math inline">\(x^\alpha\)</span>，匹配系数： <span class="math display">\[\text{coeff}_{x^\alpha}(p) = \text{coeff}_{x^\alpha}(z^\top Q z),\]</span> 得到一堆对 <span class="math inline">\(Q\)</span> 元素的<strong>线性等式约束</strong>。<br></li></ul><ol start="5" type="1"><li>再加上 PSD 约束： <span class="math display">\[Q\succeq 0.\]</span></li></ol><p>最终：SOS 约束变成了 SDP 约束。</p><hr><h3 id="为什么系数匹配是线性的">4.2 为什么“系数匹配”是线性的</h3><p><span class="math inline">\(z^\top Q z\)</span> 对 <span class="math inline">\(Q\)</span> 是线性的，因为 <span class="math display">\[z^\top Q z = \sum_{i,j} Q_{ij}\,z_i(x)\,z_j(x),\]</span> 其中 <span class="math inline">\(z_i z_j\)</span> 是已知的单项式（或多项式），未知量只有 <span class="math inline">\(Q_{ij}\)</span>，且是一阶出现，所以系数匹配给出线性方程组。</p><p>这也是为什么 SOS programming 大多落在 <strong>凸优化（SDP）</strong> 的范畴内：未知量只以线性方式进入，外加一个凸锥约束（PSD 锥）。</p><hr><h3 id="乘子multipliersos-的底层更多-gram-矩阵-更多线性约束">4.3 乘子（multiplier）SOS 的底层：更多 Gram 矩阵 + 更多线性约束</h3><p>当写 <span class="math display">\[p=\sigma_0+\sum_i \sigma_i g_i,\quad \sigma_i\in\Sigma[x],\]</span> 工具会： - 为每个 <span class="math inline">\(\sigma_i\)</span> 建一个 Gram 矩阵 <span class="math inline">\(Q_i\succeq 0\)</span>； - 将 <span class="math inline">\(\sigma_i(x)=z_i(x)^\top Q_i z_i(x)\)</span>； - 把 <span class="math inline">\(p - \sum_i \sigma_i g_i\)</span> 展开； - 对每个单项式做系数匹配，得到线性等式； - 合起来依然是 SDP。</p><p>因此应当把“复杂 SOS 证书”理解为： - PSD 变量（可能很多块） - 线性耦合约束（系数匹配把块耦合起来）</p><hr><h3 id="工具如何选择单项式基细节决定规模">4.4 工具如何“选择单项式基”——细节决定规模</h3><p>单项式向量 <span class="math inline">\(z(x)\)</span> 的长度决定 Gram 矩阵大小。</p><p>若 <span class="math inline">\(n\)</span> 变量、最高次数 <span class="math inline">\(d\)</span>，<span class="math inline">\(z\)</span> 的长度是组合数： <span class="math display">\[|z| = \binom{n+d}{d}.\]</span></p><p>Gram 矩阵 <span class="math inline">\(Q\)</span> 维度是 <span class="math inline">\(|z|\times |z|\)</span>，独立变量数量约为 <span class="math inline">\(|z|(|z|+1)/2\)</span>（对称）。</p><p>必须具备这种数量级直觉，否则会在“n=6, d=6”这类设置上直接爆炸。</p><hr><h3 id="sdp-求解器实际在做什么">4.5 SDP 求解器实际在做什么</h3><p>SOS 工具本身通常不“求解”，它只是把模型<strong>编译</strong>成标准 SDP 形式，然后交给 SDP 求解器。</p><p>标准 SDP（原始-对偶）形式可写成： <span class="math display">\[\begin{align}\min_x\ &amp; c^\top x\\\text{s.t. }&amp; F(x)=F_0+\sum_k x_k F_k \succeq 0,\\&amp; Ax=b.\end{align}\]</span></p><p>对应到 SOS： - <span class="math inline">\(x\)</span> 包括：Lyapunov/控制律/乘子多项式系数、以及 Gram 矩阵自由元素； - <span class="math inline">\(F(x)\succeq 0\)</span> 包括所有 Gram PSD 约束（可能是块对角）； - <span class="math inline">\(Ax=b\)</span> 就是系数匹配产生的线性等式。</p><p>大多数 SDP 求解器用<strong>内点法（primal-dual interior-point）</strong>，本质是不断求解一系列线性化的 KKT 系统，直到残差足够小。</p><p>需要知道的现实影响： 1) 规模越大，KKT 系统越大，时间/内存迅速上升；<br>2) 病态（尺度差、次数高、系数跨度大）会导致数值不稳定；<br>3) 很多“不可行”其实是数值不可行（tolerance 问题），不是数学上不可行。</p><hr><h3 id="求出来的-sos如何读回到平方和分解">4.6 “求出来的 SOS”如何读回到“平方和分解”</h3><p>如果得到 <span class="math inline">\(Q\succeq 0\)</span>，理论上可分解 <span class="math inline">\(Q=R^\top R\)</span>，于是 <span class="math display">\[p(x)=z^\top Q z = \|R z(x)\|^2 = \sum_i f_i(x)^2,\]</span> 其中 <span class="math inline">\(f_i\)</span> 是 <span class="math inline">\(Rz\)</span> 的各分量。</p><p>但实际工具常不显式给 <span class="math inline">\(f_i\)</span>，而只给 <span class="math inline">\(Q\)</span> 或多项式系数。 如果要“证书可展示”，可以： - 数值上做 Cholesky/特征分解拿到一个近似的平方和； - 或者只展示 SOS 约束满足与残差（更常见、更稳健）。</p><hr><h3 id="次数选择与双线性bilinearity">4.7 次数选择与双线性（Bilinearity）</h3><p>很多“合成”问题一旦同时优化多个多项式，会出现双线性项，典型如： - 同时求 <span class="math inline">\(V\)</span> 和 乘子 <span class="math inline">\(\sigma\)</span>：<span class="math inline">\(\sigma(\rho-V)\)</span> 中 <span class="math inline">\(\sigma\)</span> 与 <span class="math inline">\(V\)</span> 相乘； - 同时求 <span class="math inline">\(V\)</span> 和 控制律 <span class="math inline">\(u(x)\)</span>：<span class="math inline">\(\nabla V^\top g(x)u(x)\)</span> 中 <span class="math inline">\(V\)</span> 与 <span class="math inline">\(u\)</span> 相乘。</p><p>这会把 SDP 变成非凸问题（BMI 或 polynomial bilinear constraints）。 工具可能还能“跑”，但若不理解这一点，就会误以为“SOS 一定是凸的”。</p><p>工程上常见处理： - 固定一部分变量，解另一个 SDP； - 交替迭代（coordinate descent 风格）； - 或引入结构化参数化降低耦合。</p><hr><h2 id="如何让-sos-结果更可信更可扩展">5. 如何让 SOS 结果更可信、更可扩展</h2><ol type="1"><li><strong>先做验证再做合成</strong>：先给一个结构合理的 <span class="math inline">\(V\)</span> 验证稳定性，确认系统与尺度没问题，再谈合成。<br></li><li><strong>控制尺度</strong>：变量归一化，避免系数跨越过大；必要时加入 <span class="math inline">\(\epsilon\)</span> 松弛。<br></li><li><strong>从低次数开始</strong>：<span class="math inline">\(d=2\)</span>、<span class="math inline">\(d=3\)</span> 起步，逐步加；不要一开始就追求“更强证书”。<br></li><li><strong>残差检查</strong>：将求解后的 <span class="math inline">\(p(x)-z^\top Q z\)</span> 展开检查最大系数残差；对关键不等式做采样验证。<br></li><li><strong>明确保守性来源</strong>：是 SOS 放松导致，还是次数限制导致，还是集合证书结构限制导致。<br></li><li><strong>识别是否非凸</strong>：一旦同时优化多个多项式并出现乘积，别再把它当 SDP；要用迭代/固定策略。</li></ol><hr><h2 id="总结">6. 总结</h2><p>SOS programming 的核心不是“平方和”本身，而是这条链：</p><p><span class="math display">\[\begin{align*}\text{多项式非负性（难）} &amp;\Rightarrow \text{SOS 证书（充分条件）} \Rightarrow \\\text{Gram 矩阵 PSD + 系数匹配（SDP）}  &amp;\Rightarrow \text{数值求解 + 证书回读/验证}.\end{align*}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>SOS Programming</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SOS Tools</tag>
      
      <tag>optimization</tag>
      
      <tag>matlab</tag>
      
      <tag>SDP</tag>
      
      <tag>cvx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>L2 概念整理</title>
    <link href="/2025/04/29/00=Daily/4-L_2/"/>
    <url>/2025/04/29/00=Daily/4-L_2/</url>
    
    <content type="html"><![CDATA[<hr><h2 id="一输入输出稳定性与增益约束-robot">一、输入输出稳定性与增益约束 <span class="github-emoji"><span>🤖</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h2><p>输入输出系统被建模为一个算子 $ G $，将输入信号 <span class="math inline">\(u : {\mathbb R} \mapsto {\mathbb R}^m\)</span> 映射为输出信号 $ y : {R} ^p $，即 $ y = G(u)$。</p><p>输入输出信号被认为属于 $ {L}_q[0, ) $ 空间，该空间由满足：</p><p><span class="math display">\[\|f\|_q := \left( \int_0^{\infty} \|f(t)\|^q {\mathrm {d}}t \right)^{1/q} &lt; \infty\]</span></p><p>的测度函数构成。这一约束使我们关注信号整体能量而非瞬时行为。</p><p>输入输出映射的最基本要求是<strong>因果性</strong>（causality）：系统在时间 $ t $ 时刻的输出 $y(t) $ 不能依赖于未来的输入 $ u(s) &nbsp;(s &gt; t) $。数学上，映射 <span class="math inline">\(G : L_q \to L_p\)</span> 是因果的，当且仅当对任意 $ u,v _q $ 和任意 $ T &gt; 0 $，若</p><p><span class="math display">\[u(t) = v(t) \quad \forall t \in [0, T] ,\]</span></p><p>则有</p><p><span class="math display">\[(G(u))(t) = (G(v))(t), \quad \forall t \in [0, T].\]</span></p><p>这保证了系统对未来信息的不可预测性，是物理实现的基本要求。</p><p>在此基础上，若存在常数 $ &gt; 0 $，使得：</p><p><span class="math display">\[\|G(u)\|_q \leq \gamma \|u\|_q,\]</span></p><p>则系统被称为具有有限 $ {L}_q $-增益，该增益 $ $ 被称为系统的 ${L}_q $-增益。更进一步，若系统对任意有界输入 $ u _q $ 输出 $ y _q $，则称为 $ {L}_q $-stable。</p><hr><h2 id="二小增益定理与闭环稳定性-boom">二、小增益定理与闭环稳定性 <span class="github-emoji"><span>💥</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a5.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h2><p>当两个系统 $ G_1, G_2 $ 以负反馈互联构成闭环系统时，其输入输出关系将变得复杂。系统互联后，扰动信号可能在环路中反复传播、放大或衰减，因此需要严密的准则判断系统整体的稳定性。若我们将系统互联结构建模为：</p><p><span class="math display">\[\begin{cases}y_1 = G_1(u_1), \quad u_1 = e_1 - y_2, \\y_2 = G_2(u_2), \quad u_2 = e_2 + y_1,\end{cases}\]</span></p><p>则可以通过替换得到：</p><p><span class="math display">\[u = e - FG(u), \quad y = G(u), \quad F = \begin{bmatrix}0 &amp; I \\-I &amp; 0\end{bmatrix}.\]</span></p><p>此时系统整体的闭环行为由非线性代数方程 $ u = (I + FG)^{-1} e $ 决定。映射 $ (e_1, e_2) (y_1, y_2) $ 是否定义良好（即闭环是否存在且唯一），并具有 $ {L}_q $-稳定性，成为关键问题。</p><p><strong>小增益定理</strong> 给出了这个问题的充分条件之一：</p><p>若两个因果系统 $ G_1, G_2 $ 分别具有有限 $ {L}_q $-增益 $ _1, _2 $，且满足：</p><p><span class="math display">\[\gamma_1 \cdot \gamma_2 &lt; 1,\]</span></p><p>则闭环系统整体是 $ {}_q $-稳定的，且映射 $ e y $ 是连续、唯一的。</p><p>这一定理以代数乘积揭示了稳定性的本质：在能量意义下，只要反馈回路的总“放大能力”小于1，就能保证系统“收敛”而不是发散。对于线性系统，这一结果直接对应于频域的 <span class="math inline">\(H_{\infty}\)</span> 范数约束；而对于非线性系统，小增益定理提供了从局部 Lipschitz 性或数值界估计出发的通用判据。</p><hr><h2 id="三无源性系统与能量守恒结构-mechanical_arm">三、无源性系统与能量守恒结构 <span class="github-emoji"><span>🦾</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f9be.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h2><p>相较于小增益定理的代数判据，无源性理论（passivity）从物理角度出发，刻画系统在能量传递上的性质。</p><p>一个系统是<strong>无源的</strong>，意味着它无法生成能量：系统输出给环境的能量不超过它从环境获取的能量。</p><p>形式上，若对系统 $ G $ 存在常数 $  $，对任意 $ u _2 $ 和任意时间 $ T &gt; 0 $，有：</p><p><span class="math display">\[\int_0^T \langle (G(u))(t), u(t) \rangle {\mathrm {d}}t \geq \beta,\]</span></p><p>则称 $G $ 为<strong>无源系统</strong>。进一步地，若存在 $ &gt; 0 $ 使得：</p><p><span class="math display">\[\int_0^T \langle (G(u))(t), u(t) \rangle {\mathrm{d}}t \geq \varepsilon \|G(u)\|_{\mathcal{L}_2[0,T]}^2 + \beta,\]</span></p><p>则称 $ G $ 为<strong>严格输出无源系统</strong>。</p><p>无源系统的结构性质具有深远影响。首先，它天然满足因果性；其次，两个无源系统的互联系统仍然无源，从而自动继承稳定性和有限增益性质。</p><p>更重要的是，严格输出无源系统必定具有有限 $ {L}_2 $-gain，其增益满足：</p><p><span class="math display">\[\|G(u)\|_{\mathcal{L}_2} \leq \frac{1}{\sqrt\varepsilon} \|u\|_{\mathcal{L}_2} ,\]</span></p><p>因此，无源性不仅是系统的物理属性，也是一种增益约束条件，为稳定性与性能分析提供了结构化的数学工具。</p><hr><h2 id="四耗散系统-person_fencing">四、耗散系统 <span class="github-emoji"><span>🤺</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f93a.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h2><p>为了将输入输出的增益性和状态空间的 Lyapunov 稳定性统一，提出了<strong>耗散系统</strong>的框架。</p><p>系统 $  = f(x, u), &nbsp;y = h(x, u) $ 被称为关于供能率 $ s(u, y) $ 的耗散系统，若存在函数 $ S(x)  $，称为<strong>存储函数</strong>，使得对任意时间区间 <span class="math inline">\([t_0, t_1]\)</span>：</p><p><span class="math display">\[S(x(t_1)) - S(x(t_0)) \le \int_{t_0}^{t_1} s(u(t), y(t)) {\mathrm{d}}t.\]</span></p><p>这一不等式意味着系统“吸收”的供能 $ s(u, y) $ 不会转化为比 $ S $ 所表示的内部能量更多的输出，换言之，系统具有某种“能量耗散”特性。</p><p>根据所选供能率 $ s(u,y) $ 的不同，可以刻画出不同的系统属性，如下分析。</p><h3 id="globe_with_meridians-4-1-供能率的选择与系统性能"><span class="github-emoji"><span>🌐</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f310.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 4-1 供能率的选择与系统性能</h3><p>耗散系统理论的核心，是通过定义一个输入输出对 $ (u, y) $ 上的<strong>供能率函数</strong> $ s(u, y) $，来描述系统的能量如何从输入传递到输出，以及是否会在系统内部以某种形式“被”存储或“耗散”。这一函数并非任意，而是根据我们对系统特性的不同关注点选出。典型的供能率如下：</p><h4 id="earth_africa-1.-su-y-uy"><span class="github-emoji"><span>🌍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 1. $ s(u, y) = u^y $</h4><p>此时，系统是无源的（Passive）。这是最自然的选择，反映系统从外界吸收的瞬时功率。此时，耗散不等式变为：</p><p><span class="math display">\[S(x(t_1)) - S(x(t_0)) \leq \int_{t_0}^{t_1} u^\top(t) y(t) {\mathrm {d}}t,\]</span></p><p>表示系统内部存储的能量增量不超过输入与输出之间的能量传递。其背后逻辑是：如果一个系统不会凭空生成能量，那它从输入吸收的功必须转化为内部能量或者输出能量。</p><p>在输入输出映射的形式下，该条件等价于：</p><p><span class="math display">\[\int_0^T u^\top(t) G(u)(t) {\mathrm {d}}t \geq \beta,\]</span></p><p>即映射 $ G $ 为无源系统。</p><h4 id="earth_africa-2.-su-y-uy---y2"><span class="github-emoji"><span>🌍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 2. $ s(u, y) = u^y - |y|^2 $</h4><p>此时，系统是<strong>严格输出无源的</strong>（Strict Output Passive）。在实际物理系统中，能量不仅被储存，还可能由于摩擦、电阻等原因被耗散。此时系统对输出信号的能量具有“惩罚项”，形式化为：</p><p><span class="math display">\[\int u^\top y \, {\mathrm {d}}t \geq \varepsilon \int \|y\|^2 {\mathrm {d}}t + \beta.\]</span></p><p>这说明系统输出越大，能量耗散越多。等价于：</p><p><span class="math display">\[\int_0^T u^\top(t) y(t) {\mathrm {d}}t - \varepsilon \int_0^T \|y(t)\|^2 {\mathrm {d}}t \geq \beta.\]</span></p><p>在无状态模型中，它等价于映射 $ G $ 满足：</p><p><span class="math display">\[\|G(u)\|_{\mathcal{L}_2}^2 \leq \frac{1}{\varepsilon} \int u^\top G(u) \, {\mathrm {d}}t.\]</span></p><p>因此，该供能率刻画的是<strong>输出能量耗散对输入功率的约束</strong>，其物理意义在于系统能从输出“泄露能量”。</p><h4 id="earth_africa-3.-su-y-2-u2---y2"><span class="github-emoji"><span>🌍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 3. $ s(u, y) =  ^2 |u|^2 -  |y|^2 $</h4><p>此时，系统具有有限 $ {L}_2 $-增益，不超过 $ $。该形式直接对应于系统从输入信号到输出信号的<strong>增益控制</strong>。若存在存储函数 $ S(x) $ 使得：</p><p><span class="math display">\[S(x(t_1)) - S(x(t_0)) \leq \int_{t_0}^{t_1} \left( \frac{1}{2} \gamma^2 \|u(t)\|^2 - \|y(t)\|^2 \right) {\mathrm {d}}t,\]</span></p><p>则可推出：</p><p><span class="math display">\[\|y\|^2_{\mathcal{L}_2} \leq \gamma^2 \|u\|^2_{\mathcal{L}_2} + 2 S(x_0),\]</span></p><p>这说明系统的输出能量不会超过输入能量的 $ ^2 $ 倍，加上初始能量的一个常数项。若 $ S(x) = 0 $，这就成了标准的 $ {L}_2 $-增益界定。</p><h4 id="earth_africa-4.-su-y--y2"><span class="github-emoji"><span>🌍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 4. $ s(u, y) = -|y|^2 $</h4><p>此时，系统输出能量呈现衰减趋势。这是一个强约束：系统的能量<strong>不但不能增加</strong>，甚至其输出总能量<strong>必须“亏损”</strong>。从不等式：</p><p><span class="math display">\[S(x(t_1)) - S(x(t_0)) \leq - \int_{t_0}^{t_1} \|y(t)\|^2 {\mathrm{d}}t,\]</span></p><p>可知只要输出有能量，存储函数必然下降。这是某些无源耗散系统（如阻尼结构）的典型模型，表明系统必须将输出转化为内部的损耗耗散，直至最终达到稳定。</p><h4 id="earth_africa-5.-su-y-y2---2-u2"><span class="github-emoji"><span>🌍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 5. $ s(u, y) = |y|^2 - {}^2 |u|^2 $</h4><p>此时，系统放大能力有限，适用于构造 <span class="math inline">\(H_{\infty}\)</span> 控制目标。这是一种对偶形式，更倾向于用于性能评估。若系统关于该 $ s(u, y) $ 是耗散的，则必然满足：</p><p><span class="math display">\[\int \|y\|^2 {\mathrm {d}}t \leq \gamma^2 \int \|u\|^2 {\mathrm {d}}t + S(x_0),\]</span></p><p>进而推出增益约束：</p><p><span class="math display">\[\|y\|_{\mathcal{L}_2} \leq \gamma \|u\|_{\mathcal{L}_2}.\]</span></p><p>它是最直接的 <span class="math inline">\(H_{\infty}\)</span> 范数定义形式，用作控制设计中的目标函数。该供能率的优势在于它对输出信号进行直接约束，适用于性能评估（如抗扰动、最坏场景最小化等场景）。</p><h3 id="globe_with_meridians4-2-available-storage"><span class="github-emoji"><span>🌐</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f310.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>4-2 available storage</h3><p><strong>定义：</strong></p><p><span class="math display">\[S_a(x) := \sup_{u \in {\mathcal L}_2} \left\{ - \int_0^\infty s(u(t), y(t)) {\mathrm {d}}t \right\}, \quad x(0) = x.\]</span></p><p>该函数描述从当前状态 $ x $ 出发，系统在所有可行输入作用下，<strong>所能向外界“释放”的最大能量</strong>。其物理含义与热力学中的“可用能”相同，表明状态 $ x $ 所蕴含的“控制势能”极限。</p><p><strong>从控制意义看：</strong></p><ul><li>若 $ S_a(x) = 0 $，说明此状态下系统无法向外界输出能量，是耗尽状态；</li><li>若 $ S_a(x) &gt; 0 $，说明存在输入使系统输出净能量，状态具有能量转换能力。</li></ul><p><strong>数学上，available storage 还有如下性质：</strong></p><ul><li>它是所有存储函数的<strong>下界</strong>，即：若 $ S(x) $ 是任意使系统耗散的函数，则：</li></ul><p><span class="math display">\[S_a(x) \leq S(x), \quad \forall x.\]</span></p><ul><li>它自身就是一个存储函数；</li><li>在 passivity 条件下，$ s(u, y) = u^y $，可得：</li></ul><p><span class="math display">\[\int_0^T u^\top y \, {\mathrm {d}}t \geq -S_a(x(0)).\]</span></p><p>这为系统耗散性提供了<strong>最小能量约束</strong>，有助于进行能量一致性判断与系统设计下界估计。</p><hr><h2 id="五h_infty-控制问题-mag">五、<span class="math inline">\(H_{\infty}\)</span> 控制问题 <span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h2><p>考虑非线性系统：</p><p><span class="math display">\[\dot{x} = f(x) + g(x) u + k(x) d,\]</span></p><p>其中：<span class="math inline">\(x \in \mathbb{R}^n\)</span>：系统状态；<span class="math inline">\(u \in \mathbb{R}^{n_u}\)</span>：控制输入；<span class="math inline">\(d \in \mathbb{R}^{n_d}\)</span>：外部扰动；<span class="math inline">\(f(x), g(x), k(x)\)</span> 为局部 Lipschitz 连续函数。</p><p>系统的性能输出定义为：</p><p><span class="math display">\[z(x, u) = Q(x) + u^\top R u,\]</span></p><p>其中 <span class="math inline">\(Q(x) \geq 0\)</span> 是正定函数，<span class="math inline">\(R \succ 0\)</span> 是控制输入的权重矩阵。性能目标为：对于任意扰动 <span class="math inline">\(d \in \mathcal{L}_2\)</span>，闭环系统满足：</p><p><span class="math display">\[\int_0^\infty \left( Q(x(t)) + u^\top(t) R u(t) \right) {\mathrm {d}}t \leq \gamma^2 \|d\|^2_{\mathcal{L}_2}.\]</span></p><p>这是一种标准的非线性 <span class="math inline">\(H_\infty\)</span> 控制形式，目标是以最小控制代价抑制最强扰动。</p><h3 id="bulb-5-1.-耗散系统-h_infty-性能指标"><span class="github-emoji"><span>💡</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 5-1. 耗散系统 <span class="math inline">\(H_\infty\)</span> 性能指标</h3><p>构造供能率函数：</p><p><span class="math display">\[s(d, x, u) := Q(x) + u^\top R u - \gamma^2 \|d\|^2,\]</span></p><p>若存在光滑非负函数 <span class="math inline">\(V(x)\)</span> 使得系统沿解轨迹满足：</p><p><span class="math display">\[\dot{V}(x) \leq - s(d, x, u) ,\]</span></p><p>则沿系统轨迹有</p><p><span class="math display">\[\frac{\mathrm{d}}{\mathrm{d}t} V(x(t))\le -Q(x) - u^\top R u + \gamma^2 \|d\|^2.\]</span></p><p>则通过积分可得：</p><p><span class="math display">\[V(x(0)) \geq \int_0^\infty \left( Q(x(t)) + u^\top(t) R u(t) \right) {\mathrm {d}}t - \gamma^2 \|d\|^2_{\mathcal{L}_2},\]</span></p><p>即系统满足期望的 <span class="math inline">\(H_\infty\)</span> 性能指标。</p><h3 id="bulb-5-2.-hji-推导"><span class="github-emoji"><span>💡</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 5-2. HJI 推导</h3><p>考虑沿系统轨迹对 <span class="math inline">\(V(x)\)</span> 求导：</p><p><span class="math display">\[\dot{V}(x) = \nabla V(x)^\top \left( f(x) + g(x) u + k(x) d \right).\]</span></p><p>代入上述不等式，有：</p><p><span class="math display">\[\nabla V^\top f(x) + \nabla V^\top g(x) u + \nabla V^\top k(x) d \leq - Q(x) - u^\top R u + \gamma^2 \|d\|^2.\]</span></p><p>整理后构造 Hamiltonian：</p><p><span class="math display">\[H(x, \nabla V, u, d) := \nabla V^\top f(x) + Q(x) + \nabla V^\top g(x) u + u^\top R u + \nabla V^\top k(x) d - \gamma^2 \|d\|^2.\]</span></p><p>要求：</p><p><span class="math display">\[\inf_{u} \sup_{d} H(x, \nabla V, u, d) \leq 0,\]</span></p><p>即系统在最坏扰动下依然满足性能限制。</p><h3 id="bulb-5-3.-扰动与控制的最优化"><span class="github-emoji"><span>💡</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 5-3. 扰动与控制的最优化</h3><ul><li><strong>(1) 对扰动 <span class="math inline">\(d\)</span> 的极大化（扰动 <span class="math inline">\(d\)</span> 试图最大化供能）</strong>：</li></ul><p>求：</p><p><span class="math display">\[\sup_d \left\{ \nabla V^\top k(x) d - \gamma^2 \|d\|^2 \right\}\]</span></p><p>最优扰动为：</p><p><span class="math display">\[d^* = \frac{1}{2\gamma^2} k(x)^\top \nabla V(x)\]</span></p><p>对应最大值为：</p><p><span class="math display">\[\frac{1}{4\gamma^2} \| \nabla V^\top k(x) \|^2.\]</span></p><ul><li><strong>(2) 对控制输入 <span class="math inline">\(u\)</span> 的极小化（控制 <span class="math inline">\(u\)</span> 试图最小化供能）</strong>：</li></ul><p>求：</p><p><span class="math display">\[\inf _u \left\{ \nabla V^\top g(x) u + u^\top R u \right\} \]</span></p><p>最优控制为：</p><p><span class="math display">\[u^* = - \frac{1}{2} R^{-1} g(x)^\top \nabla V(x)\]</span></p><p>对应最小值为：</p><p><span class="math display">\[ -\frac{1}{4} \nabla V^\top g(x) R^{-1} g(x)^\top \nabla V.\]</span></p><p>最优控制为：</p><p><span class="math display">\[u^* = - \frac{1}{2} R^{-1} g(x)^\top \nabla V(x)\]</span></p><p>对应最小值为：</p><p><span class="math display">\[ -\frac{1}{4} \nabla V^\top g(x) R^{-1} g(x)^\top \nabla V.\]</span></p><h3 id="bulb-5-4.-hji-不等式最终形式"><span class="github-emoji"><span>💡</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 5-4. HJI 不等式最终形式</h3><p>将控制项与扰动项代入 Hamiltonian，得到最终形式的 HJI 不等式：</p><p><span class="math display">\[\nabla V^\top f(x) + Q(x) - \frac{1}{4} \nabla V^\top g(x) R^{-1} g(x)^\top \nabla V + \frac{1}{4\gamma^2} \|\nabla V^\top k(x)\|^2 \leq 0.\]</span></p><p>若存在正定光滑函数 <span class="math inline">\(V(x)\)</span> 满足上述不等式，则控制器：</p><p><span class="math display">\[u(x) = -\frac{1}{2} R^{-1} g(x)^\top \nabla V(x)\]</span></p><p>确保系统满足性能约束：</p><p><span class="math display">\[\int_0^\infty \left( Q(x(t)) + u^\top(t) R u(t) \right) dt \leq \gamma^2 \|d\|^2_{\mathcal{L}_2}.\]</span></p><hr>]]></content>
    
    
    <categories>
      
      <category>control</category>
      
    </categories>
    
    
    <tags>
      
      <tag>耗散系统</tag>
      
      <tag>L2</tag>
      
      <tag>H_infty control</tag>
      
      <tag>HJI</tag>
      
      <tag>Robust control</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github 端口连接</title>
    <link href="/2025/03/27/00=Daily/3-git_port/"/>
    <url>/2025/03/27/00=Daily/3-git_port/</url>
    
    <content type="html"><![CDATA[<h2 id="rocket-一问题描述"><span class="github-emoji"><span>🚀</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 一、问题描述</h2><p>以前从本地 <code>pull/push</code> 文件向 <code>github</code> 仓库远程操作时，没有任何问题，今天操作出现如下报错： </p><figure class="highlight pgsql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">bash<br>$ git pull<br>ssh: <span class="hljs-keyword">connect</span> <span class="hljs-keyword">to</span> host github.com port <span class="hljs-number">22</span>: <span class="hljs-keyword">Connection</span> refused<br>fatal: Could <span class="hljs-keyword">not</span> <span class="hljs-keyword">read</span> <span class="hljs-keyword">from</span> remote repository.<br>​<br>Please make sure you have the correct <span class="hljs-keyword">access</span> rights<br><span class="hljs-keyword">and</span> the repository <span class="hljs-keyword">exists</span>.<br></code></pre></td></tr></tbody></table></figure> 提示链接 <code>github</code> 的 <code>22</code> 端口被拒绝。<p></p><h2 id="zap-二解决方案"><span class="github-emoji"><span>⚡</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/26a1.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 二、解决方案</h2><ul><li>修改 <code>SSH</code> 的 <code>config</code> 文件。在 <code>C:\Users\用户名\.ssh</code> 目录下，新建一个名为 <code>config</code> 的文件，内容如下：（如果目录下有 <code>config</code> 文件，直接在文件里添加即可） <figure class="highlight stylus"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">Host github<span class="hljs-selector-class">.com</span><br>  HostName github<span class="hljs-selector-class">.com</span><br>  Port <span class="hljs-number">443</span><br></code></pre></td></tr></tbody></table></figure> 切换端口为<code>443</code>。</li><li>打开 <code>Git Bash</code>，输入以下命令，测试连接： <figure class="highlight nginx"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">ssh</span> -T git<span class="hljs-variable">@github</span>.com<br></code></pre></td></tr></tbody></table></figure></li><li>如果连接成功，会显示以下信息： <figure class="highlight ada"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">Hi username! You<span class="hljs-symbol">'ve</span> successfully authenticated, but GitHub does <span class="hljs-keyword">not</span> provide shell <span class="hljs-keyword">access</span>.<br></code></pre></td></tr></tbody></table></figure></li><li>重新 <code>pull/push</code> 即可。</li></ul><h2 id="dart-三报错原因"><span class="github-emoji"><span>🎯</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 三、报错原因</h2><ul><li>可能是因为 <code>SSH</code> 的 <code>config</code> 文件中配置的端口与实际使用的端口不一致。</li><li>用了 <code>VPN</code> 导致的网络配置问题。</li></ul>]]></content>
    
    
    <categories>
      
      <category>github</category>
      
    </categories>
    
    
    <tags>
      
      <tag>github</tag>
      
      <tag>debug</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VS code 编译 .tex 文件</title>
    <link href="/2025/03/04/00=Daily/2-vs_tex/"/>
    <url>/2025/03/04/00=Daily/2-vs_tex/</url>
    
    <content type="html"><![CDATA[<h2 id="安装-vs-code-插件latex-workshop">1. 安装 VS code 插件：<code>Latex Workshop</code></h2><p><span class="github-emoji"><span>🚀</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 在 VS code 扩展商店中找到 <a href="https://github.com/James-Yu/LaTeX-Workshop/wiki/Compile#placeholders">Latex Workshop</a> 插件（如下），安装即可。 <img src="/img/VSTeX/LatexWorkshop.jpg" alt="LatexWorkshop"></p><h2 id="打开一个-.tex-文件">2. 打开一个 <code>.tex</code> 文件</h2><p><span class="github-emoji"><span>🤖</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 可以看到 VS code 已经自动高亮文档内容 <img src="/img/VSTeX/latex.jpg" alt="latex"></p><h2 id="配置-latex-workshop">3. 配置 Latex Workshop</h2><ul><li><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 按 <code>F1</code>，搜索<code>setjson</code>，打开配置文件 <img src="/img/VSTeX/setjson.jpg" alt="setjson"></li><li><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 可以看到在项目中生成了<code>.vscode</code>文件夹，在其中打开<code>settings.json</code>文件，添加如下代码： <figure class="highlight autoit"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><code class="hljs autoit">{<br>    <span class="hljs-string">"latex-workshop.latex.autoBuild.run"</span>: <span class="hljs-string">"never"</span>,        <span class="hljs-meta"># 从不自动编译</span><br>    <span class="hljs-string">"latex-workshop.showContextMenu"</span>: <span class="hljs-literal">true</span>,               <span class="hljs-meta"># 显示右键菜单，出现<span class="hljs-string">"Build Latex Project"</span>选项</span><br>    <span class="hljs-string">"latex-workshop.intellisense.package.enabled"</span>: <span class="hljs-literal">true</span>,  <span class="hljs-meta"># 启用包自动完成</span><br>    <span class="hljs-string">"latex-workshop.message.error.show"</span>: <span class="hljs-literal">false</span>,           <span class="hljs-meta"># 不显示错误信息（从终端获取）</span><br>    <span class="hljs-string">"latex-workshop.message.warning.show"</span>: <span class="hljs-literal">false</span>,         <span class="hljs-meta"># 不显示警告信息（从终端获取）</span><br>    <span class="hljs-string">"latex-workshop.latex.tools"</span>: [<br>        {<br>            <span class="hljs-string">"name"</span>: <span class="hljs-string">"xelatex"</span>,<br>            <span class="hljs-string">"command"</span>: <span class="hljs-string">"xelatex"</span>,<br>            <span class="hljs-string">"args"</span>: [<br>                <span class="hljs-string">"-synctex=1"</span>,<br>                <span class="hljs-string">"-interaction=nonstopmode"</span>,<br>                <span class="hljs-string">"-file-line-error"</span>,<br>                <span class="hljs-string">"%DOCFILE%"</span><br>            ]<br>        },<br>        {<br>            <span class="hljs-string">"name"</span>: <span class="hljs-string">"pdflatex"</span>,<br>            <span class="hljs-string">"command"</span>: <span class="hljs-string">"pdflatex"</span>,<br>            <span class="hljs-string">"args"</span>: [<br>                <span class="hljs-string">"-synctex=1"</span>,<br>                <span class="hljs-string">"-interaction=nonstopmode"</span>,<br>                <span class="hljs-string">"-file-line-error"</span>,<br>                <span class="hljs-string">"%DOCFILE%"</span>                                <span class="hljs-meta"># 文件所在路径可以为中文，若为<span class="hljs-string">"%DOC"</span>则不可有中文</span><br>            ]<br>        },<br>        {<br>            <span class="hljs-string">"name"</span>: <span class="hljs-string">"latexmk"</span>,<br>            <span class="hljs-string">"command"</span>: <span class="hljs-string">"latexmk"</span>,<br>            <span class="hljs-string">"args"</span>: [<br>                <span class="hljs-string">"-synctex=1"</span>,<br>                <span class="hljs-string">"-interaction=nonstopmode"</span>,<br>                <span class="hljs-string">"-file-line-error"</span>,<br>                <span class="hljs-string">"-pdf"</span>,<br>                <span class="hljs-string">"-outdir=%OUTDIR%"</span>,<br>                <span class="hljs-string">"%DOCFILE%"</span><br>            ]<br>        },<br>        {<br>            <span class="hljs-string">"name"</span>: <span class="hljs-string">"bibtex"</span>,<br>            <span class="hljs-string">"command"</span>: <span class="hljs-string">"bibtex"</span>,<br>            <span class="hljs-string">"args"</span>: [<br>                <span class="hljs-string">"%DOCFILE%"</span><br>            ]<br>        }<br>    ],<br>    <span class="hljs-string">"latex-workshop.latex.recipes"</span>: [<br>        {<br>            <span class="hljs-string">"name"</span>: <span class="hljs-string">"XeLaTeX"</span>,<br>            <span class="hljs-string">"tools"</span>: [<br>                <span class="hljs-string">"xelatex"</span><br>            ]<br>        },<br>        {<br>            <span class="hljs-string">"name"</span>: <span class="hljs-string">"PDFLaTeX"</span>,<br>            <span class="hljs-string">"tools"</span>: [<br>                <span class="hljs-string">"pdflatex"</span><br>            ]<br>        },<br>        {<br>            <span class="hljs-string">"name"</span>: <span class="hljs-string">"BibTeX"</span>,<br>            <span class="hljs-string">"tools"</span>: [<br>                <span class="hljs-string">"bibtex"</span><br>            ]<br>        },<br>        {<br>            <span class="hljs-string">"name"</span>: <span class="hljs-string">"LaTeXmk"</span>,<br>            <span class="hljs-string">"tools"</span>: [<br>                <span class="hljs-string">"latexmk"</span><br>            ]<br>        },<br>        {<br>            <span class="hljs-string">"name"</span>: <span class="hljs-string">"xelatex -&gt; bibtex -&gt; xelatex*2"</span>,<br>            <span class="hljs-string">"tools"</span>: [<br>                <span class="hljs-string">"xelatex"</span>,<br>                <span class="hljs-string">"bibtex"</span>,<br>                <span class="hljs-string">"xelatex"</span>,<br>                <span class="hljs-string">"xelatex"</span><br>            ]<br>        },<br>        {<br>            <span class="hljs-string">"name"</span>: <span class="hljs-string">"pdflatex -&gt; bibtex -&gt; pdflatex*2"</span>,<br>            <span class="hljs-string">"tools"</span>: [<br>                <span class="hljs-string">"pdflatex"</span>,<br>                <span class="hljs-string">"bibtex"</span>,<br>                <span class="hljs-string">"pdflatex"</span>,<br>                <span class="hljs-string">"pdflatex"</span><br>            ]<br>        },<br>    ],<br>    <span class="hljs-string">"latex-workshop.latex.clean.fileTypes"</span>: [<br>        <span class="hljs-string">"*.aux"</span>,<br>        <span class="hljs-string">"*.bbl"</span>,<br>        <span class="hljs-string">"*.blg"</span>,<br>        <span class="hljs-string">"*.idx"</span>,<br>        <span class="hljs-string">"*.ind"</span>,<br>        <span class="hljs-string">"*.lof"</span>,<br>        <span class="hljs-string">"*.lot"</span>,<br>        <span class="hljs-string">"*.out"</span>,<br>        <span class="hljs-string">"*.toc"</span>,<br>        <span class="hljs-string">"*.acn"</span>,<br>        <span class="hljs-string">"*.acr"</span>,<br>        <span class="hljs-string">"*.alg"</span>,<br>        <span class="hljs-string">"*.glg"</span>,<br>        <span class="hljs-string">"*.glo"</span>,<br>        <span class="hljs-string">"*.gls"</span>,<br>        <span class="hljs-string">"*.ist"</span>,<br>        <span class="hljs-string">"*.fls"</span>,<br>        <span class="hljs-string">"*.log"</span>,<br>        <span class="hljs-string">"*.fdb_latexmk"</span><br>    ],<br>    <span class="hljs-string">"latex-workshop.latex.autoClean.run"</span>: <span class="hljs-string">"onFailed"</span>,                       <span class="hljs-meta"># 编译失败时自动清理上述类型的辅助文件（<span class="hljs-string">"OnBuilt"</span>:无论编译是否成功，都清除辅助文件；<span class="hljs-string">"never"</span>:无论编译是否成功，都不清除）</span><br>    <span class="hljs-string">"latex-workshop.latex.recipe.default"</span>: <span class="hljs-string">"lastUsed"</span>,                      <span class="hljs-meta"># 编译时使用上次使用的编译方式</span><br>    <span class="hljs-string">"latex-workshop.view.pdf.internal.synctex.keybinding"</span>: <span class="hljs-string">"double-click"</span>   <span class="hljs-meta"># 点击 pdf 可以跳转到相应代码位置</span><br>}<br></code></pre></td></tr></tbody></table></figure></li></ul><h2 id="设置编译快捷键可选">4. 设置编译快捷键（可选）</h2><ul><li><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 左下角打开设置 -&gt; <code>Keyboard Shortcut</code>-&gt; 搜索<code>recipe</code> -&gt; 双击中间的<code>When</code> -&gt; 按下想要使用的快捷键</li><li><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 选择编译方式 <img src="/img/VSTeX/compile.jpg" alt="compile"></li><li><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <strong>直接运行<code>.tex</code>文件也可以编译</strong></li></ul><h2 id="测试">5. 测试</h2><figure><img src="/img/VSTeX/test.jpg" alt="test"><figcaption aria-hidden="true">test</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>Latex</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VS code</tag>
      
      <tag>Latex</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>什么是 .bat 文件？</title>
    <link href="/2025/03/03/00=Daily/1-bat_explain/"/>
    <url>/2025/03/03/00=Daily/1-bat_explain/</url>
    
    <content type="html"><![CDATA[<h2 id="monocle_face-what-is-the-file-with-the-suffix-.bat"><span class="github-emoji"><span>🧐</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f9d0.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> What is the file with the suffix “.bat” ?</h2><ul><li>后缀为“.bat”的文档是一个批处理文件。它是一种<strong>可执行</strong>文件，其中包含由操作系统执行的一系列命令。批处理文件通常用于自动化涉及多个命令的任务。它们通常用于<strong>安装</strong>软件、<strong>配置</strong>系统设置或<strong>执行</strong>其他需要多个步骤的任务。</li></ul><h2 id="rocket-example"><span class="github-emoji"><span>🚀</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> Example</h2><p><strong>这是一个批处理文件的示例，它将“open with VS code”选项添加到Windows右键菜单中，允许用户直接右键单击文件夹或其背景（空白），并在 VS code 中打开文件夹。</strong></p><figure class="highlight nix"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs nix">@echo off<br>REG ADD <span class="hljs-string">"HKEY_CLASSES_ROOT<span class="hljs-char escape_">\D</span>irectory<span class="hljs-char escape_">\B</span>ackground<span class="hljs-char escape_">\s</span>hell<span class="hljs-char escape_">\O</span>pen with VS Code"</span> <span class="hljs-symbol">/ve</span> <span class="hljs-symbol">/t</span> REG_SZ <span class="hljs-symbol">/d</span> <span class="hljs-string">"open here with VS Code"</span> <span class="hljs-symbol">/f</span><br>REG ADD <span class="hljs-string">"HKEY_CLASSES_ROOT<span class="hljs-char escape_">\D</span>irectory<span class="hljs-char escape_">\B</span>ackground<span class="hljs-char escape_">\s</span>hell<span class="hljs-char escape_">\O</span>pen with VS Code<span class="hljs-char escape_">\c</span>ommand"</span> <span class="hljs-symbol">/ve</span> <span class="hljs-symbol">/t</span> REG_SZ <span class="hljs-symbol">/d</span> <span class="hljs-string">"<span class="hljs-char escape_">\"</span>E:<span class="hljs-char escape_">\V</span>S code<span class="hljs-char escape_">\M</span>icrosoft VS Code<span class="hljs-char escape_">\C</span>ode.exe<span class="hljs-char escape_">\"</span> <span class="hljs-char escape_">\"</span>%%V<span class="hljs-char escape_">\"</span>"</span> <span class="hljs-symbol">/f</span><br>echo VS Code Successful！<br>pause<br></code></pre></td></tr></tbody></table></figure><ul><li><span class="github-emoji"><span>📞</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4de.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <strong><code>@echo off</code></strong> 上述代码的第一行<strong>关闭</strong>命令行窗口的响应，以防止每次<strong>执行命令</strong>时它出现在屏幕上，使界面更清晰。<p style="margin-bottom:40px;"></p></li><li><span class="github-emoji"><span>🖥</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f5a5.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <strong><code>REG ADD "HKEY_CLASSES_ROOT\Directory\Background\shell\Open with VS Code" /ve /t REG_SZ /d "open here with VS Code" /f</code></strong> 将键或值添加到Windows注册表。<ul><li><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <strong><code>KEY_CLASSES_ROOT\Directory\Background\shell\Open with VS Code</code></strong> : 在Windows资源管理器的右键菜单中创建一个新选项“open with VS code”。</li><li><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <strong><code>/ve</code></strong> : 创建一个默认值（未命名键值）。</li><li><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <strong><code>/t REG_SZ</code></strong> : 将值的数据类型设置为<strong>REG_SZ</strong>（<em>字符串</em>）。</li><li><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <strong><code>/d "open here with VS Code"</code></strong> : 设置注册表的值，即右键菜单中显示的文本（<strong>open here with VS code</strong>）。</li><li><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <strong><code>/f</code></strong> : 强制操作<strong>覆盖</strong>任何现有键或值。<p style="margin-bottom:40px;"></p></li></ul></li><li><span class="github-emoji"><span>🖥</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f5a5.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <strong><code>REG ADD "HKEY_CLASSES_ROOT\Directory\Background\shell\Open with VS Code\command" /ve /t REG_SZ /d "\"E:\VS code\Microsoft VS Code\Code.exe\" \"%%V\"" /f</code></strong><ul><li><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <strong><code>HKEY_CLASSES_ROOT\Directory\Background\shell\Open with VS Code\command</code></strong>: 定义单击菜单项后要执行的命令。</li><li><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <strong><code>"E:\VS code\Microsoft VS Code\Code.exe"</code></strong>: VS code 的可执行文件路径。</li><li><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <strong><code>"%%V"</code></strong>: <strong>当前</strong>文件夹路径，即用户右键单击的文件夹。<p style="margin-bottom:40px;"></p></li></ul></li><li><span class="github-emoji"><span>🖥</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f5a5.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <strong><code>echo VS Code Successful！</code></strong><ul><li><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 在命令行窗口中显示提示，表明注册表已成功修改。</li></ul></li></ul><h2 id="file_folder-operation-effect"><span class="github-emoji"><span>📁</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c1.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> Operation effect</h2><ul><li><span class="github-emoji"><span>👆</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f446.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> Right-click the folder or blank space inside the folder, select “Open with VS Code”. <img src="/img/BatExplain/operation1.jpg" alt="right-click1"> <img src="/img/BatExplain/operation2.jpg" alt="right-click2"></li></ul>]]></content>
    
    
    <categories>
      
      <category>VS code</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VS code</tag>
      
      <tag>bat</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
